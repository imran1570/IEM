{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import keras\n",
    "#import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import average_precision_score, f1_score, precision_recall_curve\n",
    "from numpy.random import choice\n",
    "from scipy.optimize import minimize_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_week_data_filename_QD(week_number, simulated_org_number):\n",
    "    head_folder_name = \"C:/Users/Mimran/OneDrive - George Mason University/C4I PC Backup/SCITE/RCPs Fifth Quarter/RCP14/Dev/Mohanad July 23 Pop\"\n",
    "    full_filename = \"{}/Week{:0>3}/Week{}_{:0>3}.csv\".format(head_folder_name, \n",
    "                                                     week_number, week_number,\n",
    "                                                     simulated_org_number)\n",
    "    return full_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/Mimran/OneDrive - George Mason University/C4I PC Backup/SCITE/RCPs Fifth Quarter/RCP14/Dev/Mohanad July 23 Pop/Week005/Week5_020.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_week_data_filename_QD(5, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_week_data_QD(week_number, simulated_org_number):\n",
    "    full_filename = get_week_data_filename_QD(week_number, simulated_org_number)\n",
    "    week_df = pd.read_csv(full_filename, usecols = list(range(1,67))) # Note this assumes similar order of users everywhere\n",
    "    week_df.replace([-np.inf,np.inf], np.nan, inplace=True) #(no matching of users is necessary *under this assumption*)\n",
    "    return week_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_list = read_week_data_QD(34, 4).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['001a',\n",
       " '001b',\n",
       " '001c',\n",
       " '014a',\n",
       " '015a',\n",
       " '021a',\n",
       " '021d',\n",
       " '021e',\n",
       " '021f',\n",
       " '021g',\n",
       " '021h',\n",
       " '021i',\n",
       " '021j',\n",
       " '022a',\n",
       " '022d',\n",
       " '022e',\n",
       " '022f',\n",
       " '022g',\n",
       " '022h',\n",
       " '022i',\n",
       " '022j',\n",
       " '027a',\n",
       " '027d',\n",
       " '027e',\n",
       " '027f',\n",
       " '027g',\n",
       " '027h',\n",
       " '027i',\n",
       " '027j',\n",
       " '028a',\n",
       " '028d',\n",
       " '028e',\n",
       " '028f',\n",
       " '028g',\n",
       " '028h',\n",
       " '028i',\n",
       " '028j',\n",
       " '029a',\n",
       " '030a',\n",
       " '031a',\n",
       " '032a',\n",
       " '033a',\n",
       " '034a',\n",
       " '035a',\n",
       " '036a',\n",
       " '037a',\n",
       " '038a',\n",
       " '039a',\n",
       " '040a',\n",
       " '041a',\n",
       " '042a',\n",
       " '043a',\n",
       " '044a',\n",
       " '045a',\n",
       " '046a',\n",
       " '047a',\n",
       " '048a',\n",
       " '049a',\n",
       " '050a',\n",
       " '051a',\n",
       " '052a',\n",
       " '053a',\n",
       " '058a',\n",
       " '059a',\n",
       " '060a']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "det_names = feature_list[1:]\n",
    "det_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ES = keras.callbacks.EarlyStopping(monitor = 'val_loss', min_delta = 10 ** (-5), patience = 2)\n",
    "\n",
    "def create_keras_model():\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(325, input_shape = (325,), activation='tanh'))\n",
    "    model.add(keras.layers.Dense(325, activation='tanh'))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer = 'sgd', loss = 'binary_crossentropy')\n",
    "    return model    \n",
    "\n",
    "def get_model(S1_x, S1_y, S2_x, S2_y, test_class_weight):\n",
    "    current_model = create_keras_model()\n",
    "    current_model.fit(S1_x, S1_y, \n",
    "                      callbacks = [ES], \n",
    "                      validation_data = (S2_x, S2_y), \n",
    "                      epochs = 100, \n",
    "                      class_weight = {0:1, 1:test_class_weight}, verbose = 0)\n",
    "    return current_model\n",
    "\n",
    "def get_model_predictions(S1_x, S1_y, S2_x, S2_y, test_class_weight, S3_x):\n",
    "    current_model = create_keras_model()\n",
    "    current_model.fit(S1_x, S1_y, \n",
    "                      callbacks = [ES], \n",
    "                      validation_data = (S2_x, S2_y), \n",
    "                      epochs = 100, \n",
    "                      class_weight = {0:1, 1:test_class_weight})\n",
    "    return current_model.predict(S3_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['001a_t-2',\n",
       " '001a_t-1',\n",
       " '001a_t',\n",
       " '001a_t+1',\n",
       " '001a_t+2',\n",
       " '001b_t-2',\n",
       " '001b_t-1',\n",
       " '001b_t',\n",
       " '001b_t+1',\n",
       " '001b_t+2',\n",
       " '001c_t-2',\n",
       " '001c_t-1',\n",
       " '001c_t',\n",
       " '001c_t+1',\n",
       " '001c_t+2',\n",
       " '014a_t-2',\n",
       " '014a_t-1',\n",
       " '014a_t',\n",
       " '014a_t+1',\n",
       " '014a_t+2',\n",
       " '015a_t-2',\n",
       " '015a_t-1',\n",
       " '015a_t',\n",
       " '015a_t+1',\n",
       " '015a_t+2',\n",
       " '021a_t-2',\n",
       " '021a_t-1',\n",
       " '021a_t',\n",
       " '021a_t+1',\n",
       " '021a_t+2',\n",
       " '021d_t-2',\n",
       " '021d_t-1',\n",
       " '021d_t',\n",
       " '021d_t+1',\n",
       " '021d_t+2',\n",
       " '021e_t-2',\n",
       " '021e_t-1',\n",
       " '021e_t',\n",
       " '021e_t+1',\n",
       " '021e_t+2',\n",
       " '021f_t-2',\n",
       " '021f_t-1',\n",
       " '021f_t',\n",
       " '021f_t+1',\n",
       " '021f_t+2',\n",
       " '021g_t-2',\n",
       " '021g_t-1',\n",
       " '021g_t',\n",
       " '021g_t+1',\n",
       " '021g_t+2',\n",
       " '021h_t-2',\n",
       " '021h_t-1',\n",
       " '021h_t',\n",
       " '021h_t+1',\n",
       " '021h_t+2',\n",
       " '021i_t-2',\n",
       " '021i_t-1',\n",
       " '021i_t',\n",
       " '021i_t+1',\n",
       " '021i_t+2',\n",
       " '021j_t-2',\n",
       " '021j_t-1',\n",
       " '021j_t',\n",
       " '021j_t+1',\n",
       " '021j_t+2',\n",
       " '022a_t-2',\n",
       " '022a_t-1',\n",
       " '022a_t',\n",
       " '022a_t+1',\n",
       " '022a_t+2',\n",
       " '022d_t-2',\n",
       " '022d_t-1',\n",
       " '022d_t',\n",
       " '022d_t+1',\n",
       " '022d_t+2',\n",
       " '022e_t-2',\n",
       " '022e_t-1',\n",
       " '022e_t',\n",
       " '022e_t+1',\n",
       " '022e_t+2',\n",
       " '022f_t-2',\n",
       " '022f_t-1',\n",
       " '022f_t',\n",
       " '022f_t+1',\n",
       " '022f_t+2',\n",
       " '022g_t-2',\n",
       " '022g_t-1',\n",
       " '022g_t',\n",
       " '022g_t+1',\n",
       " '022g_t+2',\n",
       " '022h_t-2',\n",
       " '022h_t-1',\n",
       " '022h_t',\n",
       " '022h_t+1',\n",
       " '022h_t+2',\n",
       " '022i_t-2',\n",
       " '022i_t-1',\n",
       " '022i_t',\n",
       " '022i_t+1',\n",
       " '022i_t+2',\n",
       " '022j_t-2',\n",
       " '022j_t-1',\n",
       " '022j_t',\n",
       " '022j_t+1',\n",
       " '022j_t+2',\n",
       " '027a_t-2',\n",
       " '027a_t-1',\n",
       " '027a_t',\n",
       " '027a_t+1',\n",
       " '027a_t+2',\n",
       " '027d_t-2',\n",
       " '027d_t-1',\n",
       " '027d_t',\n",
       " '027d_t+1',\n",
       " '027d_t+2',\n",
       " '027e_t-2',\n",
       " '027e_t-1',\n",
       " '027e_t',\n",
       " '027e_t+1',\n",
       " '027e_t+2',\n",
       " '027f_t-2',\n",
       " '027f_t-1',\n",
       " '027f_t',\n",
       " '027f_t+1',\n",
       " '027f_t+2',\n",
       " '027g_t-2',\n",
       " '027g_t-1',\n",
       " '027g_t',\n",
       " '027g_t+1',\n",
       " '027g_t+2',\n",
       " '027h_t-2',\n",
       " '027h_t-1',\n",
       " '027h_t',\n",
       " '027h_t+1',\n",
       " '027h_t+2',\n",
       " '027i_t-2',\n",
       " '027i_t-1',\n",
       " '027i_t',\n",
       " '027i_t+1',\n",
       " '027i_t+2',\n",
       " '027j_t-2',\n",
       " '027j_t-1',\n",
       " '027j_t',\n",
       " '027j_t+1',\n",
       " '027j_t+2',\n",
       " '028a_t-2',\n",
       " '028a_t-1',\n",
       " '028a_t',\n",
       " '028a_t+1',\n",
       " '028a_t+2',\n",
       " '028d_t-2',\n",
       " '028d_t-1',\n",
       " '028d_t',\n",
       " '028d_t+1',\n",
       " '028d_t+2',\n",
       " '028e_t-2',\n",
       " '028e_t-1',\n",
       " '028e_t',\n",
       " '028e_t+1',\n",
       " '028e_t+2',\n",
       " '028f_t-2',\n",
       " '028f_t-1',\n",
       " '028f_t',\n",
       " '028f_t+1',\n",
       " '028f_t+2',\n",
       " '028g_t-2',\n",
       " '028g_t-1',\n",
       " '028g_t',\n",
       " '028g_t+1',\n",
       " '028g_t+2',\n",
       " '028h_t-2',\n",
       " '028h_t-1',\n",
       " '028h_t',\n",
       " '028h_t+1',\n",
       " '028h_t+2',\n",
       " '028i_t-2',\n",
       " '028i_t-1',\n",
       " '028i_t',\n",
       " '028i_t+1',\n",
       " '028i_t+2',\n",
       " '028j_t-2',\n",
       " '028j_t-1',\n",
       " '028j_t',\n",
       " '028j_t+1',\n",
       " '028j_t+2',\n",
       " '029a_t-2',\n",
       " '029a_t-1',\n",
       " '029a_t',\n",
       " '029a_t+1',\n",
       " '029a_t+2',\n",
       " '030a_t-2',\n",
       " '030a_t-1',\n",
       " '030a_t',\n",
       " '030a_t+1',\n",
       " '030a_t+2',\n",
       " '031a_t-2',\n",
       " '031a_t-1',\n",
       " '031a_t',\n",
       " '031a_t+1',\n",
       " '031a_t+2',\n",
       " '032a_t-2',\n",
       " '032a_t-1',\n",
       " '032a_t',\n",
       " '032a_t+1',\n",
       " '032a_t+2',\n",
       " '033a_t-2',\n",
       " '033a_t-1',\n",
       " '033a_t',\n",
       " '033a_t+1',\n",
       " '033a_t+2',\n",
       " '034a_t-2',\n",
       " '034a_t-1',\n",
       " '034a_t',\n",
       " '034a_t+1',\n",
       " '034a_t+2',\n",
       " '035a_t-2',\n",
       " '035a_t-1',\n",
       " '035a_t',\n",
       " '035a_t+1',\n",
       " '035a_t+2',\n",
       " '036a_t-2',\n",
       " '036a_t-1',\n",
       " '036a_t',\n",
       " '036a_t+1',\n",
       " '036a_t+2',\n",
       " '037a_t-2',\n",
       " '037a_t-1',\n",
       " '037a_t',\n",
       " '037a_t+1',\n",
       " '037a_t+2',\n",
       " '038a_t-2',\n",
       " '038a_t-1',\n",
       " '038a_t',\n",
       " '038a_t+1',\n",
       " '038a_t+2',\n",
       " '039a_t-2',\n",
       " '039a_t-1',\n",
       " '039a_t',\n",
       " '039a_t+1',\n",
       " '039a_t+2',\n",
       " '040a_t-2',\n",
       " '040a_t-1',\n",
       " '040a_t',\n",
       " '040a_t+1',\n",
       " '040a_t+2',\n",
       " '041a_t-2',\n",
       " '041a_t-1',\n",
       " '041a_t',\n",
       " '041a_t+1',\n",
       " '041a_t+2',\n",
       " '042a_t-2',\n",
       " '042a_t-1',\n",
       " '042a_t',\n",
       " '042a_t+1',\n",
       " '042a_t+2',\n",
       " '043a_t-2',\n",
       " '043a_t-1',\n",
       " '043a_t',\n",
       " '043a_t+1',\n",
       " '043a_t+2',\n",
       " '044a_t-2',\n",
       " '044a_t-1',\n",
       " '044a_t',\n",
       " '044a_t+1',\n",
       " '044a_t+2',\n",
       " '045a_t-2',\n",
       " '045a_t-1',\n",
       " '045a_t',\n",
       " '045a_t+1',\n",
       " '045a_t+2',\n",
       " '046a_t-2',\n",
       " '046a_t-1',\n",
       " '046a_t',\n",
       " '046a_t+1',\n",
       " '046a_t+2',\n",
       " '047a_t-2',\n",
       " '047a_t-1',\n",
       " '047a_t',\n",
       " '047a_t+1',\n",
       " '047a_t+2',\n",
       " '048a_t-2',\n",
       " '048a_t-1',\n",
       " '048a_t',\n",
       " '048a_t+1',\n",
       " '048a_t+2',\n",
       " '049a_t-2',\n",
       " '049a_t-1',\n",
       " '049a_t',\n",
       " '049a_t+1',\n",
       " '049a_t+2',\n",
       " '050a_t-2',\n",
       " '050a_t-1',\n",
       " '050a_t',\n",
       " '050a_t+1',\n",
       " '050a_t+2',\n",
       " '051a_t-2',\n",
       " '051a_t-1',\n",
       " '051a_t',\n",
       " '051a_t+1',\n",
       " '051a_t+2',\n",
       " '052a_t-2',\n",
       " '052a_t-1',\n",
       " '052a_t',\n",
       " '052a_t+1',\n",
       " '052a_t+2',\n",
       " '053a_t-2',\n",
       " '053a_t-1',\n",
       " '053a_t',\n",
       " '053a_t+1',\n",
       " '053a_t+2',\n",
       " '058a_t-2',\n",
       " '058a_t-1',\n",
       " '058a_t',\n",
       " '058a_t+1',\n",
       " '058a_t+2',\n",
       " '059a_t-2',\n",
       " '059a_t-1',\n",
       " '059a_t',\n",
       " '059a_t+1',\n",
       " '059a_t+2',\n",
       " '060a_t-2',\n",
       " '060a_t-1',\n",
       " '060a_t',\n",
       " '060a_t+1',\n",
       " '060a_t+2']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "det_names_in_correct_order = [detector + time_period for detector in det_names for time_period in [\"_t-2\", \"_t-1\", \"_t\", \"_t+1\", \"_t+2\"] ]\n",
    "det_names_in_correct_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_trait_names = [\"trait_\" + str(trait_num) for trait_num in range(4, 21, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_org_data(simulated_org_number, first_full_week, last_full_week):\n",
    "    all_full_week_dfs = []\n",
    "    list_of_dfs_for_feature_vectors = [read_week_data_QD(week_number, simulated_org_number) for week_number in range(first_full_week - 2, first_full_week + 2)]\n",
    "    for current_week in range(first_full_week, last_full_week + 1):\n",
    "        list_of_dfs_for_feature_vectors.append(read_week_data_QD(current_week + 2, simulated_org_number))\n",
    "        current_week_df = pd.concat([list_of_dfs_for_feature_vectors[0].rename(columns = lambda some_str: some_str + \"_t-2\"), \n",
    "                                     list_of_dfs_for_feature_vectors[1].rename(columns = lambda some_str: some_str + \"_t-1\"), \n",
    "                                     list_of_dfs_for_feature_vectors[2].rename(columns = lambda some_str: some_str + \"_t\"), \n",
    "                                     list_of_dfs_for_feature_vectors[3].rename(columns = lambda some_str: some_str + \"_t+1\"), \n",
    "                                     list_of_dfs_for_feature_vectors[4].rename(columns = lambda some_str: some_str + \"_t+2\")], \n",
    "                                    axis = 1)\n",
    "        current_week_df.dropna(inplace=True)\n",
    "        all_full_week_dfs.append(current_week_df)\n",
    "        del list_of_dfs_for_feature_vectors[0]\n",
    "    return pd.concat(all_full_week_dfs)[det_names_in_correct_order + ['Target_t-2',\n",
    "                                                                              'Target_t-1',\n",
    "                                                                              'Target_t',\n",
    "                                                                              'Target_t+1',\n",
    "                                                                              'Target_t+2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_training_data(sample_training_data_df):\n",
    "    S12_x, S3_x, S12_y, S3_y = train_test_split(sample_training_data_df.drop(['Target_t-2',\n",
    "                                                                              'Target_t-1',\n",
    "                                                                              'Target_t',\n",
    "                                                                              'Target_t+1',\n",
    "                                                                              'Target_t+2'], 1),\n",
    "                                                sample_training_data_df['Target_t'],\n",
    "                                                test_size = 0.2, stratify = sample_training_data_df['Target_t'])\n",
    "    scaler = StandardScaler().fit(S12_x)\n",
    "    S12_x = scaler.transform(S12_x)\n",
    "    S3_x = scaler.transform(S3_x)\n",
    "    S1_x, S2_x, S1_y, S2_y = train_test_split(S12_x, S12_y, test_size = 0.125, stratify = S12_y)\n",
    "    return S1_x, S2_x, S3_x, S1_y.values, S2_y.values, S3_y.values, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_org_test_data(simulated_org_number, first_full_week, last_full_week):\n",
    "    all_full_week_dfs = []\n",
    "    list_of_dfs_for_feature_vectors = [read_week_data_QD(week_number, simulated_org_number) for week_number in range(first_full_week - 2, first_full_week + 2)]\n",
    "    for current_week in range(first_full_week, last_full_week + 1):\n",
    "        list_of_dfs_for_feature_vectors.append(read_week_data_QD(current_week + 2, simulated_org_number))\n",
    "        current_week_df = pd.concat([list_of_dfs_for_feature_vectors[0].rename(columns = lambda some_str: some_str + \"_t-2\"), \n",
    "                                     list_of_dfs_for_feature_vectors[1].rename(columns = lambda some_str: some_str + \"_t-1\"), \n",
    "                                     list_of_dfs_for_feature_vectors[2].rename(columns = lambda some_str: some_str + \"_t\"), \n",
    "                                     list_of_dfs_for_feature_vectors[3].rename(columns = lambda some_str: some_str + \"_t+1\"), \n",
    "                                     list_of_dfs_for_feature_vectors[4].rename(columns = lambda some_str: some_str + \"_t+2\")], \n",
    "                                    axis = 1)\n",
    "        current_week_df.dropna(inplace=True)\n",
    "        detector_string = '021f'\n",
    "        current_week_df['trait_4'] = ((current_week_df['{}_t-2'.format(detector_string)] > np.percentile(current_week_df['{}_t-2'.format(detector_string)], 90)) | \n",
    "                                      (current_week_df['{}_t-1'.format(detector_string)] > np.percentile(current_week_df['{}_t-1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t'.format(detector_string)] > np.percentile(current_week_df['{}_t'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+1'.format(detector_string)] > np.percentile(current_week_df['{}_t+1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+2'.format(detector_string)] > np.percentile(current_week_df['{}_t+2'.format(detector_string)], 90)))\n",
    "        detector_string = '021h'\n",
    "        current_week_df['trait_6'] = ((current_week_df['{}_t-2'.format(detector_string)] > np.percentile(current_week_df['{}_t-2'.format(detector_string)], 90)) | \n",
    "                                      (current_week_df['{}_t-1'.format(detector_string)] > np.percentile(current_week_df['{}_t-1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t'.format(detector_string)] > np.percentile(current_week_df['{}_t'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+1'.format(detector_string)] > np.percentile(current_week_df['{}_t+1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+2'.format(detector_string)] > np.percentile(current_week_df['{}_t+2'.format(detector_string)], 90)))\n",
    "        detector_string = '022f'\n",
    "        current_week_df['trait_8'] = ((current_week_df['{}_t-2'.format(detector_string)] > np.percentile(current_week_df['{}_t-2'.format(detector_string)], 90)) | \n",
    "                                      (current_week_df['{}_t-1'.format(detector_string)] > np.percentile(current_week_df['{}_t-1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t'.format(detector_string)] > np.percentile(current_week_df['{}_t'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+1'.format(detector_string)] > np.percentile(current_week_df['{}_t+1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+2'.format(detector_string)] > np.percentile(current_week_df['{}_t+2'.format(detector_string)], 90)))\n",
    "        detector_string = '022h'\n",
    "        current_week_df['trait_10'] = ((current_week_df['{}_t-2'.format(detector_string)] > np.percentile(current_week_df['{}_t-2'.format(detector_string)], 90)) | \n",
    "                                      (current_week_df['{}_t-1'.format(detector_string)] > np.percentile(current_week_df['{}_t-1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t'.format(detector_string)] > np.percentile(current_week_df['{}_t'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+1'.format(detector_string)] > np.percentile(current_week_df['{}_t+1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+2'.format(detector_string)] > np.percentile(current_week_df['{}_t+2'.format(detector_string)], 90)))\n",
    "        detector_string = '027f'\n",
    "        current_week_df['trait_12'] = ((current_week_df['{}_t-2'.format(detector_string)] > np.percentile(current_week_df['{}_t-2'.format(detector_string)], 90)) | \n",
    "                                      (current_week_df['{}_t-1'.format(detector_string)] > np.percentile(current_week_df['{}_t-1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t'.format(detector_string)] > np.percentile(current_week_df['{}_t'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+1'.format(detector_string)] > np.percentile(current_week_df['{}_t+1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+2'.format(detector_string)] > np.percentile(current_week_df['{}_t+2'.format(detector_string)], 90)))\n",
    "        detector_string = '027h'\n",
    "        current_week_df['trait_14'] = ((current_week_df['{}_t-2'.format(detector_string)] > np.percentile(current_week_df['{}_t-2'.format(detector_string)], 90)) | \n",
    "                                      (current_week_df['{}_t-1'.format(detector_string)] > np.percentile(current_week_df['{}_t-1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t'.format(detector_string)] > np.percentile(current_week_df['{}_t'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+1'.format(detector_string)] > np.percentile(current_week_df['{}_t+1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+2'.format(detector_string)] > np.percentile(current_week_df['{}_t+2'.format(detector_string)], 90)))\n",
    "        detector_string = '028f'\n",
    "        current_week_df['trait_16'] = ((current_week_df['{}_t-2'.format(detector_string)] > np.percentile(current_week_df['{}_t-2'.format(detector_string)], 90)) | \n",
    "                                      (current_week_df['{}_t-1'.format(detector_string)] > np.percentile(current_week_df['{}_t-1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t'.format(detector_string)] > np.percentile(current_week_df['{}_t'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+1'.format(detector_string)] > np.percentile(current_week_df['{}_t+1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+2'.format(detector_string)] > np.percentile(current_week_df['{}_t+2'.format(detector_string)], 90)))\n",
    "        detector_string = '028h'\n",
    "        current_week_df['trait_18'] = ((current_week_df['{}_t-2'.format(detector_string)] > np.percentile(current_week_df['{}_t-2'.format(detector_string)], 90)) | \n",
    "                                      (current_week_df['{}_t-1'.format(detector_string)] > np.percentile(current_week_df['{}_t-1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t'.format(detector_string)] > np.percentile(current_week_df['{}_t'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+1'.format(detector_string)] > np.percentile(current_week_df['{}_t+1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+2'.format(detector_string)] > np.percentile(current_week_df['{}_t+2'.format(detector_string)], 90)))\n",
    "        detector_string = '058a'\n",
    "        current_week_df['trait_20'] = ((current_week_df['{}_t-2'.format(detector_string)]).astype(int) | \n",
    "                                      (current_week_df['{}_t-1'.format(detector_string)]).astype(int) |\n",
    "                                      (current_week_df['{}_t'.format(detector_string)]).astype(int) |\n",
    "                                      (current_week_df['{}_t+1'.format(detector_string)]).astype(int) |\n",
    "                                      (current_week_df['{}_t+2'.format(detector_string)]).astype(int))\n",
    "        all_full_week_dfs.append(current_week_df)\n",
    "        del list_of_dfs_for_feature_vectors[0]\n",
    "    return pd.concat(all_full_week_dfs)[det_names_in_correct_order + ['Target_t-2',\n",
    "                                                                              'Target_t-1',\n",
    "                                                                              'Target_t',\n",
    "                                                                              'Target_t+1',\n",
    "                                                                              'Target_t+2'] + all_trait_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = read_org_data(4, 7, 33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>001a_t-2</th>\n",
       "      <th>001a_t-1</th>\n",
       "      <th>001a_t</th>\n",
       "      <th>001a_t+1</th>\n",
       "      <th>001a_t+2</th>\n",
       "      <th>001b_t-2</th>\n",
       "      <th>001b_t-1</th>\n",
       "      <th>001b_t</th>\n",
       "      <th>001b_t+1</th>\n",
       "      <th>001b_t+2</th>\n",
       "      <th>...</th>\n",
       "      <th>060a_t-2</th>\n",
       "      <th>060a_t-1</th>\n",
       "      <th>060a_t</th>\n",
       "      <th>060a_t+1</th>\n",
       "      <th>060a_t+2</th>\n",
       "      <th>Target_t-2</th>\n",
       "      <th>Target_t-1</th>\n",
       "      <th>Target_t</th>\n",
       "      <th>Target_t+1</th>\n",
       "      <th>Target_t+2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91647.0</td>\n",
       "      <td>27684.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99559.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13990.0</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27571.0</td>\n",
       "      <td>33708.0</td>\n",
       "      <td>94522.0</td>\n",
       "      <td>62116.0</td>\n",
       "      <td>1783.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>29</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>61715.0</td>\n",
       "      <td>21997.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9995.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51320.0</td>\n",
       "      <td>45063.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7292.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26547.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66372.0</td>\n",
       "      <td>25894.0</td>\n",
       "      <td>51525.0</td>\n",
       "      <td>2889.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1825.0</td>\n",
       "      <td>13571.0</td>\n",
       "      <td>49611.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33367.0</td>\n",
       "      <td>37137.0</td>\n",
       "      <td>73056.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>171270.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26857.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3037.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>163290.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7204.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30155.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>64394.0</td>\n",
       "      <td>81503.0</td>\n",
       "      <td>69709.0</td>\n",
       "      <td>101300.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65600.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>194200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14739.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58333.0</td>\n",
       "      <td>28951.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14922.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57559.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>49526.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>133950.0</td>\n",
       "      <td>44800.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3428.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>299030.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>189050.0</td>\n",
       "      <td>244870.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28193.0</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>129320.0</td>\n",
       "      <td>43932.0</td>\n",
       "      <td>3668.0</td>\n",
       "      <td>435870.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>173370.0</td>\n",
       "      <td>13923.0</td>\n",
       "      <td>29384.0</td>\n",
       "      <td>53757.0</td>\n",
       "      <td>178620.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18895.0</td>\n",
       "      <td>13640.0</td>\n",
       "      <td>14576.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>66737.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1702.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6233.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>53305.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4105.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6096.0</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>70368.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40830.0</td>\n",
       "      <td>29447.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13277.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>28394.0</td>\n",
       "      <td>19702.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>215080.0</td>\n",
       "      <td>164310.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>149620.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37245.0</td>\n",
       "      <td>...</td>\n",
       "      <td>43</td>\n",
       "      <td>26</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8952.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>136540.0</td>\n",
       "      <td>25202.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30828.0</td>\n",
       "      <td>7785.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>86930.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69601.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20923.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>48</td>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9971.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11885.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1134.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28682.0</td>\n",
       "      <td>21029.0</td>\n",
       "      <td>23849.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59946.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>14193.0</td>\n",
       "      <td>46501.0</td>\n",
       "      <td>15701.0</td>\n",
       "      <td>160630.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28764.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>88907.0</td>\n",
       "      <td>27816.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58470.0</td>\n",
       "      <td>9212.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63448.0</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>42076.0</td>\n",
       "      <td>66445.0</td>\n",
       "      <td>12849.0</td>\n",
       "      <td>45446.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>14579.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>335330.0</td>\n",
       "      <td>279950.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3698.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>40703.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39776.0</td>\n",
       "      <td>724280.0</td>\n",
       "      <td>38300.0</td>\n",
       "      <td>29052.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>46295.0</td>\n",
       "      <td>20749.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15592.0</td>\n",
       "      <td>134430.0</td>\n",
       "      <td>10589.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>19</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94081.0</td>\n",
       "      <td>30813.0</td>\n",
       "      <td>39412.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28260.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>38065.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14044.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>9549.0</td>\n",
       "      <td>33920.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22187.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>46</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3067</th>\n",
       "      <td>15013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37068.0</td>\n",
       "      <td>27717.0</td>\n",
       "      <td>20255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5368.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2329.0</td>\n",
       "      <td>5189.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3068</th>\n",
       "      <td>35487.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6247.0</td>\n",
       "      <td>14532.0</td>\n",
       "      <td>5597.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3089.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3069</th>\n",
       "      <td>74356.0</td>\n",
       "      <td>11696.0</td>\n",
       "      <td>608520.0</td>\n",
       "      <td>114030.0</td>\n",
       "      <td>45686.0</td>\n",
       "      <td>8053.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3334.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3070</th>\n",
       "      <td>4125.0</td>\n",
       "      <td>8407.0</td>\n",
       "      <td>4380.0</td>\n",
       "      <td>12157.0</td>\n",
       "      <td>10871.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2354.0</td>\n",
       "      <td>7648.0</td>\n",
       "      <td>7274.0</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3071</th>\n",
       "      <td>4146.0</td>\n",
       "      <td>4918.0</td>\n",
       "      <td>12548.0</td>\n",
       "      <td>6147.0</td>\n",
       "      <td>3383.0</td>\n",
       "      <td>8193.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9452.0</td>\n",
       "      <td>204950.0</td>\n",
       "      <td>4378.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3072</th>\n",
       "      <td>1572.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18326.0</td>\n",
       "      <td>29550.0</td>\n",
       "      <td>51485.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5929.0</td>\n",
       "      <td>14248.0</td>\n",
       "      <td>...</td>\n",
       "      <td>37</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3073</th>\n",
       "      <td>10798.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2735.0</td>\n",
       "      <td>5292.0</td>\n",
       "      <td>7409.0</td>\n",
       "      <td>1817.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3806.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3074</th>\n",
       "      <td>96727.0</td>\n",
       "      <td>27556.0</td>\n",
       "      <td>51463.0</td>\n",
       "      <td>40874.0</td>\n",
       "      <td>27524.0</td>\n",
       "      <td>2263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1833.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3075</th>\n",
       "      <td>84402.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35613.0</td>\n",
       "      <td>12251.0</td>\n",
       "      <td>19851.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4625.0</td>\n",
       "      <td>5008.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3076</th>\n",
       "      <td>51555.0</td>\n",
       "      <td>29729.0</td>\n",
       "      <td>77848.0</td>\n",
       "      <td>11589.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1116.0</td>\n",
       "      <td>7049.0</td>\n",
       "      <td>5042.0</td>\n",
       "      <td>3490.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3077</th>\n",
       "      <td>22751.0</td>\n",
       "      <td>15231.0</td>\n",
       "      <td>51676.0</td>\n",
       "      <td>29463.0</td>\n",
       "      <td>29714.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23260.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1718.0</td>\n",
       "      <td>8112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3078</th>\n",
       "      <td>6485.0</td>\n",
       "      <td>7251.0</td>\n",
       "      <td>19067.0</td>\n",
       "      <td>14487.0</td>\n",
       "      <td>30080.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4282.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3079</th>\n",
       "      <td>50306.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>188640.0</td>\n",
       "      <td>232270.0</td>\n",
       "      <td>105630.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69109.0</td>\n",
       "      <td>30160.0</td>\n",
       "      <td>49517.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3080</th>\n",
       "      <td>1982.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33926.0</td>\n",
       "      <td>35205.0</td>\n",
       "      <td>6170.0</td>\n",
       "      <td>2760.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3001.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3081</th>\n",
       "      <td>1740.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10587.0</td>\n",
       "      <td>22606.0</td>\n",
       "      <td>14396.0</td>\n",
       "      <td>5866.0</td>\n",
       "      <td>5529.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>26</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3082</th>\n",
       "      <td>14274.0</td>\n",
       "      <td>8060.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14949.0</td>\n",
       "      <td>19791.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4154.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083</th>\n",
       "      <td>12115.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21859.0</td>\n",
       "      <td>49142.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2934.0</td>\n",
       "      <td>9971.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3084</th>\n",
       "      <td>15364.0</td>\n",
       "      <td>16457.0</td>\n",
       "      <td>47311.0</td>\n",
       "      <td>47215.0</td>\n",
       "      <td>36610.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3125.0</td>\n",
       "      <td>4630.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3085</th>\n",
       "      <td>22905.0</td>\n",
       "      <td>28299.0</td>\n",
       "      <td>21328.0</td>\n",
       "      <td>41321.0</td>\n",
       "      <td>17515.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4716.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3086</th>\n",
       "      <td>3602.0</td>\n",
       "      <td>1942.0</td>\n",
       "      <td>116290.0</td>\n",
       "      <td>143980.0</td>\n",
       "      <td>28204.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5462.0</td>\n",
       "      <td>1157.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3087</th>\n",
       "      <td>34299.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14303.0</td>\n",
       "      <td>15858.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8696.0</td>\n",
       "      <td>46802.0</td>\n",
       "      <td>10726.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3088</th>\n",
       "      <td>8286.0</td>\n",
       "      <td>20795.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2447.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>769.0</td>\n",
       "      <td>11077.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3089</th>\n",
       "      <td>54345.0</td>\n",
       "      <td>6826.0</td>\n",
       "      <td>93021.0</td>\n",
       "      <td>49143.0</td>\n",
       "      <td>38281.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2329.0</td>\n",
       "      <td>6966.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3090</th>\n",
       "      <td>2919.0</td>\n",
       "      <td>18077.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7066.0</td>\n",
       "      <td>2968.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14659.0</td>\n",
       "      <td>17067.0</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>15</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3091</th>\n",
       "      <td>24479.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9491.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3092</th>\n",
       "      <td>48091.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25764.0</td>\n",
       "      <td>27742.0</td>\n",
       "      <td>27264.0</td>\n",
       "      <td>6783.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75596.0</td>\n",
       "      <td>79734.0</td>\n",
       "      <td>5912.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3093</th>\n",
       "      <td>11413.0</td>\n",
       "      <td>17989.0</td>\n",
       "      <td>16547.0</td>\n",
       "      <td>16800.0</td>\n",
       "      <td>49594.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1541.0</td>\n",
       "      <td>5864.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>38</td>\n",
       "      <td>41</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3094</th>\n",
       "      <td>4576.0</td>\n",
       "      <td>28229.0</td>\n",
       "      <td>30726.0</td>\n",
       "      <td>105530.0</td>\n",
       "      <td>65131.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>587.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3095</th>\n",
       "      <td>13421.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6615.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2408.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096</th>\n",
       "      <td>41903.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6439.0</td>\n",
       "      <td>7614.0</td>\n",
       "      <td>7399.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2403.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83619 rows × 330 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      001a_t-2  001a_t-1    001a_t  001a_t+1  001a_t+2  001b_t-2  001b_t-1  \\\n",
       "0      91647.0   27684.0       0.0       0.0   99559.0       0.0       0.0   \n",
       "1      27571.0   33708.0   94522.0   62116.0    1783.0       0.0       0.0   \n",
       "2          0.0   61715.0   21997.0       0.0    9995.0       0.0   51320.0   \n",
       "3      26547.0       0.0   66372.0   25894.0   51525.0    2889.0       0.0   \n",
       "4      33367.0   37137.0   73056.0       0.0  171270.0       0.0       0.0   \n",
       "5          0.0       0.0       0.0  163290.0       0.0    7204.0       0.0   \n",
       "6          0.0   64394.0   81503.0   69709.0  101300.0       0.0       0.0   \n",
       "7     194200.0       0.0   14739.0       0.0   58333.0   28951.0       0.0   \n",
       "8          0.0   49526.0       0.0  133950.0   44800.0       0.0       0.0   \n",
       "9     299030.0       0.0       0.0       0.0  189050.0  244870.0       0.0   \n",
       "10    129320.0   43932.0    3668.0  435870.0       0.0       0.0       0.0   \n",
       "11    173370.0   13923.0   29384.0   53757.0  178620.0       0.0   18895.0   \n",
       "12     66737.0       0.0       0.0       0.0    1702.0       0.0       0.0   \n",
       "13         0.0    2139.0       0.0    6233.0       0.0       0.0       0.0   \n",
       "14         0.0   53305.0       0.0       0.0    4105.0       0.0       0.0   \n",
       "15     70368.0       0.0       0.0       0.0   40830.0   29447.0       0.0   \n",
       "16     28394.0   19702.0       0.0  215080.0  164310.0       0.0  149620.0   \n",
       "17      8952.0       0.0       0.0  136540.0   25202.0       0.0       0.0   \n",
       "18     86930.0       0.0       0.0   69601.0       0.0       0.0       0.0   \n",
       "19         0.0    9971.0       0.0       0.0   11885.0       0.0       0.0   \n",
       "20      1134.0       0.0   28682.0   21029.0   23849.0       0.0       0.0   \n",
       "21     14193.0   46501.0   15701.0  160630.0       0.0       0.0       0.0   \n",
       "22     88907.0   27816.0       0.0       0.0   58470.0    9212.0       0.0   \n",
       "23         0.0   42076.0   66445.0   12849.0   45446.0       0.0       0.0   \n",
       "24         0.0   14579.0       0.0  335330.0  279950.0       0.0       0.0   \n",
       "25     40703.0       0.0   39776.0  724280.0   38300.0   29052.0       0.0   \n",
       "26     46295.0   20749.0       0.0   15592.0  134430.0   10589.0       0.0   \n",
       "27         0.0       0.0   94081.0   30813.0   39412.0       0.0       0.0   \n",
       "28         0.0   38065.0      36.0       0.0   14044.0       0.0       0.0   \n",
       "29      9549.0   33920.0       0.0       0.0   22187.0       0.0       0.0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3067   15013.0       0.0   37068.0   27717.0   20255.0       0.0    5368.0   \n",
       "3068   35487.0       0.0       0.0    6247.0   14532.0    5597.0       0.0   \n",
       "3069   74356.0   11696.0  608520.0  114030.0   45686.0    8053.0       0.0   \n",
       "3070    4125.0    8407.0    4380.0   12157.0   10871.0       0.0       0.0   \n",
       "3071    4146.0    4918.0   12548.0    6147.0    3383.0    8193.0       0.0   \n",
       "3072    1572.0       0.0   18326.0   29550.0   51485.0       0.0       0.0   \n",
       "3073   10798.0       0.0       0.0    2735.0    5292.0    7409.0    1817.0   \n",
       "3074   96727.0   27556.0   51463.0   40874.0   27524.0    2263.0       0.0   \n",
       "3075   84402.0       0.0   35613.0   12251.0   19851.0       0.0       0.0   \n",
       "3076   51555.0   29729.0   77848.0   11589.0       0.0    1116.0    7049.0   \n",
       "3077   22751.0   15231.0   51676.0   29463.0   29714.0       0.0   23260.0   \n",
       "3078    6485.0    7251.0   19067.0   14487.0   30080.0       0.0    4282.0   \n",
       "3079   50306.0       0.0  188640.0  232270.0  105630.0       0.0       0.0   \n",
       "3080    1982.0       0.0       0.0   33926.0   35205.0    6170.0    2760.0   \n",
       "3081    1740.0       0.0   10587.0   22606.0   14396.0    5866.0    5529.0   \n",
       "3082   14274.0    8060.0       0.0   14949.0   19791.0       0.0    4154.0   \n",
       "3083   12115.0       0.0       0.0   21859.0   49142.0       0.0       0.0   \n",
       "3084   15364.0   16457.0   47311.0   47215.0   36610.0       0.0       0.0   \n",
       "3085   22905.0   28299.0   21328.0   41321.0   17515.0       0.0       0.0   \n",
       "3086    3602.0    1942.0  116290.0  143980.0   28204.0       0.0       0.0   \n",
       "3087   34299.0       0.0   14303.0   15858.0       0.0       0.0       0.0   \n",
       "3088    8286.0   20795.0       0.0    2447.0       0.0       0.0       0.0   \n",
       "3089   54345.0    6826.0   93021.0   49143.0   38281.0       0.0       0.0   \n",
       "3090    2919.0   18077.0       0.0    7066.0    2968.0       0.0       0.0   \n",
       "3091   24479.0       0.0       0.0       0.0       0.0      62.0       0.0   \n",
       "3092   48091.0       0.0   25764.0   27742.0   27264.0    6783.0       0.0   \n",
       "3093   11413.0   17989.0   16547.0   16800.0   49594.0       0.0       0.0   \n",
       "3094    4576.0   28229.0   30726.0  105530.0   65131.0       0.0       0.0   \n",
       "3095   13421.0       0.0       0.0       0.0       0.0       0.0    6615.0   \n",
       "3096   41903.0       0.0       0.0    6439.0    7614.0    7399.0       0.0   \n",
       "\n",
       "       001b_t  001b_t+1  001b_t+2     ...      060a_t-2  060a_t-1  060a_t  \\\n",
       "0         0.0       0.0   13990.0     ...            29         6       5   \n",
       "1         0.0       0.0       0.0     ...             3        26      29   \n",
       "2     45063.0       0.0    7292.0     ...             2         6      11   \n",
       "3      1825.0   13571.0   49611.0     ...            13        23       1   \n",
       "4     26857.0       0.0    3037.0     ...            16        10       4   \n",
       "5         0.0   30155.0       0.0     ...            17         2      24   \n",
       "6         0.0   65600.0       0.0     ...            29         1      25   \n",
       "7     14922.0       0.0   57559.0     ...             1        12      10   \n",
       "8         0.0       0.0    3428.0     ...            23        10      25   \n",
       "9         0.0       0.0   28193.0     ...            30         8       3   \n",
       "10        0.0       0.0       0.0     ...            10        18      32   \n",
       "11    13640.0   14576.0       0.0     ...            11         1      17   \n",
       "12        0.0       0.0       0.0     ...             9         9       4   \n",
       "13        0.0       0.0       0.0     ...             4        12       1   \n",
       "14        0.0       0.0    6096.0     ...            17        13       6   \n",
       "15        0.0       0.0   13277.0     ...             8        15      36   \n",
       "16        0.0       0.0   37245.0     ...            43        26      17   \n",
       "17        0.0   30828.0    7785.0     ...            16         2       9   \n",
       "18        0.0   20923.0       0.0     ...             2        33      48   \n",
       "19        0.0       0.0       0.0     ...             1        30      16   \n",
       "20        0.0   59946.0       0.0     ...            38         2      38   \n",
       "21        0.0   28764.0       0.0     ...            19        11      15   \n",
       "22        0.0       0.0   63448.0     ...            29         4      29   \n",
       "23        0.0       0.0       0.0     ...             1        31      20   \n",
       "24     3698.0       0.0       0.0     ...             6         1       8   \n",
       "25        0.0       0.0       0.0     ...            26         2      31   \n",
       "26        0.0       0.0       0.0     ...             6        26      27   \n",
       "27        0.0   28260.0       0.0     ...            13         3      18   \n",
       "28        0.0       0.0       0.0     ...            13         1       1   \n",
       "29        0.0       0.0       0.0     ...            30        46       6   \n",
       "...       ...       ...       ...     ...           ...       ...     ...   \n",
       "3067      0.0    2329.0    5189.0     ...             2        28       2   \n",
       "3068      0.0    3089.0       0.0     ...             2        35       2   \n",
       "3069      0.0    3334.0       0.0     ...            52         6      20   \n",
       "3070   2354.0    7648.0    7274.0     ...            34         7       2   \n",
       "3071   9452.0  204950.0    4378.0     ...             2        12      16   \n",
       "3072      0.0    5929.0   14248.0     ...            37        10       5   \n",
       "3073      0.0    3806.0       0.0     ...             5         8      27   \n",
       "3074      0.0    1833.0       0.0     ...             2         2      29   \n",
       "3075   4625.0    5008.0       0.0     ...             7        23       1   \n",
       "3076   5042.0    3490.0       0.0     ...             5         1       2   \n",
       "3077      0.0    1718.0    8112.0     ...             1         9       1   \n",
       "3078      0.0       0.0       0.0     ...            34         6       6   \n",
       "3079  69109.0   30160.0   49517.0     ...             1        15       5   \n",
       "3080      0.0    3001.0       0.0     ...             6        34      28   \n",
       "3081      0.0    4174.0       0.0     ...            34        26      19   \n",
       "3082      0.0       0.0     750.0     ...            23        26       3   \n",
       "3083   2934.0    9971.0       0.0     ...            36         1       7   \n",
       "3084   3125.0    4630.0       0.0     ...             7         1       7   \n",
       "3085      0.0    4716.0     201.0     ...            16         7      14   \n",
       "3086   5462.0    1157.0       0.0     ...             1        31       3   \n",
       "3087   8696.0   46802.0   10726.0     ...             1         9      24   \n",
       "3088      0.0     769.0   11077.0     ...             7        16      14   \n",
       "3089   2329.0    6966.0       0.0     ...            30        29       3   \n",
       "3090      0.0   14659.0   17067.0     ...            27        15      29   \n",
       "3091      0.0    9491.0       0.0     ...             3        22      28   \n",
       "3092  75596.0   79734.0    5912.0     ...             2         2      19   \n",
       "3093      0.0    1541.0    5864.0     ...            12        14      38   \n",
       "3094      0.0     587.0       0.0     ...            10         9      13   \n",
       "3095      0.0    2408.0       0.0     ...             3        10      17   \n",
       "3096      0.0    2403.0       0.0     ...             1        29       7   \n",
       "\n",
       "      060a_t+1  060a_t+2  Target_t-2  Target_t-1  Target_t  Target_t+1  \\\n",
       "0            4        20           0           0         0           0   \n",
       "1            9         2           0           0         0           0   \n",
       "2           26         9           0           0         0           0   \n",
       "3            3         5           0           0         0           0   \n",
       "4           25        26           0           0         0           0   \n",
       "5            1        13           0           0         0           0   \n",
       "6           29         6           0           0         0           0   \n",
       "7            1        17           0           0         0           0   \n",
       "8            4         3           0           0         0           0   \n",
       "9           27         5           0           0         0           0   \n",
       "10           4        36           0           0         0           0   \n",
       "11           1         2           0           0         0           0   \n",
       "12           1        30           0           0         0           0   \n",
       "13           5         4           0           0         0           0   \n",
       "14           1         5           0           0         0           0   \n",
       "15           4        11           0           0         0           0   \n",
       "16          10        10           0           0         0           0   \n",
       "17          15         1           0           0         0           0   \n",
       "18          16        24           0           0         0           0   \n",
       "19          12         6           0           0         0           0   \n",
       "20           1        11           0           0         0           0   \n",
       "21          11        26           0           0         0           0   \n",
       "22           1         4           0           0         0           0   \n",
       "23           4         8           0           0         0           0   \n",
       "24          23        38           0           0         0           0   \n",
       "25           3         8           0           0         0           0   \n",
       "26          19        37           0           0         0           0   \n",
       "27           8        25           0           0         0           0   \n",
       "28          17        29           0           0         0           0   \n",
       "29          12         1           0           0         0           0   \n",
       "...        ...       ...         ...         ...       ...         ...   \n",
       "3067         2         3           0           0         0           0   \n",
       "3068         2         3           0           0         0           0   \n",
       "3069        20        20           0           0         0           0   \n",
       "3070         2         3           0           0         0           0   \n",
       "3071        16        16           0           0         0           0   \n",
       "3072         4         4           0           0         0           0   \n",
       "3073        27        27           0           0         0           0   \n",
       "3074        29        29           0           0         0           0   \n",
       "3075         1         2           0           0         0           0   \n",
       "3076         2         3           0           0         0           0   \n",
       "3077         2         3           0           0         0           0   \n",
       "3078         6         6           0           0         0           0   \n",
       "3079         4         4           0           0         0           0   \n",
       "3080        28        28           0           0         0           0   \n",
       "3081        18        18           0           0         0           0   \n",
       "3082         3        30           0           0         0           0   \n",
       "3083         7         7           0           0         0           0   \n",
       "3084         7         7           0           0         0           0   \n",
       "3085        14        14           0           0         0           0   \n",
       "3086         3         4           0           0         0           0   \n",
       "3087        24        21           0           0         0           0   \n",
       "3088        14        14           0           0         0           0   \n",
       "3089         3         4           0           0         0           0   \n",
       "3090        30        30           0           0         0           0   \n",
       "3091        28        28           0           0         0           0   \n",
       "3092        18        18           0           0         0           0   \n",
       "3093        41        38           0           0         0           0   \n",
       "3094        13        11           0           0         0           0   \n",
       "3095        17        16           0           0         0           0   \n",
       "3096         7         7           0           0         0           0   \n",
       "\n",
       "      Target_t+2  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  \n",
       "5              0  \n",
       "6              0  \n",
       "7              0  \n",
       "8              0  \n",
       "9              0  \n",
       "10             0  \n",
       "11             0  \n",
       "12             0  \n",
       "13             0  \n",
       "14             0  \n",
       "15             0  \n",
       "16             0  \n",
       "17             0  \n",
       "18             0  \n",
       "19             0  \n",
       "20             0  \n",
       "21             0  \n",
       "22             0  \n",
       "23             0  \n",
       "24             0  \n",
       "25             0  \n",
       "26             0  \n",
       "27             0  \n",
       "28             0  \n",
       "29             0  \n",
       "...          ...  \n",
       "3067           0  \n",
       "3068           0  \n",
       "3069           0  \n",
       "3070           0  \n",
       "3071           0  \n",
       "3072           0  \n",
       "3073           0  \n",
       "3074           0  \n",
       "3075           0  \n",
       "3076           0  \n",
       "3077           0  \n",
       "3078           0  \n",
       "3079           0  \n",
       "3080           0  \n",
       "3081           0  \n",
       "3082           0  \n",
       "3083           0  \n",
       "3084           0  \n",
       "3085           0  \n",
       "3086           0  \n",
       "3087           0  \n",
       "3088           0  \n",
       "3089           0  \n",
       "3090           0  \n",
       "3091           0  \n",
       "3092           0  \n",
       "3093           0  \n",
       "3094           0  \n",
       "3095           0  \n",
       "3096           0  \n",
       "\n",
       "[83619 rows x 330 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "some_test_data = read_org_test_data(4, 34, 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>001a_t-2</th>\n",
       "      <th>001a_t-1</th>\n",
       "      <th>001a_t</th>\n",
       "      <th>001a_t+1</th>\n",
       "      <th>001a_t+2</th>\n",
       "      <th>001b_t-2</th>\n",
       "      <th>001b_t-1</th>\n",
       "      <th>001b_t</th>\n",
       "      <th>001b_t+1</th>\n",
       "      <th>001b_t+2</th>\n",
       "      <th>...</th>\n",
       "      <th>Target_t+2</th>\n",
       "      <th>trait_4</th>\n",
       "      <th>trait_6</th>\n",
       "      <th>trait_8</th>\n",
       "      <th>trait_10</th>\n",
       "      <th>trait_12</th>\n",
       "      <th>trait_14</th>\n",
       "      <th>trait_16</th>\n",
       "      <th>trait_18</th>\n",
       "      <th>trait_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>36155.0</td>\n",
       "      <td>50680.0</td>\n",
       "      <td>34752.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6362.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>94800.0</td>\n",
       "      <td>34704.0</td>\n",
       "      <td>7322.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>44961.0</td>\n",
       "      <td>43250.0</td>\n",
       "      <td>19081.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46872.0</td>\n",
       "      <td>43345.0</td>\n",
       "      <td>6818.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>82054.0</td>\n",
       "      <td>59264.0</td>\n",
       "      <td>123750.0</td>\n",
       "      <td>40571.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14976.0</td>\n",
       "      <td>123110.0</td>\n",
       "      <td>43542.0</td>\n",
       "      <td>14490.0</td>\n",
       "      <td>15043.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7764.0</td>\n",
       "      <td>3464.0</td>\n",
       "      <td>2506.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9023.0</td>\n",
       "      <td>36821.0</td>\n",
       "      <td>30637.0</td>\n",
       "      <td>28443.0</td>\n",
       "      <td>26175.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1017.0</td>\n",
       "      <td>5417.0</td>\n",
       "      <td>29407.0</td>\n",
       "      <td>24584.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27558.0</td>\n",
       "      <td>72947.0</td>\n",
       "      <td>45228.0</td>\n",
       "      <td>36956.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>128660.0</td>\n",
       "      <td>13090.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4599.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>96713.0</td>\n",
       "      <td>14020.0</td>\n",
       "      <td>9382.0</td>\n",
       "      <td>7810.0</td>\n",
       "      <td>34125.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2256.0</td>\n",
       "      <td>4013.0</td>\n",
       "      <td>9524.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>38280.0</td>\n",
       "      <td>12364.0</td>\n",
       "      <td>14944.0</td>\n",
       "      <td>78718.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10549.0</td>\n",
       "      <td>19416.0</td>\n",
       "      <td>21105.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>102960.0</td>\n",
       "      <td>12369.0</td>\n",
       "      <td>28311.0</td>\n",
       "      <td>11776.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8895.0</td>\n",
       "      <td>10844.0</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13693.0</td>\n",
       "      <td>3252.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>67080.0</td>\n",
       "      <td>24213.0</td>\n",
       "      <td>13187.0</td>\n",
       "      <td>14374.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>80559.0</td>\n",
       "      <td>55717.0</td>\n",
       "      <td>24014.0</td>\n",
       "      <td>22457.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1226.0</td>\n",
       "      <td>2302.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2521.0</td>\n",
       "      <td>86266.0</td>\n",
       "      <td>14451.0</td>\n",
       "      <td>5997.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20106.0</td>\n",
       "      <td>106470.0</td>\n",
       "      <td>70585.0</td>\n",
       "      <td>60052.0</td>\n",
       "      <td>44647.0</td>\n",
       "      <td>515.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>143160.0</td>\n",
       "      <td>84311.0</td>\n",
       "      <td>60364.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>732.0</td>\n",
       "      <td>527.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1097.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>50181.0</td>\n",
       "      <td>22547.0</td>\n",
       "      <td>13032.0</td>\n",
       "      <td>13019.0</td>\n",
       "      <td>12498.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4046.0</td>\n",
       "      <td>8330.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>67998.0</td>\n",
       "      <td>4725.0</td>\n",
       "      <td>94385.0</td>\n",
       "      <td>19976.0</td>\n",
       "      <td>28827.0</td>\n",
       "      <td>2857.0</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>1134.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1607.0</td>\n",
       "      <td>3285.0</td>\n",
       "      <td>7380.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>21449.0</td>\n",
       "      <td>17259.0</td>\n",
       "      <td>23501.0</td>\n",
       "      <td>7241.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2268.0</td>\n",
       "      <td>38021.0</td>\n",
       "      <td>13879.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14165.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3090.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>76674.0</td>\n",
       "      <td>45598.0</td>\n",
       "      <td>38420.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6036.0</td>\n",
       "      <td>1259.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>87667.0</td>\n",
       "      <td>20985.0</td>\n",
       "      <td>20508.0</td>\n",
       "      <td>12060.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8184.0</td>\n",
       "      <td>10637.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>32606.0</td>\n",
       "      <td>44626.0</td>\n",
       "      <td>18593.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>19561.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>21892.0</td>\n",
       "      <td>72516.0</td>\n",
       "      <td>48906.0</td>\n",
       "      <td>52036.0</td>\n",
       "      <td>4293.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>61085.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3203.0</td>\n",
       "      <td>13382.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3067</th>\n",
       "      <td>81526.0</td>\n",
       "      <td>41270.0</td>\n",
       "      <td>34073.0</td>\n",
       "      <td>32290.0</td>\n",
       "      <td>109740.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8301.0</td>\n",
       "      <td>39268.0</td>\n",
       "      <td>2775.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3068</th>\n",
       "      <td>20761.0</td>\n",
       "      <td>53273.0</td>\n",
       "      <td>22174.0</td>\n",
       "      <td>8049.0</td>\n",
       "      <td>1580.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50712.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3069</th>\n",
       "      <td>48041.0</td>\n",
       "      <td>15578.0</td>\n",
       "      <td>8442.0</td>\n",
       "      <td>57825.0</td>\n",
       "      <td>21784.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>516.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8633.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3070</th>\n",
       "      <td>49227.0</td>\n",
       "      <td>50679.0</td>\n",
       "      <td>45015.0</td>\n",
       "      <td>17736.0</td>\n",
       "      <td>30717.0</td>\n",
       "      <td>10793.0</td>\n",
       "      <td>15203.0</td>\n",
       "      <td>7930.0</td>\n",
       "      <td>11315.0</td>\n",
       "      <td>22155.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3071</th>\n",
       "      <td>6633.0</td>\n",
       "      <td>11521.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14940.0</td>\n",
       "      <td>18347.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3072</th>\n",
       "      <td>0.0</td>\n",
       "      <td>21654.0</td>\n",
       "      <td>51646.0</td>\n",
       "      <td>15321.0</td>\n",
       "      <td>17998.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13923.0</td>\n",
       "      <td>21724.0</td>\n",
       "      <td>2182.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3073</th>\n",
       "      <td>0.0</td>\n",
       "      <td>20383.0</td>\n",
       "      <td>27398.0</td>\n",
       "      <td>34772.0</td>\n",
       "      <td>25253.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11434.0</td>\n",
       "      <td>3982.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3074</th>\n",
       "      <td>5390.0</td>\n",
       "      <td>20420.0</td>\n",
       "      <td>72341.0</td>\n",
       "      <td>60112.0</td>\n",
       "      <td>53433.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3075</th>\n",
       "      <td>34373.0</td>\n",
       "      <td>55850.0</td>\n",
       "      <td>29333.0</td>\n",
       "      <td>34438.0</td>\n",
       "      <td>36446.0</td>\n",
       "      <td>13167.0</td>\n",
       "      <td>32641.0</td>\n",
       "      <td>9481.0</td>\n",
       "      <td>3749.0</td>\n",
       "      <td>23084.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3076</th>\n",
       "      <td>34507.0</td>\n",
       "      <td>20835.0</td>\n",
       "      <td>68530.0</td>\n",
       "      <td>101470.0</td>\n",
       "      <td>127900.0</td>\n",
       "      <td>3649.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3737.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2318.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3077</th>\n",
       "      <td>29855.0</td>\n",
       "      <td>19189.0</td>\n",
       "      <td>16781.0</td>\n",
       "      <td>17407.0</td>\n",
       "      <td>24957.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>475.0</td>\n",
       "      <td>2115.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3078</th>\n",
       "      <td>140720.0</td>\n",
       "      <td>87321.0</td>\n",
       "      <td>100590.0</td>\n",
       "      <td>36976.0</td>\n",
       "      <td>35547.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14284.0</td>\n",
       "      <td>319330.0</td>\n",
       "      <td>234390.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3079</th>\n",
       "      <td>26445.0</td>\n",
       "      <td>28047.0</td>\n",
       "      <td>9276.0</td>\n",
       "      <td>13909.0</td>\n",
       "      <td>8017.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3152.0</td>\n",
       "      <td>15875.0</td>\n",
       "      <td>8256.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3080</th>\n",
       "      <td>60999.0</td>\n",
       "      <td>38483.0</td>\n",
       "      <td>13456.0</td>\n",
       "      <td>6518.0</td>\n",
       "      <td>1016.0</td>\n",
       "      <td>7750.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41754.0</td>\n",
       "      <td>17202.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3081</th>\n",
       "      <td>53904.0</td>\n",
       "      <td>26796.0</td>\n",
       "      <td>11729.0</td>\n",
       "      <td>17125.0</td>\n",
       "      <td>25193.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15646.0</td>\n",
       "      <td>21077.0</td>\n",
       "      <td>44038.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3082</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6882.0</td>\n",
       "      <td>11428.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>3880.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3120.0</td>\n",
       "      <td>13434.0</td>\n",
       "      <td>32652.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083</th>\n",
       "      <td>22068.0</td>\n",
       "      <td>26465.0</td>\n",
       "      <td>43318.0</td>\n",
       "      <td>61088.0</td>\n",
       "      <td>56300.0</td>\n",
       "      <td>12348.0</td>\n",
       "      <td>17911.0</td>\n",
       "      <td>2710.0</td>\n",
       "      <td>3025.0</td>\n",
       "      <td>15132.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3084</th>\n",
       "      <td>0.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>9599.0</td>\n",
       "      <td>25123.0</td>\n",
       "      <td>24841.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2054.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3085</th>\n",
       "      <td>15019.0</td>\n",
       "      <td>4822.0</td>\n",
       "      <td>39665.0</td>\n",
       "      <td>52938.0</td>\n",
       "      <td>68520.0</td>\n",
       "      <td>152570.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16660.0</td>\n",
       "      <td>30499.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3086</th>\n",
       "      <td>11559.0</td>\n",
       "      <td>17785.0</td>\n",
       "      <td>9900.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6380.0</td>\n",
       "      <td>2698.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8681.0</td>\n",
       "      <td>4712.0</td>\n",
       "      <td>1873.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3087</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26784.0</td>\n",
       "      <td>39151.0</td>\n",
       "      <td>39120.0</td>\n",
       "      <td>16604.0</td>\n",
       "      <td>29343.0</td>\n",
       "      <td>3681.0</td>\n",
       "      <td>3673.0</td>\n",
       "      <td>3543.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3088</th>\n",
       "      <td>46908.0</td>\n",
       "      <td>36602.0</td>\n",
       "      <td>25865.0</td>\n",
       "      <td>13560.0</td>\n",
       "      <td>12650.0</td>\n",
       "      <td>21711.0</td>\n",
       "      <td>261500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3089</th>\n",
       "      <td>13447.0</td>\n",
       "      <td>71835.0</td>\n",
       "      <td>55866.0</td>\n",
       "      <td>36303.0</td>\n",
       "      <td>48874.0</td>\n",
       "      <td>11037.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>364.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>50033.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3090</th>\n",
       "      <td>42695.0</td>\n",
       "      <td>21526.0</td>\n",
       "      <td>6802.0</td>\n",
       "      <td>1103.0</td>\n",
       "      <td>7094.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32696.0</td>\n",
       "      <td>4055.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3091</th>\n",
       "      <td>39894.0</td>\n",
       "      <td>34873.0</td>\n",
       "      <td>37745.0</td>\n",
       "      <td>7667.0</td>\n",
       "      <td>3237.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>661.0</td>\n",
       "      <td>9753.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3092</th>\n",
       "      <td>74790.0</td>\n",
       "      <td>45751.0</td>\n",
       "      <td>8752.0</td>\n",
       "      <td>16986.0</td>\n",
       "      <td>10597.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7434.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3093</th>\n",
       "      <td>11947.0</td>\n",
       "      <td>12465.0</td>\n",
       "      <td>32283.0</td>\n",
       "      <td>22326.0</td>\n",
       "      <td>37862.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3337.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3094</th>\n",
       "      <td>945.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21283.0</td>\n",
       "      <td>37455.0</td>\n",
       "      <td>23424.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9260.0</td>\n",
       "      <td>6460.0</td>\n",
       "      <td>662.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3095</th>\n",
       "      <td>47984.0</td>\n",
       "      <td>59170.0</td>\n",
       "      <td>40152.0</td>\n",
       "      <td>14073.0</td>\n",
       "      <td>12525.0</td>\n",
       "      <td>100100.0</td>\n",
       "      <td>606.0</td>\n",
       "      <td>6534.0</td>\n",
       "      <td>38375.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096</th>\n",
       "      <td>2133.0</td>\n",
       "      <td>4378.0</td>\n",
       "      <td>801.0</td>\n",
       "      <td>9732.0</td>\n",
       "      <td>12024.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3854.0</td>\n",
       "      <td>6610.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49552 rows × 339 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      001a_t-2  001a_t-1    001a_t  001a_t+1  001a_t+2  001b_t-2  001b_t-1  \\\n",
       "0          0.0   36155.0   50680.0   34752.0       0.0    6362.0       0.0   \n",
       "1          0.0   94800.0   34704.0    7322.0       0.0       0.0       0.0   \n",
       "2          0.0   44961.0   43250.0   19081.0       0.0       0.0   46872.0   \n",
       "3          0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "4          0.0   82054.0   59264.0  123750.0   40571.0       0.0       0.0   \n",
       "5      14976.0  123110.0   43542.0   14490.0   15043.0       0.0    7764.0   \n",
       "6       9023.0   36821.0   30637.0   28443.0   26175.0       0.0    1017.0   \n",
       "7      27558.0   72947.0   45228.0   36956.0       0.0       0.0       0.0   \n",
       "8     128660.0   13090.0       0.0       0.0       0.0    4599.0       0.0   \n",
       "9      96713.0   14020.0    9382.0    7810.0   34125.0       0.0    2256.0   \n",
       "10         0.0   38280.0   12364.0   14944.0   78718.0       0.0   10549.0   \n",
       "11    102960.0   12369.0   28311.0   11776.0       0.0       0.0       0.0   \n",
       "12      8895.0   10844.0    1185.0       0.0       0.0       0.0   13693.0   \n",
       "13     67080.0   24213.0   13187.0   14374.0       0.0       0.0       0.0   \n",
       "14     80559.0   55717.0   24014.0   22457.0       0.0       0.0    1226.0   \n",
       "15      2521.0   86266.0   14451.0    5997.0       0.0       0.0       0.0   \n",
       "16     20106.0  106470.0   70585.0   60052.0   44647.0     515.0       0.0   \n",
       "17         0.0  143160.0   84311.0   60364.0       0.0       0.0     732.0   \n",
       "18         0.0       0.0       0.0       0.0       0.0    1097.0       0.0   \n",
       "19     50181.0   22547.0   13032.0   13019.0   12498.0       0.0    4046.0   \n",
       "20     67998.0    4725.0   94385.0   19976.0   28827.0    2857.0    1959.0   \n",
       "21         0.0       0.0       0.0       0.0       0.0       0.0    1607.0   \n",
       "22     21449.0   17259.0   23501.0    7241.0       0.0       0.0    2268.0   \n",
       "23     14165.0       0.0       0.0       0.0       0.0    3090.0       0.0   \n",
       "24         0.0   76674.0   45598.0   38420.0       0.0       0.0    6036.0   \n",
       "25     87667.0   20985.0   20508.0   12060.0       0.0       0.0    8184.0   \n",
       "26         0.0   32606.0   44626.0   18593.0       0.0       0.0       0.0   \n",
       "27     19561.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "28     21892.0   72516.0   48906.0   52036.0    4293.0       0.0       0.0   \n",
       "29     61085.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3067   81526.0   41270.0   34073.0   32290.0  109740.0       0.0       0.0   \n",
       "3068   20761.0   53273.0   22174.0    8049.0    1580.0       0.0   50712.0   \n",
       "3069   48041.0   15578.0    8442.0   57825.0   21784.0       0.0       0.0   \n",
       "3070   49227.0   50679.0   45015.0   17736.0   30717.0   10793.0   15203.0   \n",
       "3071    6633.0   11521.0    2021.0       0.0   14940.0   18347.0       0.0   \n",
       "3072       0.0   21654.0   51646.0   15321.0   17998.0       0.0       0.0   \n",
       "3073       0.0   20383.0   27398.0   34772.0   25253.0       0.0       0.0   \n",
       "3074    5390.0   20420.0   72341.0   60112.0   53433.0       0.0       0.0   \n",
       "3075   34373.0   55850.0   29333.0   34438.0   36446.0   13167.0   32641.0   \n",
       "3076   34507.0   20835.0   68530.0  101470.0  127900.0    3649.0       0.0   \n",
       "3077   29855.0   19189.0   16781.0   17407.0   24957.0       0.0       0.0   \n",
       "3078  140720.0   87321.0  100590.0   36976.0   35547.0       0.0       0.0   \n",
       "3079   26445.0   28047.0    9276.0   13909.0    8017.0       0.0       0.0   \n",
       "3080   60999.0   38483.0   13456.0    6518.0    1016.0    7750.0       0.0   \n",
       "3081   53904.0   26796.0   11729.0   17125.0   25193.0       0.0       0.0   \n",
       "3082       0.0    6882.0   11428.0     117.0    3880.0       0.0       0.0   \n",
       "3083   22068.0   26465.0   43318.0   61088.0   56300.0   12348.0   17911.0   \n",
       "3084       0.0   33878.0    9599.0   25123.0   24841.0       0.0       0.0   \n",
       "3085   15019.0    4822.0   39665.0   52938.0   68520.0  152570.0       0.0   \n",
       "3086   11559.0   17785.0    9900.0       0.0    6380.0    2698.0       0.0   \n",
       "3087       0.0       0.0   26784.0   39151.0   39120.0   16604.0   29343.0   \n",
       "3088   46908.0   36602.0   25865.0   13560.0   12650.0   21711.0  261500.0   \n",
       "3089   13447.0   71835.0   55866.0   36303.0   48874.0   11037.0       0.0   \n",
       "3090   42695.0   21526.0    6802.0    1103.0    7094.0       0.0       0.0   \n",
       "3091   39894.0   34873.0   37745.0    7667.0    3237.0       0.0       0.0   \n",
       "3092   74790.0   45751.0    8752.0   16986.0   10597.0       0.0       0.0   \n",
       "3093   11947.0   12465.0   32283.0   22326.0   37862.0       0.0       0.0   \n",
       "3094     945.0       0.0   21283.0   37455.0   23424.0       0.0       0.0   \n",
       "3095   47984.0   59170.0   40152.0   14073.0   12525.0  100100.0     606.0   \n",
       "3096    2133.0    4378.0     801.0    9732.0   12024.0       0.0       0.0   \n",
       "\n",
       "       001b_t  001b_t+1  001b_t+2    ...     Target_t+2  trait_4  trait_6  \\\n",
       "0         0.0       0.0       0.0    ...              0     True    False   \n",
       "1         0.0       0.0       0.0    ...              0    False    False   \n",
       "2     43345.0    6818.0       0.0    ...              0    False     True   \n",
       "3         0.0       0.0       0.0    ...              0     True     True   \n",
       "4         0.0       0.0       0.0    ...              0    False    False   \n",
       "5      3464.0    2506.0       0.0    ...              0    False    False   \n",
       "6      5417.0   29407.0   24584.0    ...              0     True    False   \n",
       "7         0.0       0.0       0.0    ...              0    False    False   \n",
       "8         0.0       0.0       0.0    ...              0    False    False   \n",
       "9      4013.0    9524.0       0.0    ...              0     True    False   \n",
       "10    19416.0   21105.0       0.0    ...              0     True    False   \n",
       "11        0.0       0.0       0.0    ...              0     True    False   \n",
       "12     3252.0       0.0       0.0    ...              0     True    False   \n",
       "13        0.0       0.0       0.0    ...              0    False     True   \n",
       "14     2302.0       0.0       0.0    ...              0    False    False   \n",
       "15        0.0       0.0       0.0    ...              0    False     True   \n",
       "16        0.0       0.0       0.0    ...              0    False    False   \n",
       "17      527.0       0.0       0.0    ...              0    False    False   \n",
       "18        0.0       0.0       0.0    ...              0    False     True   \n",
       "19     8330.0       0.0       0.0    ...              0    False    False   \n",
       "20     1134.0       0.0       0.0    ...              0    False    False   \n",
       "21     3285.0    7380.0       0.0    ...              0    False    False   \n",
       "22    38021.0   13879.0       0.0    ...              0    False    False   \n",
       "23        0.0       0.0       0.0    ...              0    False    False   \n",
       "24     1259.0       0.0       0.0    ...              0    False    False   \n",
       "25    10637.0       0.0       0.0    ...              0    False    False   \n",
       "26        0.0       0.0       0.0    ...              0    False    False   \n",
       "27        0.0       0.0       0.0    ...              0    False    False   \n",
       "28        0.0       0.0       0.0    ...              0    False    False   \n",
       "29     3203.0   13382.0       0.0    ...              0     True     True   \n",
       "...       ...       ...       ...    ...            ...      ...      ...   \n",
       "3067   8301.0   39268.0    2775.0    ...              0     True    False   \n",
       "3068      0.0       0.0       0.0    ...              0    False     True   \n",
       "3069    516.0       0.0    8633.0    ...              0    False     True   \n",
       "3070   7930.0   11315.0   22155.0    ...              0    False     True   \n",
       "3071      0.0       0.0       0.0    ...              0    False    False   \n",
       "3072  13923.0   21724.0    2182.0    ...              0     True    False   \n",
       "3073  11434.0    3982.0       0.0    ...              0    False     True   \n",
       "3074      0.0       0.0       0.0    ...              0     True    False   \n",
       "3075   9481.0    3749.0   23084.0    ...              0    False     True   \n",
       "3076   3737.0       0.0    2318.0    ...              0    False    False   \n",
       "3077      0.0     475.0    2115.0    ...              0    False    False   \n",
       "3078  14284.0  319330.0  234390.0    ...              0    False     True   \n",
       "3079   3152.0   15875.0    8256.0    ...              0    False     True   \n",
       "3080      0.0   41754.0   17202.0    ...              0    False    False   \n",
       "3081  15646.0   21077.0   44038.0    ...              0    False    False   \n",
       "3082   3120.0   13434.0   32652.0    ...              0    False    False   \n",
       "3083   2710.0    3025.0   15132.0    ...              0    False    False   \n",
       "3084   2054.0       0.0       0.0    ...              0     True    False   \n",
       "3085      0.0   16660.0   30499.0    ...              0    False     True   \n",
       "3086   8681.0    4712.0    1873.0    ...              0    False    False   \n",
       "3087   3681.0    3673.0    3543.0    ...              0    False    False   \n",
       "3088      0.0       0.0       0.0    ...              0    False    False   \n",
       "3089    364.0      53.0   50033.0    ...              0    False    False   \n",
       "3090      0.0   32696.0    4055.0    ...              0    False     True   \n",
       "3091    661.0    9753.0       0.0    ...              0    False    False   \n",
       "3092      0.0    7434.0       0.0    ...              0    False    False   \n",
       "3093   3337.0       0.0       0.0    ...              0     True    False   \n",
       "3094   9260.0    6460.0     662.0    ...              0    False    False   \n",
       "3095   6534.0   38375.0       0.0    ...              0    False    False   \n",
       "3096   3854.0    6610.0       0.0    ...              0    False    False   \n",
       "\n",
       "      trait_8  trait_10  trait_12  trait_14  trait_16  trait_18  trait_20  \n",
       "0       False     False      True     False     False     False         0  \n",
       "1       False     False     False     False     False     False         0  \n",
       "2       False      True     False      True      True      True         0  \n",
       "3       False      True     False     False     False     False         0  \n",
       "4       False     False     False     False     False     False         0  \n",
       "5       False     False     False     False     False     False         0  \n",
       "6       False     False      True     False     False     False         0  \n",
       "7       False     False     False     False     False     False         0  \n",
       "8       False     False     False     False     False     False         0  \n",
       "9        True     False      True     False     False     False         0  \n",
       "10       True     False      True     False      True     False         0  \n",
       "11       True      True      True     False      True     False         0  \n",
       "12      False     False     False     False     False     False         0  \n",
       "13      False     False     False     False     False     False         0  \n",
       "14      False     False      True     False     False     False         0  \n",
       "15      False     False      True     False     False     False         0  \n",
       "16      False     False     False      True     False     False         0  \n",
       "17      False     False     False     False      True     False         0  \n",
       "18      False      True      True      True     False      True         0  \n",
       "19      False     False     False     False     False     False         0  \n",
       "20       True     False     False     False      True     False         0  \n",
       "21      False     False     False     False      True     False         0  \n",
       "22      False      True     False     False     False     False         0  \n",
       "23      False     False      True      True     False     False         0  \n",
       "24      False     False     False     False     False     False         0  \n",
       "25       True      True     False     False      True     False         0  \n",
       "26       True     False     False     False     False     False         0  \n",
       "27      False     False     False     False     False     False         0  \n",
       "28      False     False     False     False     False     False         0  \n",
       "29      False     False     False      True     False      True         0  \n",
       "...       ...       ...       ...       ...       ...       ...       ...  \n",
       "3067    False     False     False      True     False      True         0  \n",
       "3068    False     False     False      True      True     False         0  \n",
       "3069    False     False     False     False     False     False         0  \n",
       "3070     True     False      True     False      True      True         0  \n",
       "3071     True      True     False     False      True     False         0  \n",
       "3072     True      True     False      True     False     False         0  \n",
       "3073    False     False     False     False      True     False         0  \n",
       "3074    False     False      True     False      True     False         0  \n",
       "3075     True     False     False     False     False      True         0  \n",
       "3076     True     False      True     False     False      True         0  \n",
       "3077    False     False     False     False     False      True         0  \n",
       "3078     True     False     False     False      True     False         0  \n",
       "3079     True     False     False     False     False     False         0  \n",
       "3080    False     False      True     False     False      True         0  \n",
       "3081     True     False     False     False      True     False         0  \n",
       "3082    False     False      True      True     False     False         0  \n",
       "3083    False     False      True     False      True     False         0  \n",
       "3084    False      True      True     False      True     False         0  \n",
       "3085    False      True     False      True     False     False         0  \n",
       "3086     True     False     False     False      True     False         0  \n",
       "3087    False     False     False     False     False      True         0  \n",
       "3088    False     False      True      True     False      True         0  \n",
       "3089     True     False     False      True      True      True         0  \n",
       "3090    False      True      True      True      True     False         0  \n",
       "3091    False     False      True      True      True     False         0  \n",
       "3092    False      True     False     False     False      True         0  \n",
       "3093     True     False     False      True      True     False         0  \n",
       "3094     True     False     False      True     False      True         0  \n",
       "3095    False     False      True     False     False     False         0  \n",
       "3096     True     False     False     False     False     False         0  \n",
       "\n",
       "[49552 rows x 339 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_test_data(sample_test_data_df, scaler_from_training_data):\n",
    "    T_x, T_y, T_generated_attributes = (sample_test_data_df[det_names_in_correct_order], \n",
    "                                        sample_test_data_df['Target_t'], \n",
    "                                        sample_test_data_df[all_trait_names])\n",
    "    T_x = scaler_from_training_data.transform(T_x)\n",
    "    return T_x, T_y.values, T_generated_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_threshold(true_y_values, pred_results):\n",
    "    prec, rec, thres = precision_recall_curve(true_y_values, pred_results)\n",
    "    score = np.zeros(len(thres))\n",
    "\n",
    "    # Score Thresholds\n",
    "    for i in range(0,len(thres)):\n",
    "        score[i] = f1_score(true_y_values, (pred_results > thres[i]).astype('int'))\n",
    "    return thres[np.argmax(score)], score.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RCP14_Algorithm_1_Ted_enhanced(S1_x, S2_x, S3_x, S1_y, S2_y, S3_y, iter_per_weight_per_round):\n",
    "    weights_to_test = list(range(10, 210, 10))\n",
    "    weight_scores = {trial_weight : [] for trial_weight in range(10, 210, 10)}\n",
    "    for elimination_round in range(3):\n",
    "        for trial_weight in weights_to_test:\n",
    "            for iteration_num in range(iter_per_weight_per_round):\n",
    "                model_prediction_output = get_model_predictions(S1_x, S1_y, S2_x, S2_y, trial_weight, S3_x)\n",
    "                weight_scores[trial_weight].append(average_precision_score(S3_y, model_prediction_output))\n",
    "                print(\"current area is {} for weight {} in elimination round {} for iteration number {}\".format(weight_scores[trial_weight][-1], trial_weight, elimination_round, iteration_num))\n",
    "            #np.savetxt(fname = \"weight_scores_weight_{}_round_{}_{}.csv\".format(trial_weight, elimination_round, added_name_string), \n",
    "            #           X = weight_scores[trial_weight], delimiter = \",\")\n",
    "        weight_avg_scores = {trial_weight : np.mean(weight_scores[trial_weight]) for trial_weight in range(10, 210, 10)}\n",
    "        worst_5_weights = sorted(weights_to_test, key=lambda k: weight_avg_scores[k])[0:5]\n",
    "        weights_to_test = [some_weight for some_weight in weights_to_test if some_weight not in worst_5_weights]\n",
    "    return weights_to_test, weight_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10,\n",
       " 20,\n",
       " 30,\n",
       " 40,\n",
       " 50,\n",
       " 60,\n",
       " 70,\n",
       " 80,\n",
       " 90,\n",
       " 100,\n",
       " 110,\n",
       " 120,\n",
       " 130,\n",
       " 140,\n",
       " 150,\n",
       " 160,\n",
       " 170,\n",
       " 180,\n",
       " 190,\n",
       " 200]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(10, 210, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_weights(possible_weights, number_of_results, relative_probability_vector):\n",
    "    if len(possible_weights) != len(relative_probability_vector):\n",
    "        raise Exception(\"Weights vector must be the same size as the probability vector\")\n",
    "    relative_probability_vector = np.array(relative_probability_vector)\n",
    "    abs_prob_vector = relative_probability_vector/relative_probability_vector.sum()\n",
    "    return choice(a = possible_weights, size = number_of_results, p = abs_prob_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weights_for_org(simulated_org_number, iter_per_weight_per_round = 5):\n",
    "    training_data = read_org_data(simulated_org_number, 7, 33)\n",
    "    training_data.replace([-np.inf,np.inf], np.nan, inplace=True)\n",
    "    training_data.dropna(inplace=True)\n",
    "    S1_x, S2_x, S3_x, S1_y, S2_y, S3_y, scaler_from_training_data = split_training_data(training_data)\n",
    "    return RCP14_Algorithm_1_Ted_enhanced(S1_x, S2_x, S3_x, S1_y, S2_y, S3_y, iter_per_weight_per_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.1155 - val_loss: 0.0227\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0573 - val_loss: 0.0189\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0458 - val_loss: 0.0189\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0359 - val_loss: 0.0146\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0288 - val_loss: 0.0190\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0220 - val_loss: 0.0144\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0166 - val_loss: 0.0142\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0123 - val_loss: 0.0137\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0083 - val_loss: 0.0114\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0058 - val_loss: 0.0122\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0042 - val_loss: 0.0117\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 22s - loss: 0.0030 - val_loss: 0.0115\n",
      "current area is 0.058039417874286243 for weight 10 in elimination round 0 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 25s - loss: 0.1667 - val_loss: 0.0449\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0908 - val_loss: 0.0284\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0658 - val_loss: 0.0219\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0486 - val_loss: 0.0223\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0321 - val_loss: 0.0150\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0209 - val_loss: 0.0188\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0110 - val_loss: 0.0114\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0073 - val_loss: 0.0107\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0041 - val_loss: 0.0103\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0030 - val_loss: 0.0102\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0024 - val_loss: 0.0103\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0019 - val_loss: 0.0100\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0017 - val_loss: 0.0102\n",
      "Epoch 14/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0014 - val_loss: 0.0102\n",
      "Epoch 15/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0012 - val_loss: 0.0104\n",
      "current area is 0.06752539639376617 for weight 20 in elimination round 0 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.2093 - val_loss: 0.0622\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.1264 - val_loss: 0.0516\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0905 - val_loss: 0.0628\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0584 - val_loss: 0.0289\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0399 - val_loss: 0.0223\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0223 - val_loss: 0.0158\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0097 - val_loss: 0.0154\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0061 - val_loss: 0.0127\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0042 - val_loss: 0.0116\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0028 - val_loss: 0.0112\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0022 - val_loss: 0.0114\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0018 - val_loss: 0.0112\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0016 - val_loss: 0.0114\n",
      "current area is 0.10025576619213784 for weight 30 in elimination round 0 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.2495 - val_loss: 0.0587\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.1537 - val_loss: 0.0548\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.1034 - val_loss: 0.0461\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0728 - val_loss: 0.0696\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0350 - val_loss: 0.0255\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0237 - val_loss: 0.0167\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0095 - val_loss: 0.0146\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0050 - val_loss: 0.0126\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0033 - val_loss: 0.0125\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0026 - val_loss: 0.0120\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0021 - val_loss: 0.0119\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0017 - val_loss: 0.0119\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0015 - val_loss: 0.0121\n",
      "Epoch 14/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0013 - val_loss: 0.0122\n",
      "Epoch 15/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0011 - val_loss: 0.0122\n",
      "current area is 0.07893001492269253 for weight 40 in elimination round 0 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 22s - loss: 0.2954 - val_loss: 0.1058\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.1798 - val_loss: 0.0724\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.1284 - val_loss: 0.0899\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0734 - val_loss: 0.0283\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 22s - loss: 0.0382 - val_loss: 0.0253\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0208 - val_loss: 0.0192\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0099 - val_loss: 0.0142\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 22s - loss: 0.0051 - val_loss: 0.0111\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 22s - loss: 0.0033 - val_loss: 0.0111\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0024 - val_loss: 0.0106\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0020 - val_loss: 0.0106\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0017 - val_loss: 0.0107\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 22s - loss: 0.0014 - val_loss: 0.0108\n",
      "current area is 0.11439317048893004 for weight 50 in elimination round 0 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.3297 - val_loss: 0.0887\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.2084 - val_loss: 0.0819\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.1511 - val_loss: 0.0763\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0996 - val_loss: 0.0507\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0587 - val_loss: 0.0430\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0229 - val_loss: 0.0181\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0097 - val_loss: 0.0131\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0055 - val_loss: 0.0124\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 16s - loss: 0.0037 - val_loss: 0.0123\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 16s - loss: 0.0028 - val_loss: 0.0116\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 16s - loss: 0.0023 - val_loss: 0.0116\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0019 - val_loss: 0.0117\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 16s - loss: 0.0016 - val_loss: 0.0117\n",
      "Epoch 14/100\n",
      "58533/58533 [==============================] - 16s - loss: 0.0014 - val_loss: 0.0118\n",
      "current area is 0.10808264935492361 for weight 60 in elimination round 0 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.3717 - val_loss: 0.1943\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 16s - loss: 0.2264 - val_loss: 0.0664\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 16s - loss: 0.1701 - val_loss: 0.0846\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 16s - loss: 0.0862 - val_loss: 0.0337\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 16s - loss: 0.0592 - val_loss: 0.0428\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 16s - loss: 0.0248 - val_loss: 0.0161\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 16s - loss: 0.0089 - val_loss: 0.0128\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 16s - loss: 0.0045 - val_loss: 0.0114\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 16s - loss: 0.0032 - val_loss: 0.0113\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 16s - loss: 0.0025 - val_loss: 0.0115\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 16s - loss: 0.0020 - val_loss: 0.0113\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0017 - val_loss: 0.0113\n",
      "current area is 0.11832274456690846 for weight 70 in elimination round 0 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.3998 - val_loss: 0.1299\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.2626 - val_loss: 0.0862\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.1886 - val_loss: 0.0684\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.1191 - val_loss: 0.0549\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0498 - val_loss: 0.0251\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0265 - val_loss: 0.0172\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0110 - val_loss: 0.0128\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0054 - val_loss: 0.0111\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0037 - val_loss: 0.0110\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0027 - val_loss: 0.0104\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0022 - val_loss: 0.0106\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0018 - val_loss: 0.0107\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0016 - val_loss: 0.0106\n",
      "current area is 0.08262870411685361 for weight 80 in elimination round 0 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.4415 - val_loss: 0.1559\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.2931 - val_loss: 0.1177\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.1945 - val_loss: 0.0749\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.1284 - val_loss: 0.0675\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0805 - val_loss: 0.0285\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0331 - val_loss: 0.0205\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0113 - val_loss: 0.0143\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0054 - val_loss: 0.0124\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0039 - val_loss: 0.0122\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0029 - val_loss: 0.0122\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0023 - val_loss: 0.0121\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0019 - val_loss: 0.0121\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0016 - val_loss: 0.0122\n",
      "Epoch 14/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0014 - val_loss: 0.0122\n",
      "current area is 0.10562039558522815 for weight 90 in elimination round 0 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.4719 - val_loss: 0.1410\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.3250 - val_loss: 0.1175\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.2303 - val_loss: 0.1034\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.1523 - val_loss: 0.0505\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0570 - val_loss: 0.0907\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0323 - val_loss: 0.0519\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0187 - val_loss: 0.0239\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0063 - val_loss: 0.0123\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0036 - val_loss: 0.0123\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0027 - val_loss: 0.0121\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0021 - val_loss: 0.0120\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0018 - val_loss: 0.0119\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0015 - val_loss: 0.0120\n",
      "Epoch 14/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0013 - val_loss: 0.0121\n",
      "Epoch 15/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0012 - val_loss: 0.0122\n",
      "current area is 0.08873714296023646 for weight 100 in elimination round 0 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.5021 - val_loss: 0.2147\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.3550 - val_loss: 0.0906\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.2723 - val_loss: 0.0872\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.1553 - val_loss: 0.1816\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.1043 - val_loss: 0.0347\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0394 - val_loss: 0.0769\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0153 - val_loss: 0.0128\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0061 - val_loss: 0.0123\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0039 - val_loss: 0.0113\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0030 - val_loss: 0.0114\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0023 - val_loss: 0.0113\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0019 - val_loss: 0.0113\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0016 - val_loss: 0.0114\n",
      "Epoch 14/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0014 - val_loss: 0.0115\n",
      "current area is 0.08204379268361633 for weight 110 in elimination round 0 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.5190 - val_loss: 0.2726\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.3810 - val_loss: 0.1464\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.3008 - val_loss: 0.2555\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.2079 - val_loss: 0.0497\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0763 - val_loss: 0.0720\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0575 - val_loss: 0.0335\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0175 - val_loss: 0.0228\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0102 - val_loss: 0.0315\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0148 - val_loss: 0.0164\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0053 - val_loss: 0.0122\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0040 - val_loss: 0.0135\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0026 - val_loss: 0.0118\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0020 - val_loss: 0.0120\n",
      "Epoch 14/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0017 - val_loss: 0.0120\n",
      "Epoch 15/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0014 - val_loss: 0.0122\n",
      "current area is 0.09220997128232669 for weight 120 in elimination round 0 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.5891 - val_loss: 0.2635\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.4490 - val_loss: 0.1984\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.3232 - val_loss: 0.1616\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.2514 - val_loss: 0.0760\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.1267 - val_loss: 0.0420\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0504 - val_loss: 0.0477\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0157 - val_loss: 0.0157\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0072 - val_loss: 0.0145\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0046 - val_loss: 0.0128\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0033 - val_loss: 0.0123\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0025 - val_loss: 0.0121\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0021 - val_loss: 0.0121\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0018 - val_loss: 0.0122\n",
      "Epoch 14/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0015 - val_loss: 0.0122\n",
      "Epoch 15/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0013 - val_loss: 0.0123\n",
      "current area is 0.07966617959215513 for weight 130 in elimination round 0 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.6117 - val_loss: 0.2442\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.4774 - val_loss: 0.2017\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.3615 - val_loss: 0.0777\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.2830 - val_loss: 0.3288\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.1451 - val_loss: 0.0699\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0673 - val_loss: 0.0486\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0178 - val_loss: 0.0230\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0084 - val_loss: 0.0211\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0052 - val_loss: 0.0131\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0033 - val_loss: 0.0130\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0026 - val_loss: 0.0123\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0020 - val_loss: 0.0124\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0017 - val_loss: 0.0123\n",
      "Epoch 14/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0015 - val_loss: 0.0124\n",
      "current area is 0.11261391266078653 for weight 140 in elimination round 0 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.6571 - val_loss: 0.3486\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.5164 - val_loss: 0.1972\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.3909 - val_loss: 0.3559\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.3065 - val_loss: 0.2696\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.1392 - val_loss: 0.0303\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0656 - val_loss: 0.0414\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0230 - val_loss: 0.0223\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0109 - val_loss: 0.0149\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0057 - val_loss: 0.0136\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0042 - val_loss: 0.0137\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0029 - val_loss: 0.0126\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0023 - val_loss: 0.0127\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0019 - val_loss: 0.0126\n",
      "Epoch 14/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0017 - val_loss: 0.0129\n",
      "current area is 0.10188485577479107 for weight 150 in elimination round 0 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.6746 - val_loss: 0.1684\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.5181 - val_loss: 0.1902\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.4979 - val_loss: 0.1346\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.3762 - val_loss: 0.1223\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.1452 - val_loss: 0.0519\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0794 - val_loss: 0.1287\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0360 - val_loss: 0.0254\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0183 - val_loss: 0.0418\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0135 - val_loss: 0.0159\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0057 - val_loss: 0.0141\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0039 - val_loss: 0.0131\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0031 - val_loss: 0.0130\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0024 - val_loss: 0.0133\n",
      "Epoch 14/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0020 - val_loss: 0.0126\n",
      "Epoch 15/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0018 - val_loss: 0.0129\n",
      "Epoch 16/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0015 - val_loss: 0.0130\n",
      "Epoch 17/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0013 - val_loss: 0.0130\n",
      "current area is 0.09026006134364604 for weight 160 in elimination round 0 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.6909 - val_loss: 0.3858\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.6623 - val_loss: 0.2256\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.4632 - val_loss: 0.4429\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.3319 - val_loss: 0.2614\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.2836 - val_loss: 0.1408\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0848 - val_loss: 0.2640\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0532 - val_loss: 0.0393\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0241 - val_loss: 0.0250\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0108 - val_loss: 0.0152\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0060 - val_loss: 0.0133\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0040 - val_loss: 0.0136\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0027 - val_loss: 0.0133\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0022 - val_loss: 0.0133\n",
      "Epoch 14/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0018 - val_loss: 0.0132\n",
      "Epoch 15/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0016 - val_loss: 0.0133\n",
      "Epoch 16/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0014 - val_loss: 0.0134\n",
      "Epoch 17/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0012 - val_loss: 0.0134\n",
      "current area is 0.10305297925331636 for weight 170 in elimination round 0 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.7390 - val_loss: 0.3247\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.6640 - val_loss: 0.1156\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.5878 - val_loss: 0.3765\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.4886 - val_loss: 0.2641\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.2171 - val_loss: 0.1230\n",
      "current area is 0.005293186633337203 for weight 180 in elimination round 0 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.8174 - val_loss: 0.3454\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.7115 - val_loss: 0.5552\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.6167 - val_loss: 0.7689\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.6248 - val_loss: 0.2314\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.3004 - val_loss: 0.1009\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.1082 - val_loss: 0.0536\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0333 - val_loss: 0.0539\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0184 - val_loss: 0.0260\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0103 - val_loss: 0.0504\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0109 - val_loss: 0.0156\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0041 - val_loss: 0.0146\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0031 - val_loss: 0.0152\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0024 - val_loss: 0.0141\n",
      "Epoch 14/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0020 - val_loss: 0.0140\n",
      "Epoch 15/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0017 - val_loss: 0.0140\n",
      "Epoch 16/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0014 - val_loss: 0.0141\n",
      "Epoch 17/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0013 - val_loss: 0.0139\n",
      "Epoch 18/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0011 - val_loss: 0.0140\n",
      "Epoch 19/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0010 - val_loss: 0.0139\n",
      "Epoch 20/100\n",
      "58533/58533 [==============================] - 17s - loss: 9.2546e-04 - val_loss: 0.0140\n",
      "current area is 0.0769280032218319 for weight 190 in elimination round 0 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.8331 - val_loss: 7.4240\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 17s - loss: 2.1156 - val_loss: 2.9191\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 17s - loss: 1.3076 - val_loss: 0.2129\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.9710 - val_loss: 0.4346\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.5765 - val_loss: 0.1344\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.4636 - val_loss: 0.9967\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.3594 - val_loss: 0.1538\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.1782 - val_loss: 0.0641\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.2748 - val_loss: 0.2930\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.1958 - val_loss: 0.0555\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.1758 - val_loss: 0.0789\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0473 - val_loss: 0.0354\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0196 - val_loss: 0.0331\n",
      "Epoch 14/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0095 - val_loss: 0.0190\n",
      "Epoch 15/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0052 - val_loss: 0.0186\n",
      "Epoch 16/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0035 - val_loss: 0.0173\n",
      "Epoch 17/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0028 - val_loss: 0.0171\n",
      "Epoch 18/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0023 - val_loss: 0.0167\n",
      "Epoch 19/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0019 - val_loss: 0.0167\n",
      "Epoch 20/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0017 - val_loss: 0.0166\n",
      "Epoch 21/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0015 - val_loss: 0.0166\n",
      "Epoch 22/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0013 - val_loss: 0.0166\n",
      "Epoch 23/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0012 - val_loss: 0.0166\n",
      "Epoch 24/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0011 - val_loss: 0.0166\n",
      "Epoch 25/100\n",
      "58533/58533 [==============================] - 17s - loss: 9.7605e-04 - val_loss: 0.0166\n",
      "Epoch 26/100\n",
      "58533/58533 [==============================] - 17s - loss: 9.0268e-04 - val_loss: 0.0166\n",
      "current area is 0.08648635844229312 for weight 200 in elimination round 0 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.2142 - val_loss: 0.0520\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.1258 - val_loss: 0.0325\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0903 - val_loss: 0.0348\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0611 - val_loss: 0.0280\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0405 - val_loss: 0.0214\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0238 - val_loss: 0.0216\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0113 - val_loss: 0.0132\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0059 - val_loss: 0.0106\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0038 - val_loss: 0.0109\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0028 - val_loss: 0.0110\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0022 - val_loss: 0.0106\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0018 - val_loss: 0.0106\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0015 - val_loss: 0.0107\n",
      "Epoch 14/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0013 - val_loss: 0.0108\n",
      "current area is 0.0809122248625053 for weight 30 in elimination round 1 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.2495 - val_loss: 0.0468\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.1559 - val_loss: 0.0476\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.1086 - val_loss: 0.0505\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0749 - val_loss: 0.0283\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0363 - val_loss: 0.0297\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0203 - val_loss: 0.0222\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0048 - val_loss: 0.0122\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0035 - val_loss: 0.0121\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0025 - val_loss: 0.0119\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0020 - val_loss: 0.0118\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0017 - val_loss: 0.0121\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0015 - val_loss: 0.0120\n",
      "Epoch 14/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0013 - val_loss: 0.0123\n",
      "current area is 0.04852332944432657 for weight 40 in elimination round 1 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.2828 - val_loss: 0.0721\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.1859 - val_loss: 0.0876\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.1255 - val_loss: 0.0584\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0824 - val_loss: 0.0427\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0366 - val_loss: 0.0246\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0228 - val_loss: 0.0226\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0093 - val_loss: 0.0131\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0045 - val_loss: 0.0122\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0032 - val_loss: 0.0118\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0025 - val_loss: 0.0119\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0020 - val_loss: 0.0118\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0017 - val_loss: 0.0120\n",
      "current area is 0.11193340517110428 for weight 50 in elimination round 1 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.3331 - val_loss: 0.1155\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.2065 - val_loss: 0.0803\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.1521 - val_loss: 0.0681\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0938 - val_loss: 0.0666\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0505 - val_loss: 0.0250\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0205 - val_loss: 0.0144\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0090 - val_loss: 0.0116\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0051 - val_loss: 0.0110\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0035 - val_loss: 0.0108\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0026 - val_loss: 0.0105\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0021 - val_loss: 0.0106\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0018 - val_loss: 0.0107\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0015 - val_loss: 0.0107\n",
      "current area is 0.1191104006677256 for weight 60 in elimination round 1 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.3642 - val_loss: 0.1198\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.2450 - val_loss: 0.0764\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.1666 - val_loss: 0.1001\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.1121 - val_loss: 0.0695\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0614 - val_loss: 0.0354\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0272 - val_loss: 0.0268\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0103 - val_loss: 0.0138\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0053 - val_loss: 0.0118\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0035 - val_loss: 0.0115\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0027 - val_loss: 0.0117\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0022 - val_loss: 0.0118\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0018 - val_loss: 0.0118\n",
      "current area is 0.12585452383735235 for weight 70 in elimination round 1 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.4021 - val_loss: 0.1866\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.2625 - val_loss: 0.0988\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.1984 - val_loss: 0.0674\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.1130 - val_loss: 0.0348\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0674 - val_loss: 0.0333\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0277 - val_loss: 0.0210\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0106 - val_loss: 0.0222\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0067 - val_loss: 0.0119\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0036 - val_loss: 0.0116\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0027 - val_loss: 0.0116\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0022 - val_loss: 0.0115\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0018 - val_loss: 0.0115\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0016 - val_loss: 0.0116\n",
      "Epoch 14/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0014 - val_loss: 0.0117\n",
      "current area is 0.0916884586403992 for weight 80 in elimination round 1 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.4515 - val_loss: 0.1268\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.2965 - val_loss: 0.1260\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.2336 - val_loss: 0.0872\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.1395 - val_loss: 0.0922\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0812 - val_loss: 0.0369\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0371 - val_loss: 0.0244\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0094 - val_loss: 0.0138\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0056 - val_loss: 0.0123\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0040 - val_loss: 0.0123\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0029 - val_loss: 0.0115\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0024 - val_loss: 0.0116\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0020 - val_loss: 0.0118\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0017 - val_loss: 0.0117\n",
      "current area is 0.0792744816016031 for weight 90 in elimination round 1 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.4761 - val_loss: 0.1322\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.3352 - val_loss: 0.1225\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.2595 - val_loss: 0.0736\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 22s - loss: 0.1332 - val_loss: 0.0707\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0710 - val_loss: 0.0277\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.0313 - val_loss: 0.0259\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0104 - val_loss: 0.0128\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0051 - val_loss: 0.0116\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.0037 - val_loss: 0.0116\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.0028 - val_loss: 0.0118\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.0023 - val_loss: 0.0115\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.0019 - val_loss: 0.0116\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0016 - val_loss: 0.0116\n",
      "Epoch 14/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0014 - val_loss: 0.0116\n",
      "current area is 0.07364610964615753 for weight 100 in elimination round 1 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 25s - loss: 0.5025 - val_loss: 0.1521\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.3822 - val_loss: 0.1788\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.2409 - val_loss: 0.0940\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.1885 - val_loss: 0.1197\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.0634 - val_loss: 0.0399\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.0212 - val_loss: 0.0200\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.0096 - val_loss: 0.0316\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.0071 - val_loss: 0.0123\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.0038 - val_loss: 0.0115\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.0029 - val_loss: 0.0114\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.0023 - val_loss: 0.0113\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.0019 - val_loss: 0.0114\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.0016 - val_loss: 0.0115\n",
      "Epoch 14/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.0014 - val_loss: 0.0116\n",
      "current area is 0.10507903773977208 for weight 110 in elimination round 1 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 25s - loss: 0.5517 - val_loss: 0.1636\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.3609 - val_loss: 0.2544\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.3111 - val_loss: 0.1440\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.1717 - val_loss: 0.1201\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.1029 - val_loss: 0.0814\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0324 - val_loss: 0.0332\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.0123 - val_loss: 0.0158\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0064 - val_loss: 0.0129\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0039 - val_loss: 0.0119\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0030 - val_loss: 0.0117\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0115\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0019 - val_loss: 0.0116\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.0016 - val_loss: 0.0117\n",
      "Epoch 14/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0014 - val_loss: 0.0118\n",
      "current area is 0.08768512156618856 for weight 120 in elimination round 1 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 25s - loss: 0.5661 - val_loss: 0.2022\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.4420 - val_loss: 0.2175\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.3417 - val_loss: 0.1781\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.2343 - val_loss: 0.0774\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.1167 - val_loss: 0.0760\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0444 - val_loss: 0.0245\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0134 - val_loss: 0.0177\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0070 - val_loss: 0.0132\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0042 - val_loss: 0.0205\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0036 - val_loss: 0.0125\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0024 - val_loss: 0.0125\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0020 - val_loss: 0.0124\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0017 - val_loss: 0.0127\n",
      "Epoch 14/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0014 - val_loss: 0.0125\n",
      "Epoch 15/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0012 - val_loss: 0.0126\n",
      "current area is 0.1035005201723778 for weight 130 in elimination round 1 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 25s - loss: 0.6003 - val_loss: 0.3124\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.4661 - val_loss: 0.1580\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.3353 - val_loss: 0.2388\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.2167 - val_loss: 0.0837\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.0991 - val_loss: 0.0641\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.0605 - val_loss: 0.0696\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0413 - val_loss: 0.0181\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.0132 - val_loss: 0.0136\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.0060 - val_loss: 0.0138\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.0040 - val_loss: 0.0140\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.0029 - val_loss: 0.0123\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.0023 - val_loss: 0.0127\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.0019 - val_loss: 0.0124\n",
      "Epoch 14/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.0016 - val_loss: 0.0123\n",
      "Epoch 15/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0014 - val_loss: 0.0124\n",
      "Epoch 16/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0012 - val_loss: 0.0125\n",
      "Epoch 17/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0011 - val_loss: 0.0126\n",
      "current area is 0.10456438909776479 for weight 140 in elimination round 1 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.6293 - val_loss: 0.1669\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.4935 - val_loss: 0.1625\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.3805 - val_loss: 0.1839\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.2699 - val_loss: 0.1143\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 16s - loss: 0.1798 - val_loss: 0.1393\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.1164 - val_loss: 0.0944\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0974 - val_loss: 0.0390\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 16s - loss: 0.0312 - val_loss: 0.0614\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0179 - val_loss: 0.0167\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0074 - val_loss: 0.0138\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0047 - val_loss: 0.0134\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0033 - val_loss: 0.0138\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0025 - val_loss: 0.0129\n",
      "Epoch 14/100\n",
      "58533/58533 [==============================] - 24s - loss: 0.0021 - val_loss: 0.0129\n",
      "Epoch 15/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0017 - val_loss: 0.0130\n",
      "Epoch 16/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0015 - val_loss: 0.0130\n",
      "current area is 0.09230247900953158 for weight 150 in elimination round 1 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.6592 - val_loss: 0.1806\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.5466 - val_loss: 0.2203\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.5017 - val_loss: 0.3015\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.3598 - val_loss: 0.1496\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.1718 - val_loss: 0.1493\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 22s - loss: 0.0798 - val_loss: 0.0343\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 22s - loss: 0.0233 - val_loss: 0.0172\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0089 - val_loss: 0.0136\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0053 - val_loss: 0.0134\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0033 - val_loss: 0.0127\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0026 - val_loss: 0.0127\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0021 - val_loss: 0.0127\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0017 - val_loss: 0.0127\n",
      "Epoch 14/100\n",
      "58533/58533 [==============================] - 22s - loss: 0.0015 - val_loss: 0.0128\n",
      "current area is 0.09798170432807844 for weight 160 in elimination round 1 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 23s - loss: 0.7226 - val_loss: 0.3530\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.6095 - val_loss: 0.1603\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 22s - loss: 0.5499 - val_loss: 0.1790\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 22s - loss: 0.3853 - val_loss: 0.3570\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.1910 - val_loss: 0.1137\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0648 - val_loss: 0.0280\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0172 - val_loss: 0.0163\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0068 - val_loss: 0.0133\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0043 - val_loss: 0.0142\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0031 - val_loss: 0.0126\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0024 - val_loss: 0.0126\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0019 - val_loss: 0.0127\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0016 - val_loss: 0.0125\n",
      "Epoch 14/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0014 - val_loss: 0.0126\n",
      "Epoch 15/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0012 - val_loss: 0.0127\n",
      "Epoch 16/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0011 - val_loss: 0.0127\n",
      "current area is 0.10117843071978898 for weight 170 in elimination round 1 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.8617 - val_loss: 0.2753\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.7816 - val_loss: 0.6500\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.6924 - val_loss: 0.3127\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.5192 - val_loss: 0.4219\n",
      "current area is 0.020624842868581336 for weight 200 in elimination round 1 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.2117 - val_loss: 0.0494\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.1257 - val_loss: 0.0456\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0877 - val_loss: 0.0439\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0628 - val_loss: 0.0184\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0411 - val_loss: 0.0301\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0207 - val_loss: 0.0219\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0100 - val_loss: 0.0143\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0053 - val_loss: 0.0111\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0035 - val_loss: 0.0110\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0028 - val_loss: 0.0111\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0022 - val_loss: 0.0111\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0019 - val_loss: 0.0111\n",
      "current area is 0.10467214450142019 for weight 30 in elimination round 2 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.2865 - val_loss: 0.0498\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.1874 - val_loss: 0.0516\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.1382 - val_loss: 0.0412\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0881 - val_loss: 0.0514\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0441 - val_loss: 0.0366\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0224 - val_loss: 0.0193\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0134 - val_loss: 0.0133\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0057 - val_loss: 0.0121\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0036 - val_loss: 0.0113\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0027 - val_loss: 0.0111\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0022 - val_loss: 0.0112\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0018 - val_loss: 0.0114\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0015 - val_loss: 0.0113\n",
      "current area is 0.10023429911384485 for weight 50 in elimination round 2 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.3375 - val_loss: 0.1073\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.2172 - val_loss: 0.0707\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.1533 - val_loss: 0.0830\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0890 - val_loss: 0.0386\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0648 - val_loss: 0.0253\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0343 - val_loss: 0.0365\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0132 - val_loss: 0.0158\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0056 - val_loss: 0.0128\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0037 - val_loss: 0.0128\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0028 - val_loss: 0.0122\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0023 - val_loss: 0.0120\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0019 - val_loss: 0.0121\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0016 - val_loss: 0.0121\n",
      "Epoch 14/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0014 - val_loss: 0.0121\n",
      "current area is 0.07394839068656962 for weight 60 in elimination round 2 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.3609 - val_loss: 0.1247\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.2408 - val_loss: 0.1159\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.1634 - val_loss: 0.0988\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0993 - val_loss: 0.0715\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0662 - val_loss: 0.0322\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0224 - val_loss: 0.0206\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0091 - val_loss: 0.0123\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0050 - val_loss: 0.0117\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0035 - val_loss: 0.0114\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0027 - val_loss: 0.0112\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0022 - val_loss: 0.0113\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0018 - val_loss: 0.0112\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0016 - val_loss: 0.0113\n",
      "current area is 0.06960121966566128 for weight 70 in elimination round 2 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.4438 - val_loss: 0.1502\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.2967 - val_loss: 0.1291\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.1921 - val_loss: 0.1258\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.1374 - val_loss: 0.0478\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.1048 - val_loss: 0.1080\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0447 - val_loss: 0.0280\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0124 - val_loss: 0.0148\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0063 - val_loss: 0.0143\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0041 - val_loss: 0.0115\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0030 - val_loss: 0.0111\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0024 - val_loss: 0.0109\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0020 - val_loss: 0.0111\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0017 - val_loss: 0.0112\n",
      "Epoch 14/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0015 - val_loss: 0.0112\n",
      "current area is 0.10505202298325028 for weight 90 in elimination round 2 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.5080 - val_loss: 0.1762\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.3599 - val_loss: 0.1680\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.2666 - val_loss: 0.0988\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.1876 - val_loss: 0.0776\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0837 - val_loss: 0.0588\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0293 - val_loss: 0.0272\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0114 - val_loss: 0.0162\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0064 - val_loss: 0.0124\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0040 - val_loss: 0.0117\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0031 - val_loss: 0.0115\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0024 - val_loss: 0.0113\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0020 - val_loss: 0.0114\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0017 - val_loss: 0.0116\n",
      "Epoch 14/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0014 - val_loss: 0.0115\n",
      "current area is 0.07396142693313831 for weight 110 in elimination round 2 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.5488 - val_loss: 0.1856\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.3949 - val_loss: 0.1368\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.2823 - val_loss: 0.0702\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.2102 - val_loss: 0.0479\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.1061 - val_loss: 0.0342\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0231 - val_loss: 0.0210\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0119 - val_loss: 0.0159\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0064 - val_loss: 0.0134\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0040 - val_loss: 0.0123\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0029 - val_loss: 0.0125\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0023 - val_loss: 0.0124\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0019 - val_loss: 0.0124\n",
      "current area is 0.08037539373122744 for weight 120 in elimination round 2 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.5735 - val_loss: 0.1804\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.4146 - val_loss: 0.1668\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.3439 - val_loss: 0.3370\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.1871 - val_loss: 0.1188\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.1528 - val_loss: 0.0327\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0471 - val_loss: 0.0300\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0322 - val_loss: 0.0244\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0209 - val_loss: 0.0190\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0083 - val_loss: 0.0177\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0045 - val_loss: 0.0128\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0032 - val_loss: 0.0122\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0025 - val_loss: 0.0121\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0020 - val_loss: 0.0120\n",
      "Epoch 14/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0017 - val_loss: 0.0121\n",
      "Epoch 15/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0015 - val_loss: 0.0121\n",
      "Epoch 16/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0013 - val_loss: 0.0123\n",
      "current area is 0.08133022586047196 for weight 130 in elimination round 2 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.5979 - val_loss: 0.2356\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.4550 - val_loss: 0.1948\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.3738 - val_loss: 0.1243\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.2772 - val_loss: 0.2015\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.1303 - val_loss: 0.0644\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0928 - val_loss: 0.0563\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0266 - val_loss: 0.0157\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0102 - val_loss: 0.0138\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0053 - val_loss: 0.0135\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0040 - val_loss: 0.0130\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0030 - val_loss: 0.0130\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0022 - val_loss: 0.0129\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0019 - val_loss: 0.0129\n",
      "Epoch 14/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0016 - val_loss: 0.0131\n",
      "Epoch 15/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0014 - val_loss: 0.0131\n",
      "current area is 0.11280571992016052 for weight 140 in elimination round 2 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.6477 - val_loss: 0.1579\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.5089 - val_loss: 0.1801\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.4400 - val_loss: 0.1345\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.2412 - val_loss: 0.0699\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.1389 - val_loss: 0.0767\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0782 - val_loss: 0.0351\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0298 - val_loss: 0.0329\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0149 - val_loss: 0.0186\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0066 - val_loss: 0.0136\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0043 - val_loss: 0.0132\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0032 - val_loss: 0.0129\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0025 - val_loss: 0.0124\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0021 - val_loss: 0.0126\n",
      "Epoch 14/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0018 - val_loss: 0.0127\n",
      "Epoch 15/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0015 - val_loss: 0.0124\n",
      "current area is 0.0728308852055778 for weight 150 in elimination round 2 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.6855 - val_loss: 0.4881\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.5549 - val_loss: 0.1835\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.4405 - val_loss: 0.0886\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.3561 - val_loss: 0.2754\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.2325 - val_loss: 0.0571\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.1231 - val_loss: 0.0460\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0626 - val_loss: 0.0392\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0174 - val_loss: 0.0145\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0061 - val_loss: 0.0120\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0042 - val_loss: 0.0115\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.0032 - val_loss: 0.0115\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0025 - val_loss: 0.0115\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0020 - val_loss: 0.0116\n",
      "current area is 0.08133108854070677 for weight 160 in elimination round 2 for interatin number 0\n",
      "Train on 58533 samples, validate on 8362 samples\n",
      "Epoch 1/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.6865 - val_loss: 0.2440\n",
      "Epoch 2/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.5910 - val_loss: 0.4716\n",
      "Epoch 3/100\n",
      "58533/58533 [==============================] - 17s - loss: 0.4772 - val_loss: 0.0885\n",
      "Epoch 4/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.4258 - val_loss: 0.1381\n",
      "Epoch 5/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.2008 - val_loss: 0.0990\n",
      "Epoch 6/100\n",
      "58533/58533 [==============================] - 22s - loss: 0.0687 - val_loss: 0.0319\n",
      "Epoch 7/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0179 - val_loss: 0.0217\n",
      "Epoch 8/100\n",
      "58533/58533 [==============================] - 18s - loss: 0.0097 - val_loss: 0.0166\n",
      "Epoch 9/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0050 - val_loss: 0.0138\n",
      "Epoch 10/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0033 - val_loss: 0.0125\n",
      "Epoch 11/100\n",
      "58533/58533 [==============================] - 19s - loss: 0.0025 - val_loss: 0.0125\n",
      "Epoch 12/100\n",
      "58533/58533 [==============================] - 20s - loss: 0.0020 - val_loss: 0.0128\n",
      "Epoch 13/100\n",
      "58533/58533 [==============================] - 21s - loss: 0.0017 - val_loss: 0.0128\n",
      "current area is 0.11644959925805928 for weight 170 in elimination round 2 for interatin number 0\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: \"/Volumes/MImran's Public Folder/RCP Data/RCP14/Weights_0.csv\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-a5eda4d3d7fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0morg_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mweights_to_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_weights_for_org\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morg_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_per_weight_per_round\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_to_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Volumes/MImran's Public Folder/RCP Data/RCP14/Weights_{}.csv\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morg_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Volumes/MImran's Public Folder/RCP Data/RCP14/Weights_{}.csv\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morg_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   1411\u001b[0m                                      \u001b[0mdoublequote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m                                      escapechar=escapechar, decimal=decimal)\n\u001b[0;32m-> 1413\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1566\u001b[0m             f, handles = _get_handle(self.path_or_buf, self.mode,\n\u001b[1;32m   1567\u001b[0m                                      \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m                                      compression=self.compression)\n\u001b[0m\u001b[1;32m   1569\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;31m# Python 3 and no explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;31m# Python 3 and binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: \"/Volumes/MImran's Public Folder/RCP Data/RCP14/Weights_0.csv\""
     ]
    }
   ],
   "source": [
    "# weights_to_test = {}\n",
    "# weight_scores = {}\n",
    "# for org_num in range(1):\n",
    "#     weights_to_test, weight_scores = get_weights_for_org(org_num, iter_per_weight_per_round= 1)\n",
    "#     np.savetxt(\"/Users/MImran/Google Drive/GMU SCITE/RCPs Fifth Quarter/RCP14/Weights Mohanad July 15 Data 1 org 1 iter per weight/Weights_to_test_{}.csv\".format(org_num), weights_to_test, fmt='%d', delimiter=',')\n",
    "#     with open('/Users/MImran/Google Drive/GMU SCITE/RCPs Fifth Quarter/RCP14/Weights Mohanad July 15 Data 1 org 1 iter per weight/Weights_Scores_{}.csv'.format(org_num), 'w') as f:\n",
    "#     [f.write('{0},{1}\\n'.format(key, value)) for key, value in weight_scores.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.savetxt(\"/Users/MImran/Google Drive/GMU SCITE/RCPs Fifth Quarter/RCP14/Weights Mohanad July 15 Data 1 org 1 iter per weight/Weights_to_test_{}.csv\".format(org_num), weights_to_test, fmt='%d', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('/Users/MImran/Google Drive/GMU SCITE/RCPs Fifth Quarter/RCP14/Weights Mohanad July 15 Data 1 org 1 iter per weight/Weights_Scores_{}.csv'.format(org_num), 'w') as f:\n",
    "#     [f.write('{0},{1}\\n'.format(key, value)) for key, value in weight_scores.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30, 50, 60, 70, 90, 140, 160, 170]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_to_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10: [0.058039417874286243],\n",
       " 20: [0.067525396393766174],\n",
       " 30: [0.10025576619213784, 0.080912224862505294, 0.10467214450142019],\n",
       " 40: [0.078930014922692526, 0.048523329444326573],\n",
       " 50: [0.11439317048893004, 0.11193340517110428, 0.10023429911384485],\n",
       " 60: [0.10808264935492361, 0.1191104006677256, 0.073948390686569623],\n",
       " 70: [0.11832274456690846, 0.12585452383735235, 0.069601219665661282],\n",
       " 80: [0.082628704116853607, 0.091688458640399206],\n",
       " 90: [0.10562039558522815, 0.079274481601603095, 0.10505202298325028],\n",
       " 100: [0.088737142960236465, 0.073646109646157532],\n",
       " 110: [0.082043792683616334, 0.10507903773977208, 0.073961426933138313],\n",
       " 120: [0.092209971282326691, 0.087685121566188559, 0.080375393731227443],\n",
       " 130: [0.079666179592155134, 0.10350052017237781, 0.081330225860471964],\n",
       " 140: [0.11261391266078653, 0.10456438909776479, 0.11280571992016052],\n",
       " 150: [0.10188485577479107, 0.092302479009531577, 0.072830885205577803],\n",
       " 160: [0.090260061343646045, 0.097981704328078442, 0.081331088540706767],\n",
       " 170: [0.10305297925331636, 0.10117843071978898, 0.11644959925805928],\n",
       " 180: [0.0052931866333372031],\n",
       " 190: [0.076928003221831895],\n",
       " 200: [0.086486358442293118, 0.020624842868581336]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RCP14_Algorithm_2(S1_x, S2_x, S3_x, S1_y, S2_y, S3_y, T_x, T_y, \n",
    "                      T_generated_attributes, best_weights, num_iterations = 1, r_p_vector=[50,30,20]):\n",
    "    answer_dict = {\"Answer_\" + str(answer_num) : np.zeros(num_iterations) for answer_num in range(1, 22)}\n",
    "    chosen_weights = choose_weights(possible_weights= best_weights, number_of_results= num_iterations,\n",
    "                                    relative_probability_vector=r_p_vector)\n",
    "    for iteration_num in range(num_iterations):\n",
    "        current_model = get_model(S1_x, S1_y, S2_x, S2_y, chosen_weights[iteration_num])\n",
    "        model_prediction_output = current_model.predict(S3_x)[:,0]\n",
    "        #print(\"Model prediction output looks like\")\n",
    "        #print(model_prediction_output[0:10])\n",
    "        chosen_tau, best_score = find_threshold(true_y_values=S3_y, pred_results= model_prediction_output)\n",
    "        print(\"optimized cutoff is {}\".format(chosen_tau))\n",
    "        print(\"The F1 score for this choice is {}.\".format(best_score))\n",
    "        prediction_output_for_test_data = current_model.predict(T_x)[:,0]\n",
    "        T_labels = (prediction_output_for_test_data > chosen_tau).astype(int)\n",
    "        print(\"Number of alerts is {}.\".format(T_labels.sum()))\n",
    "        answer_dict[\"Answer_1\"][iteration_num] = (T_y & T_labels).sum() / T_y.sum()\n",
    "        answer_dict[\"Answer_2\"][iteration_num] = (T_y & T_labels).sum() / T_labels.sum()\n",
    "        answer_dict[\"Answer_3\"][iteration_num] = ((T_y ^ 1) & T_labels).sum() / (T_y ^ 1).sum()\n",
    "        answer_dict[\"Answer_4\"][iteration_num] = (T_generated_attributes['trait_4'].values & T_labels).mean() / (T_labels).mean()\n",
    "        answer_dict[\"Answer_5\"][iteration_num] = (T_generated_attributes['trait_4'].values & T_labels).mean() / np.mean(T_generated_attributes['trait_4'])\n",
    "        answer_dict[\"Answer_6\"][iteration_num] = (T_generated_attributes['trait_6'].values & T_labels).mean() / (T_labels).mean()\n",
    "        answer_dict[\"Answer_7\"][iteration_num] = (T_generated_attributes['trait_6'].values & T_labels).mean() / np.mean(T_generated_attributes['trait_6'])\n",
    "        answer_dict[\"Answer_8\"][iteration_num] = (T_generated_attributes['trait_8'].values & T_labels).mean() / (T_labels).mean()\n",
    "        answer_dict[\"Answer_9\"][iteration_num] = (T_generated_attributes['trait_8'].values & T_labels).mean() / np.mean(T_generated_attributes['trait_8'])\n",
    "        answer_dict[\"Answer_10\"][iteration_num] = (T_generated_attributes['trait_10'].values & T_labels).mean() / (T_labels).mean()\n",
    "        answer_dict[\"Answer_11\"][iteration_num] = (T_generated_attributes['trait_10'].values & T_labels).mean() / np.mean(T_generated_attributes['trait_10'])\n",
    "        answer_dict[\"Answer_12\"][iteration_num] = (T_generated_attributes['trait_12'].values & T_labels).mean() / (T_labels).mean()\n",
    "        answer_dict[\"Answer_13\"][iteration_num] = (T_generated_attributes['trait_12'].values & T_labels).mean() / np.mean(T_generated_attributes['trait_12'])\n",
    "        answer_dict[\"Answer_14\"][iteration_num] = (T_generated_attributes['trait_14'].values & T_labels).mean() / (T_labels).mean()\n",
    "        answer_dict[\"Answer_15\"][iteration_num] = (T_generated_attributes['trait_14'].values & T_labels).mean() / np.mean(T_generated_attributes['trait_14'])\n",
    "        answer_dict[\"Answer_16\"][iteration_num] = (T_generated_attributes['trait_16'].values & T_labels).mean() / (T_labels).mean()\n",
    "        answer_dict[\"Answer_17\"][iteration_num] = (T_generated_attributes['trait_16'].values & T_labels).mean() / np.mean(T_generated_attributes['trait_16'])\n",
    "        answer_dict[\"Answer_18\"][iteration_num] = (T_generated_attributes['trait_18'].values & T_labels).mean() / (T_labels).mean()\n",
    "        answer_dict[\"Answer_19\"][iteration_num] = (T_generated_attributes['trait_18'].values & T_labels).mean() / np.mean(T_generated_attributes['trait_18'])\n",
    "        answer_dict[\"Answer_20\"][iteration_num] = (T_generated_attributes['trait_20'].values & T_labels).mean() / (T_labels).mean()\n",
    "        answer_dict[\"Answer_21\"][iteration_num] = (T_generated_attributes['trait_20'].values & T_labels).mean() / np.mean(T_generated_attributes['trait_20'])\n",
    "    return answer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_all_answers_for_org(simulated_org_number, iter_per_weight = 25, answer_iterations = 1):\n",
    "#     training_data = read_org_data(simulated_org_number, 7, 33)\n",
    "#     training_data.replace([-np.inf,np.inf], np.nan, inplace=True)\n",
    "#     training_data.dropna(inplace=True)\n",
    "#     S1_x, S2_x, S3_x, S1_y, S2_y, S3_y, scaler_from_training_data = split_training_data(training_data)\n",
    "#     #weight_to_use = RCP14_Algorithm_1(S1_x, S2_x, S3_x, S1_y, S2_y, S3_y, \n",
    "#     #                                  iter_per_weight = iter_per_weight)\n",
    "#     test_data = read_org_test_data(simulated_org_number, 34, 49)\n",
    "#     test_data.replace([-np.inf,np.inf], np.nan, inplace=True)\n",
    "#     test_data.dropna(inplace=True)\n",
    "#     T_x, T_y, T_generated_attributes = split_test_data(test_data, scaler_from_training_data)\n",
    "#     return RCP14_Algorithm_2(S1_x, S2_x, S3_x, S1_y, S2_y, S3_y, T_x, T_y, \n",
    "#                       T_generated_attributes, best_weights = [170, 180, 150], num_iterations = answer_iterations, r_p_vector=[50,30,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized cutoff is 0.8312029838562012\n",
      "The F1 score for this choice is 0.20689655172413793.\n",
      "Number of alerts is 1050.\n",
      "optimized cutoff is 0.9992813467979431\n",
      "The F1 score for this choice is 0.13793103448275862.\n",
      "Number of alerts is 101.\n"
     ]
    }
   ],
   "source": [
    "Answers_for_each_org_dict = {}\n",
    "for org_num in range(2):\n",
    "    Answers_for_each_org_dict[org_num] = get_all_answers_for_org(org_num)\n",
    "    pd.DataFrame(Answers_for_each_org_dict[org_num]).to_csv(\"/Users/mimran/Google Drive/GMU SCITE/RCPs Fifth Quarter/RCP14/RCP14_algo2_Jul23_DVersion_Case3_3weights_answers/algo2_July23_answers_for_org_{}.csv\".format(org_num), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized cutoff is 0.9492578506469727\n",
      "The F1 score for this choice is 0.03225806451612903.\n",
      "Number of alerts is 760.\n",
      "optimized cutoff is 0.6545448303222656\n",
      "The F1 score for this choice is 0.06896551724137931.\n",
      "Number of alerts is 1156.\n",
      "optimized cutoff is 0.9988527297973633\n",
      "The F1 score for this choice is 0.11764705882352942.\n",
      "Number of alerts is 41.\n",
      "optimized cutoff is 0.1456790715456009\n",
      "The F1 score for this choice is 0.03636363636363636.\n",
      "Number of alerts is 2164.\n",
      "optimized cutoff is 0.9982890486717224\n",
      "The F1 score for this choice is 0.08888888888888888.\n",
      "Number of alerts is 15.\n",
      "optimized cutoff is 0.6901633739471436\n",
      "The F1 score for this choice is 0.07407407407407407.\n",
      "Number of alerts is 972.\n",
      "optimized cutoff is 0.9983780980110168\n",
      "The F1 score for this choice is 0.14634146341463414.\n",
      "Number of alerts is 146.\n",
      "optimized cutoff is 0.16116461157798767\n",
      "The F1 score for this choice is 0.07547169811320754.\n",
      "Number of alerts is 2222.\n",
      "optimized cutoff is 0.030267411842942238\n",
      "The F1 score for this choice is 0.09448818897637794.\n",
      "Number of alerts is 3489.\n",
      "optimized cutoff is 0.040193814784288406\n",
      "The F1 score for this choice is 0.05263157894736842.\n",
      "Number of alerts is 3205.\n",
      "optimized cutoff is 0.016058368608355522\n",
      "The F1 score for this choice is 0.03092783505154639.\n",
      "Number of alerts is 5295.\n",
      "optimized cutoff is 0.30329492688179016\n",
      "The F1 score for this choice is 0.1111111111111111.\n",
      "Number of alerts is 2273.\n",
      "optimized cutoff is 0.1557174026966095\n",
      "The F1 score for this choice is 0.08163265306122448.\n",
      "Number of alerts is 2976.\n",
      "optimized cutoff is 0.05041177570819855\n",
      "The F1 score for this choice is 0.0449438202247191.\n",
      "Number of alerts is 4081.\n",
      "optimized cutoff is 0.27031978964805603\n",
      "The F1 score for this choice is 0.1333333333333333.\n",
      "Number of alerts is 2651.\n",
      "optimized cutoff is 0.011210263706743717\n",
      "The F1 score for this choice is 0.02985074626865672.\n",
      "Number of alerts is 6175.\n",
      "optimized cutoff is 0.5896896719932556\n",
      "The F1 score for this choice is 0.08.\n",
      "Number of alerts is 1788.\n",
      "optimized cutoff is 0.9980529546737671\n",
      "The F1 score for this choice is 0.02484472049689441.\n",
      "Number of alerts is 751.\n",
      "optimized cutoff is 0.8982511758804321\n",
      "The F1 score for this choice is 0.03375527426160338.\n",
      "Number of alerts is 1506.\n",
      "optimized cutoff is 0.9909173846244812\n",
      "The F1 score for this choice is 0.018691588785046728.\n",
      "Number of alerts is 374.\n"
     ]
    }
   ],
   "source": [
    "Answers_for_each_org_dict = {}\n",
    "for org_num in range(2):\n",
    "    Answers_for_each_org_dict[org_num] = get_all_answers_for_org(org_num, answer_iterations=10)\n",
    "    pd.DataFrame(Answers_for_each_org_dict[org_num]).to_csv(\"/Users/mimran/Google Drive/GMU SCITE/RCPs Fifth Quarter/RCP14/RCP14_algo2_Jul23_DVersion_Case3_3weights_answers/algo2_July23_10iter_answers_for_org_{}.csv\".format(org_num), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized cutoff is 0.377481609582901\n",
      "The F1 score for this choice is 0.1290322580645161.\n",
      "Number of alerts is 2159.\n",
      "optimized cutoff is 0.3909080922603607\n",
      "The F1 score for this choice is 0.1142857142857143.\n",
      "Number of alerts is 1758.\n",
      "optimized cutoff is 0.1856638789176941\n",
      "The F1 score for this choice is 0.10909090909090909.\n",
      "Number of alerts is 2935.\n",
      "optimized cutoff is 0.33142760396003723\n",
      "The F1 score for this choice is 0.1818181818181818.\n",
      "Number of alerts is 2066.\n",
      "optimized cutoff is 0.119929239153862\n",
      "The F1 score for this choice is 0.07692307692307691.\n",
      "Number of alerts is 2952.\n",
      "optimized cutoff is 0.10351548343896866\n",
      "The F1 score for this choice is 0.05714285714285715.\n",
      "Number of alerts is 3266.\n",
      "optimized cutoff is 0.1130266860127449\n",
      "The F1 score for this choice is 0.06666666666666667.\n",
      "Number of alerts is 3067.\n",
      "optimized cutoff is 0.9944059252738953\n",
      "The F1 score for this choice is 0.12121212121212123.\n",
      "Number of alerts is 308.\n",
      "optimized cutoff is 0.3742711544036865\n",
      "The F1 score for this choice is 0.13636363636363635.\n",
      "Number of alerts is 2082.\n",
      "optimized cutoff is 0.6208968758583069\n",
      "The F1 score for this choice is 0.08.\n",
      "Number of alerts is 1583.\n",
      "optimized cutoff is 0.26712751388549805\n",
      "The F1 score for this choice is 0.20833333333333331.\n",
      "Number of alerts is 2455.\n",
      "optimized cutoff is 0.6059933304786682\n",
      "The F1 score for this choice is 0.30303030303030304.\n",
      "Number of alerts is 1764.\n",
      "optimized cutoff is 0.3700905442237854\n",
      "The F1 score for this choice is 0.25.\n",
      "Number of alerts is 1972.\n",
      "optimized cutoff is 0.48146161437034607\n",
      "The F1 score for this choice is 0.27027027027027023.\n",
      "Number of alerts is 1936.\n",
      "optimized cutoff is 0.35992518067359924\n",
      "The F1 score for this choice is 0.21739130434782608.\n",
      "Number of alerts is 2137.\n",
      "optimized cutoff is 0.5201292037963867\n",
      "The F1 score for this choice is 0.29411764705882354.\n",
      "Number of alerts is 1857.\n",
      "optimized cutoff is 0.9918800592422485\n",
      "The F1 score for this choice is 0.1509433962264151.\n",
      "Number of alerts is 544.\n",
      "optimized cutoff is 0.08732715994119644\n",
      "The F1 score for this choice is 0.17721518987341772.\n",
      "Number of alerts is 3446.\n",
      "optimized cutoff is 0.44450342655181885\n",
      "The F1 score for this choice is 0.25641025641025644.\n",
      "Number of alerts is 2011.\n",
      "optimized cutoff is 0.7792631387710571\n",
      "The F1 score for this choice is 0.22222222222222218.\n",
      "Number of alerts is 1181.\n",
      "optimized cutoff is 0.6529499888420105\n",
      "The F1 score for this choice is 0.07692307692307693.\n",
      "Number of alerts is 1027.\n",
      "optimized cutoff is 0.7975059151649475\n",
      "The F1 score for this choice is 0.08.\n",
      "Number of alerts is 892.\n",
      "optimized cutoff is 0.8739379048347473\n",
      "The F1 score for this choice is 0.08695652173913045.\n",
      "Number of alerts is 656.\n",
      "optimized cutoff is 0.914645791053772\n",
      "The F1 score for this choice is 0.08.\n",
      "Number of alerts is 584.\n",
      "optimized cutoff is 0.7976362705230713\n",
      "The F1 score for this choice is 0.08.\n",
      "Number of alerts is 890.\n",
      "optimized cutoff is 0.9914499521255493\n",
      "The F1 score for this choice is 0.04938271604938272.\n",
      "Number of alerts is 694.\n",
      "optimized cutoff is 0.7182273268699646\n",
      "The F1 score for this choice is 0.06896551724137931.\n",
      "Number of alerts is 923.\n",
      "optimized cutoff is 0.9926037788391113\n",
      "The F1 score for this choice is 0.04040404040404041.\n",
      "Number of alerts is 286.\n",
      "optimized cutoff is 0.9286842942237854\n",
      "The F1 score for this choice is 0.07692307692307693.\n",
      "Number of alerts is 574.\n",
      "optimized cutoff is 0.8138109445571899\n",
      "The F1 score for this choice is 0.08.\n",
      "Number of alerts is 721.\n",
      "optimized cutoff is 0.14233434200286865\n",
      "The F1 score for this choice is 0.0851063829787234.\n",
      "Number of alerts is 2813.\n",
      "optimized cutoff is 0.9996538162231445\n",
      "The F1 score for this choice is 0.12307692307692308.\n",
      "Number of alerts is 368.\n",
      "optimized cutoff is 0.3751761317253113\n",
      "The F1 score for this choice is 0.12500000000000003.\n",
      "Number of alerts is 2523.\n",
      "optimized cutoff is 0.9963725805282593\n",
      "The F1 score for this choice is 0.04.\n",
      "Number of alerts is 65.\n",
      "optimized cutoff is 0.5342232584953308\n",
      "The F1 score for this choice is 0.16.\n",
      "Number of alerts is 1658.\n",
      "optimized cutoff is 0.2351129800081253\n",
      "The F1 score for this choice is 0.15.\n",
      "Number of alerts is 2590.\n",
      "optimized cutoff is 0.14122118055820465\n",
      "The F1 score for this choice is 0.17391304347826086.\n",
      "Number of alerts is 2958.\n",
      "optimized cutoff is 0.490509033203125\n",
      "The F1 score for this choice is 0.1290322580645161.\n",
      "Number of alerts is 1942.\n",
      "optimized cutoff is 0.4505428671836853\n",
      "The F1 score for this choice is 0.12500000000000003.\n",
      "Number of alerts is 2003.\n",
      "optimized cutoff is 0.4510284960269928\n",
      "The F1 score for this choice is 0.14285714285714288.\n",
      "Number of alerts is 1993.\n",
      "optimized cutoff is 0.3741471767425537\n",
      "The F1 score for this choice is 0.05555555555555555.\n",
      "Number of alerts is 1562.\n",
      "optimized cutoff is 0.673222005367279\n",
      "The F1 score for this choice is 0.08.\n",
      "Number of alerts is 1060.\n",
      "optimized cutoff is 0.02729477733373642\n",
      "The F1 score for this choice is 0.029197080291970802.\n",
      "Number of alerts is 3580.\n",
      "optimized cutoff is 0.023071996867656708\n",
      "The F1 score for this choice is 0.040268456375838924.\n",
      "Number of alerts is 4236.\n",
      "optimized cutoff is 0.9538236260414124\n",
      "The F1 score for this choice is 0.014388489208633094.\n",
      "Number of alerts is 1889.\n",
      "optimized cutoff is 0.20222043991088867\n",
      "The F1 score for this choice is 0.046511627906976744.\n",
      "Number of alerts is 1938.\n",
      "optimized cutoff is 0.5130609273910522\n",
      "The F1 score for this choice is 0.06250000000000001.\n",
      "Number of alerts is 1333.\n",
      "optimized cutoff is 0.0019819829612970352\n",
      "The F1 score for this choice is 0.016282225237449117.\n",
      "Number of alerts is 7401.\n",
      "optimized cutoff is 0.2780914008617401\n",
      "The F1 score for this choice is 0.0425531914893617.\n",
      "Number of alerts is 1585.\n",
      "optimized cutoff is 0.026469454169273376\n",
      "The F1 score for this choice is 0.02484472049689441.\n",
      "Number of alerts is 3657.\n",
      "optimized cutoff is 0.2626906633377075\n",
      "The F1 score for this choice is 0.15.\n",
      "Number of alerts is 1388.\n",
      "optimized cutoff is 0.43210744857788086\n",
      "The F1 score for this choice is 0.21428571428571427.\n",
      "Number of alerts is 1062.\n",
      "optimized cutoff is 0.2265559434890747\n",
      "The F1 score for this choice is 0.1951219512195122.\n",
      "Number of alerts is 1509.\n",
      "optimized cutoff is 0.3232877850532532\n",
      "The F1 score for this choice is 0.10810810810810811.\n",
      "Number of alerts is 1329.\n",
      "optimized cutoff is 0.07758833467960358\n",
      "The F1 score for this choice is 0.10526315789473684.\n",
      "Number of alerts is 2481.\n",
      "optimized cutoff is 0.43516790866851807\n",
      "The F1 score for this choice is 0.19999999999999998.\n",
      "Number of alerts is 1041.\n",
      "optimized cutoff is 0.09599556773900986\n",
      "The F1 score for this choice is 0.136986301369863.\n",
      "Number of alerts is 2262.\n",
      "optimized cutoff is 0.10658275336027145\n",
      "The F1 score for this choice is 0.16.\n",
      "Number of alerts is 2180.\n",
      "optimized cutoff is 0.3548651337623596\n",
      "The F1 score for this choice is 0.10810810810810811.\n",
      "Number of alerts is 1259.\n",
      "optimized cutoff is 0.9950612187385559\n",
      "The F1 score for this choice is 0.08695652173913045.\n",
      "Number of alerts is 30.\n",
      "optimized cutoff is 0.5944414138793945\n",
      "The F1 score for this choice is 0.13333333333333333.\n",
      "Number of alerts is 1495.\n",
      "optimized cutoff is 0.9999806880950928\n",
      "The F1 score for this choice is 0.04132231404958678.\n",
      "Number of alerts is 605.\n",
      "optimized cutoff is 0.9977745413780212\n",
      "The F1 score for this choice is 0.13333333333333333.\n",
      "Number of alerts is 216.\n",
      "optimized cutoff is 0.37432369589805603\n",
      "The F1 score for this choice is 0.12500000000000003.\n",
      "Number of alerts is 2132.\n",
      "optimized cutoff is 0.1983051747083664\n",
      "The F1 score for this choice is 0.09090909090909091.\n",
      "Number of alerts is 2368.\n",
      "optimized cutoff is 0.25311407446861267\n",
      "The F1 score for this choice is 0.09523809523809525.\n",
      "Number of alerts is 2574.\n",
      "optimized cutoff is 0.26525822281837463\n",
      "The F1 score for this choice is 0.09523809523809525.\n",
      "Number of alerts is 2490.\n",
      "optimized cutoff is 0.44991111755371094\n",
      "The F1 score for this choice is 0.1111111111111111.\n",
      "Number of alerts is 1964.\n",
      "optimized cutoff is 0.9876647591590881\n",
      "The F1 score for this choice is 0.027397260273972605.\n",
      "Number of alerts is 841.\n",
      "optimized cutoff is 0.795183002948761\n",
      "The F1 score for this choice is 0.08333333333333334.\n",
      "Number of alerts is 1187.\n",
      "optimized cutoff is 0.4201226532459259\n",
      "The F1 score for this choice is 0.24242424242424246.\n",
      "Number of alerts is 1983.\n",
      "optimized cutoff is 0.48128294944763184\n",
      "The F1 score for this choice is 0.21428571428571427.\n",
      "Number of alerts is 1703.\n",
      "optimized cutoff is 0.9997159838676453\n",
      "The F1 score for this choice is 0.1935483870967742.\n",
      "Number of alerts is 43.\n",
      "optimized cutoff is 0.9992265701293945\n",
      "The F1 score for this choice is 0.09836065573770493.\n",
      "Number of alerts is 128.\n",
      "optimized cutoff is 0.9812255501747131\n",
      "The F1 score for this choice is 0.04580152671755725.\n",
      "Number of alerts is 359.\n",
      "optimized cutoff is 0.9874336123466492\n",
      "The F1 score for this choice is 0.22222222222222218.\n",
      "Number of alerts is 356.\n",
      "optimized cutoff is 0.9605058431625366\n",
      "The F1 score for this choice is 0.03333333333333333.\n",
      "Number of alerts is 485.\n",
      "optimized cutoff is 0.22309428453445435\n",
      "The F1 score for this choice is 0.1951219512195122.\n",
      "Number of alerts is 2395.\n",
      "optimized cutoff is 0.14953218400478363\n",
      "The F1 score for this choice is 0.12903225806451613.\n",
      "Number of alerts is 2814.\n",
      "optimized cutoff is 0.5189877152442932\n",
      "The F1 score for this choice is 0.18749999999999997.\n",
      "Number of alerts is 1709.\n",
      "optimized cutoff is 0.9458238482475281\n",
      "The F1 score for this choice is 0.25000000000000006.\n",
      "Number of alerts is 731.\n",
      "optimized cutoff is 0.3041040897369385\n",
      "The F1 score for this choice is 0.16.\n",
      "Number of alerts is 2546.\n",
      "optimized cutoff is 0.04493408650159836\n",
      "The F1 score for this choice is 0.13740458015267176.\n",
      "Number of alerts is 4458.\n",
      "optimized cutoff is 0.13423749804496765\n",
      "The F1 score for this choice is 0.1176470588235294.\n",
      "Number of alerts is 3410.\n",
      "optimized cutoff is 0.05283422768115997\n",
      "The F1 score for this choice is 0.11864406779661017.\n",
      "Number of alerts is 4067.\n",
      "optimized cutoff is 0.07749232649803162\n",
      "The F1 score for this choice is 0.12820512820512822.\n",
      "Number of alerts is 3742.\n",
      "optimized cutoff is 0.23980575799942017\n",
      "The F1 score for this choice is 0.18181818181818182.\n",
      "Number of alerts is 2830.\n",
      "optimized cutoff is 0.21530501544475555\n",
      "The F1 score for this choice is 0.16.\n",
      "Number of alerts is 2620.\n",
      "optimized cutoff is 0.22008217871189117\n",
      "The F1 score for this choice is 0.28.\n",
      "Number of alerts is 2831.\n",
      "optimized cutoff is 0.48179197311401367\n",
      "The F1 score for this choice is 0.2608695652173913.\n",
      "Number of alerts is 1944.\n",
      "optimized cutoff is 0.1961522102355957\n",
      "The F1 score for this choice is 0.08888888888888888.\n",
      "Number of alerts is 1712.\n",
      "optimized cutoff is 0.5052913427352905\n",
      "The F1 score for this choice is 0.13793103448275862.\n",
      "Number of alerts is 809.\n",
      "optimized cutoff is 0.8845526576042175\n",
      "The F1 score for this choice is 0.08695652173913045.\n",
      "Number of alerts is 335.\n",
      "optimized cutoff is 0.1495232731103897\n",
      "The F1 score for this choice is 0.07017543859649122.\n",
      "Number of alerts is 1871.\n",
      "optimized cutoff is 0.4069293439388275\n",
      "The F1 score for this choice is 0.12121212121212123.\n",
      "Number of alerts is 1082.\n",
      "optimized cutoff is 0.47469329833984375\n",
      "The F1 score for this choice is 0.1142857142857143.\n",
      "Number of alerts is 1066.\n",
      "optimized cutoff is 0.6867136359214783\n",
      "The F1 score for this choice is 0.16.\n",
      "Number of alerts is 699.\n",
      "optimized cutoff is 0.2850133180618286\n",
      "The F1 score for this choice is 0.05714285714285715.\n",
      "Number of alerts is 1597.\n",
      "optimized cutoff is 0.9908605217933655\n",
      "The F1 score for this choice is 0.08888888888888888.\n",
      "Number of alerts is 249.\n",
      "optimized cutoff is 0.3596578538417816\n",
      "The F1 score for this choice is 0.12121212121212123.\n",
      "Number of alerts is 1238.\n",
      "optimized cutoff is 0.40545639395713806\n",
      "The F1 score for this choice is 0.15384615384615383.\n",
      "Number of alerts is 1159.\n",
      "optimized cutoff is 0.514039933681488\n",
      "The F1 score for this choice is 0.13793103448275862.\n",
      "Number of alerts is 984.\n",
      "optimized cutoff is 0.5111724138259888\n",
      "The F1 score for this choice is 0.1935483870967742.\n",
      "Number of alerts is 882.\n",
      "optimized cutoff is 0.36055517196655273\n",
      "The F1 score for this choice is 0.1951219512195122.\n",
      "Number of alerts is 1250.\n",
      "optimized cutoff is 0.6712466478347778\n",
      "The F1 score for this choice is 0.13793103448275862.\n",
      "Number of alerts is 844.\n",
      "optimized cutoff is 0.27477535605430603\n",
      "The F1 score for this choice is 0.1395348837209302.\n",
      "Number of alerts is 1267.\n",
      "optimized cutoff is 0.34497788548469543\n",
      "The F1 score for this choice is 0.15789473684210525.\n",
      "Number of alerts is 1009.\n",
      "optimized cutoff is 0.4937300980091095\n",
      "The F1 score for this choice is 0.13333333333333333.\n",
      "Number of alerts is 1003.\n",
      "optimized cutoff is 0.9383952617645264\n",
      "The F1 score for this choice is 0.16666666666666669.\n",
      "Number of alerts is 247.\n",
      "optimized cutoff is 0.8713515996932983\n",
      "The F1 score for this choice is 0.15384615384615385.\n",
      "Number of alerts is 362.\n",
      "optimized cutoff is 0.2804945707321167\n",
      "The F1 score for this choice is 0.11764705882352942.\n",
      "Number of alerts is 1669.\n",
      "optimized cutoff is 0.9593620300292969\n",
      "The F1 score for this choice is 0.04545454545454545.\n",
      "Number of alerts is 831.\n",
      "optimized cutoff is 0.8318997025489807\n",
      "The F1 score for this choice is 0.08695652173913045.\n",
      "Number of alerts is 649.\n",
      "optimized cutoff is 0.9958872199058533\n",
      "The F1 score for this choice is 0.02666666666666667.\n",
      "Number of alerts is 303.\n",
      "optimized cutoff is 0.978439211845398\n",
      "The F1 score for this choice is 0.0606060606060606.\n",
      "Number of alerts is 490.\n",
      "optimized cutoff is 0.8323027491569519\n",
      "The F1 score for this choice is 0.08695652173913045.\n",
      "Number of alerts is 745.\n",
      "optimized cutoff is 0.6493485569953918\n",
      "The F1 score for this choice is 0.14285714285714288.\n",
      "Number of alerts is 946.\n",
      "optimized cutoff is 0.8561854362487793\n",
      "The F1 score for this choice is 0.07692307692307693.\n",
      "Number of alerts is 615.\n",
      "optimized cutoff is 0.9364452362060547\n",
      "The F1 score for this choice is 0.011235955056179775.\n",
      "Number of alerts is 670.\n",
      "optimized cutoff is 0.8142462968826294\n",
      "The F1 score for this choice is 0.08333333333333334.\n",
      "Number of alerts is 704.\n",
      "optimized cutoff is 0.5141302347183228\n",
      "The F1 score for this choice is 0.08333333333333334.\n",
      "Number of alerts is 1309.\n",
      "optimized cutoff is 0.7912853360176086\n",
      "The F1 score for this choice is 0.08695652173913045.\n",
      "Number of alerts is 949.\n",
      "optimized cutoff is 0.999261736869812\n",
      "The F1 score for this choice is 0.09999999999999999.\n",
      "Number of alerts is 21.\n",
      "optimized cutoff is 0.8101779222488403\n",
      "The F1 score for this choice is 0.08695652173913045.\n",
      "Number of alerts is 964.\n",
      "optimized cutoff is 0.5998554825782776\n",
      "The F1 score for this choice is 0.07692307692307693.\n",
      "Number of alerts is 1285.\n",
      "optimized cutoff is 0.260838121175766\n",
      "The F1 score for this choice is 0.11764705882352942.\n",
      "Number of alerts is 1762.\n",
      "optimized cutoff is 0.6017217636108398\n",
      "The F1 score for this choice is 0.08695652173913045.\n",
      "Number of alerts is 1235.\n",
      "optimized cutoff is 0.4674064815044403\n",
      "The F1 score for this choice is 0.07142857142857144.\n",
      "Number of alerts is 1307.\n",
      "optimized cutoff is 0.22332867980003357\n",
      "The F1 score for this choice is 0.08888888888888888.\n",
      "Number of alerts is 2010.\n",
      "optimized cutoff is 0.6110382676124573\n",
      "The F1 score for this choice is 0.08333333333333334.\n",
      "Number of alerts is 1238.\n",
      "optimized cutoff is 0.515582263469696\n",
      "The F1 score for this choice is 0.06451612903225805.\n",
      "Number of alerts is 1590.\n",
      "optimized cutoff is 0.1755138337612152\n",
      "The F1 score for this choice is 0.09090909090909091.\n",
      "Number of alerts is 2632.\n",
      "optimized cutoff is 0.44955724477767944\n",
      "The F1 score for this choice is 0.06250000000000001.\n",
      "Number of alerts is 1801.\n",
      "optimized cutoff is 0.04968487098813057\n",
      "The F1 score for this choice is 0.03508771929824561.\n",
      "Number of alerts is 4125.\n",
      "optimized cutoff is 0.30581533908843994\n",
      "The F1 score for this choice is 0.060606060606060615.\n",
      "Number of alerts is 2156.\n",
      "optimized cutoff is 0.04422195255756378\n",
      "The F1 score for this choice is 0.03305785123966942.\n",
      "Number of alerts is 4154.\n",
      "optimized cutoff is 0.2597731053829193\n",
      "The F1 score for this choice is 0.09090909090909091.\n",
      "Number of alerts is 2307.\n",
      "optimized cutoff is 0.16089428961277008\n",
      "The F1 score for this choice is 0.06349206349206349.\n",
      "Number of alerts is 2795.\n",
      "optimized cutoff is 0.33833226561546326\n",
      "The F1 score for this choice is 0.0975609756097561.\n",
      "Number of alerts is 1836.\n",
      "optimized cutoff is 0.49849069118499756\n",
      "The F1 score for this choice is 0.05882352941176471.\n",
      "Number of alerts is 1739.\n",
      "optimized cutoff is 0.8250324130058289\n",
      "The F1 score for this choice is 0.16.\n",
      "Number of alerts is 1113.\n",
      "optimized cutoff is 0.8163691759109497\n",
      "The F1 score for this choice is 0.08.\n",
      "Number of alerts is 1144.\n",
      "optimized cutoff is 0.35394367575645447\n",
      "The F1 score for this choice is 0.16666666666666663.\n",
      "Number of alerts is 2033.\n",
      "optimized cutoff is 0.2250966876745224\n",
      "The F1 score for this choice is 0.0975609756097561.\n",
      "Number of alerts is 2665.\n",
      "optimized cutoff is 0.9999251365661621\n",
      "The F1 score for this choice is 0.1290322580645161.\n",
      "Number of alerts is 20.\n",
      "optimized cutoff is 0.10085833817720413\n",
      "The F1 score for this choice is 0.0759493670886076.\n",
      "Number of alerts is 3571.\n",
      "optimized cutoff is 0.9991780519485474\n",
      "The F1 score for this choice is 0.022727272727272728.\n",
      "Number of alerts is 211.\n",
      "optimized cutoff is 0.5075715780258179\n",
      "The F1 score for this choice is 0.1290322580645161.\n",
      "Number of alerts is 1738.\n",
      "optimized cutoff is 0.34025153517723083\n",
      "The F1 score for this choice is 0.15384615384615383.\n",
      "Number of alerts is 2290.\n",
      "optimized cutoff is 0.6374536156654358\n",
      "The F1 score for this choice is 0.14285714285714288.\n",
      "Number of alerts is 1749.\n",
      "optimized cutoff is 0.5288992524147034\n",
      "The F1 score for this choice is 0.06451612903225805.\n",
      "Number of alerts is 1919.\n",
      "optimized cutoff is 0.03749540075659752\n",
      "The F1 score for this choice is 0.037037037037037035.\n",
      "Number of alerts is 4743.\n",
      "optimized cutoff is 0.022510113194584846\n",
      "The F1 score for this choice is 0.04.\n",
      "Number of alerts is 4626.\n",
      "optimized cutoff is 0.505612313747406\n",
      "The F1 score for this choice is 0.07142857142857144.\n",
      "Number of alerts is 1985.\n",
      "optimized cutoff is 0.5122897028923035\n",
      "The F1 score for this choice is 0.06896551724137931.\n",
      "Number of alerts is 1980.\n",
      "optimized cutoff is 0.02180989645421505\n",
      "The F1 score for this choice is 0.040268456375838924.\n",
      "Number of alerts is 4322.\n",
      "optimized cutoff is 0.019992699846625328\n",
      "The F1 score for this choice is 0.03529411764705882.\n",
      "Number of alerts is 4839.\n",
      "optimized cutoff is 0.9604737758636475\n",
      "The F1 score for this choice is 0.01775147928994083.\n",
      "Number of alerts is 2171.\n",
      "optimized cutoff is 0.9171726107597351\n",
      "The F1 score for this choice is 0.023076923076923078.\n",
      "Number of alerts is 1325.\n",
      "optimized cutoff is 0.9456507563591003\n",
      "The F1 score for this choice is 0.024193548387096774.\n",
      "Number of alerts is 1502.\n",
      "optimized cutoff is 0.9105278849601746\n",
      "The F1 score for this choice is 0.08695652173913045.\n",
      "Number of alerts is 471.\n",
      "optimized cutoff is 0.8615570664405823\n",
      "The F1 score for this choice is 0.015228426395939087.\n",
      "Number of alerts is 1335.\n",
      "optimized cutoff is 0.9978777766227722\n",
      "The F1 score for this choice is 0.05882352941176471.\n",
      "Number of alerts is 15.\n",
      "optimized cutoff is 0.5476285219192505\n",
      "The F1 score for this choice is 0.13333333333333333.\n",
      "Number of alerts is 1234.\n",
      "optimized cutoff is 0.48533061146736145\n",
      "The F1 score for this choice is 0.1111111111111111.\n",
      "Number of alerts is 1141.\n",
      "optimized cutoff is 0.281656414270401\n",
      "The F1 score for this choice is 0.09523809523809525.\n",
      "Number of alerts is 1814.\n",
      "optimized cutoff is 0.9485172033309937\n",
      "The F1 score for this choice is 0.08695652173913045.\n",
      "Number of alerts is 480.\n",
      "optimized cutoff is 0.8410016298294067\n",
      "The F1 score for this choice is 0.07692307692307693.\n",
      "Number of alerts is 644.\n",
      "optimized cutoff is 0.7549752593040466\n",
      "The F1 score for this choice is 0.08.\n",
      "Number of alerts is 1041.\n",
      "optimized cutoff is 0.3826671242713928\n",
      "The F1 score for this choice is 0.1142857142857143.\n",
      "Number of alerts is 1512.\n",
      "optimized cutoff is 0.4845133423805237\n",
      "The F1 score for this choice is 0.2777777777777778.\n",
      "Number of alerts is 1805.\n",
      "optimized cutoff is 0.39341288805007935\n",
      "The F1 score for this choice is 0.27027027027027023.\n",
      "Number of alerts is 2039.\n",
      "optimized cutoff is 0.456453412771225\n",
      "The F1 score for this choice is 0.25000000000000006.\n",
      "Number of alerts is 1847.\n",
      "optimized cutoff is 0.4852302074432373\n",
      "The F1 score for this choice is 0.2777777777777778.\n",
      "Number of alerts is 1816.\n",
      "optimized cutoff is 0.1908482015132904\n",
      "The F1 score for this choice is 0.21276595744680854.\n",
      "Number of alerts is 2540.\n",
      "optimized cutoff is 0.4970085918903351\n",
      "The F1 score for this choice is 0.23529411764705885.\n",
      "Number of alerts is 1766.\n",
      "optimized cutoff is 0.4911264479160309\n",
      "The F1 score for this choice is 0.23809523809523808.\n",
      "Number of alerts is 1637.\n",
      "optimized cutoff is 0.1916538029909134\n",
      "The F1 score for this choice is 0.18181818181818182.\n",
      "Number of alerts is 2887.\n",
      "optimized cutoff is 0.13880719244480133\n",
      "The F1 score for this choice is 0.21052631578947367.\n",
      "Number of alerts is 2939.\n",
      "optimized cutoff is 0.23618003726005554\n",
      "The F1 score for this choice is 0.2553191489361702.\n",
      "Number of alerts is 2474.\n"
     ]
    }
   ],
   "source": [
    "for org_num in range(2, 20):\n",
    "    Answers_for_each_org_dict[org_num] = get_all_answers_for_org(org_num, answer_iterations=10)\n",
    "    pd.DataFrame(Answers_for_each_org_dict[org_num]).to_csv(\"/Users/mimran/Google Drive/GMU SCITE/RCPs Fifth Quarter/RCP14/RCP14_algo2_Jul23_DVersion_Case3_3weights_answers/algo2_July23_10iter_answers_for_org_{}.csv\".format(org_num), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-4fb76b3d73b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mAnswers_for_each_org_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0morg_num\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mAnswers_for_each_org_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0morg_num\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_all_answers_for_org\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morg_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manswer_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAnswers_for_each_org_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0morg_num\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/Users/mimran/Google Drive/GMU SCITE/RCPs Fifth Quarter/RCP14/RCP14_algo2_Jul23_DVersion_Case3_3weights_answers/algo2_July23_10iter_answers_for_org_{}.csv\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morg_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-db79ee318611>\u001b[0m in \u001b[0;36mget_all_answers_for_org\u001b[0;34m(simulated_org_number, iter_per_weight, answer_iterations)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mT_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_generated_attributes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_test_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaler_from_training_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     return RCP14_Algorithm_2(S1_x, S2_x, S3_x, S1_y, S2_y, S3_y, T_x, T_y, \n\u001b[0;32m---> 13\u001b[0;31m                       T_generated_attributes, best_weights = [170, 180, 30, 40], num_iterations = answer_iterations, r_p_vector=[1, 1, 1, 1])\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-000499d2f9d7>\u001b[0m in \u001b[0;36mRCP14_Algorithm_2\u001b[0;34m(S1_x, S2_x, S3_x, S1_y, S2_y, S3_y, T_x, T_y, T_generated_attributes, best_weights, num_iterations, r_p_vector)\u001b[0m\n\u001b[1;32m      5\u001b[0m                                     relative_probability_vector=r_p_vector)\n\u001b[1;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0miteration_num\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mcurrent_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS1_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS1_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS2_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS2_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchosen_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0miteration_num\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mmodel_prediction_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS3_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[1;31m#print(\"Model prediction output looks like\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-784c9cbfd59c>\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(S1_x, S1_y, S2_x, S2_y, test_class_weight)\u001b[0m\n\u001b[1;32m     15\u001b[0m                       \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mS2_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS2_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                       \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                       class_weight = {0:1, 1:test_class_weight}, verbose = 0)\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcurrent_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    843\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    846\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1483\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2071\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2073\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2074\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Answers_for_each_org_dict = {}\n",
    "for org_num in range(20):\n",
    "    Answers_for_each_org_dict[org_num] = get_all_answers_for_org(org_num, answer_iterations=10)\n",
    "    pd.DataFrame(Answers_for_each_org_dict[org_num]).to_csv(\"/Users/mimran/Google Drive/GMU SCITE/RCPs Fifth Quarter/RCP14/RCP14_algo2_Jul23_DVersion_Case3_3weights_answers/algo2_July23_10iter_answers_for_org_{}.csv\".format(org_num), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.concat(pd.DataFrame(Answers_for_each_org_dict[org_num]) for org_num in range(30)).to_csv(\"/Users/mimran/Google Drive/GMU SCITE/RCPs Fifth Quarter/RCP14/RCP14_algo2_Jul23_DVersion_Case3_3weights_answers/MultiIter10Answers_for_30_orgs.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_answers_for_org(simulated_org_number, iter_per_weight = 25, answer_iterations = 1):\n",
    "    training_data = read_org_data(simulated_org_number, 7, 33)\n",
    "    training_data.replace([-np.inf,np.inf], np.nan, inplace=True)\n",
    "    training_data.dropna(inplace=True)\n",
    "    S1_x, S2_x, S3_x, S1_y, S2_y, S3_y, scaler_from_training_data = split_training_data(training_data)\n",
    "    #weight_to_use = RCP14_Algorithm_1(S1_x, S2_x, S3_x, S1_y, S2_y, S3_y, \n",
    "    #                                  iter_per_weight = iter_per_weight)\n",
    "    test_data = read_org_test_data(simulated_org_number, 34, 49)\n",
    "    test_data.replace([-np.inf,np.inf], np.nan, inplace=True)\n",
    "    test_data.dropna(inplace=True)\n",
    "    T_x, T_y, T_generated_attributes = split_test_data(test_data, scaler_from_training_data)\n",
    "    return RCP14_Algorithm_2(S1_x, S2_x, S3_x, S1_y, S2_y, S3_y, T_x, T_y, \n",
    "                      T_generated_attributes, best_weights = [170, 180, 30, 40], num_iterations = answer_iterations, r_p_vector=[1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized cutoff is 0.04261957108974457\n",
      "The F1 score for this choice is 0.09090909090909091.\n",
      "Number of alerts is 3792.\n",
      "optimized cutoff is 0.021133609116077423\n",
      "The F1 score for this choice is 0.0821917808219178.\n",
      "Number of alerts is 4397.\n",
      "optimized cutoff is 0.2168835997581482\n",
      "The F1 score for this choice is 0.10344827586206896.\n",
      "Number of alerts is 2572.\n",
      "optimized cutoff is 0.9844028949737549\n",
      "The F1 score for this choice is 0.039473684210526314.\n",
      "Number of alerts is 174.\n",
      "optimized cutoff is 0.19433560967445374\n",
      "The F1 score for this choice is 0.05882352941176471.\n",
      "Number of alerts is 2308.\n",
      "optimized cutoff is 0.07383988797664642\n",
      "The F1 score for this choice is 0.11494252873563221.\n",
      "Number of alerts is 3575.\n",
      "optimized cutoff is 0.20814576745033264\n",
      "The F1 score for this choice is 0.09523809523809525.\n",
      "Number of alerts is 2512.\n",
      "optimized cutoff is 0.3529604375362396\n",
      "The F1 score for this choice is 0.08.\n",
      "Number of alerts is 1462.\n",
      "optimized cutoff is 0.12533336877822876\n",
      "The F1 score for this choice is 0.08823529411764705.\n",
      "Number of alerts is 3278.\n",
      "optimized cutoff is 0.06391500681638718\n",
      "The F1 score for this choice is 0.09230769230769229.\n",
      "Number of alerts is 3201.\n",
      "optimized cutoff is 0.0069377245381474495\n",
      "The F1 score for this choice is 0.03414634146341463.\n",
      "Number of alerts is 6730.\n",
      "optimized cutoff is 0.0017030402086675167\n",
      "The F1 score for this choice is 0.018944519621109605.\n",
      "Number of alerts is 7934.\n",
      "optimized cutoff is 0.015623204410076141\n",
      "The F1 score for this choice is 0.02870813397129187.\n",
      "Number of alerts is 4922.\n",
      "optimized cutoff is 0.004856551066040993\n",
      "The F1 score for this choice is 0.018306636155606407.\n",
      "Number of alerts is 6595.\n",
      "optimized cutoff is 0.012182801030576229\n",
      "The F1 score for this choice is 0.029850746268656716.\n",
      "Number of alerts is 4747.\n",
      "optimized cutoff is 0.005171026568859816\n",
      "The F1 score for this choice is 0.027932960893854747.\n",
      "Number of alerts is 5918.\n",
      "optimized cutoff is 0.017804505303502083\n",
      "The F1 score for this choice is 0.023391812865497075.\n",
      "Number of alerts is 4854.\n",
      "optimized cutoff is 0.004398978780955076\n",
      "The F1 score for this choice is 0.017857142857142856.\n",
      "Number of alerts is 6391.\n",
      "optimized cutoff is 0.0030452033970505\n",
      "The F1 score for this choice is 0.024013722126929673.\n",
      "Number of alerts is 8163.\n",
      "optimized cutoff is 0.0452098622918129\n",
      "The F1 score for this choice is 0.04597701149425288.\n",
      "Number of alerts is 3803.\n",
      "optimized cutoff is 0.020144810900092125\n",
      "The F1 score for this choice is 0.046296296296296294.\n",
      "Number of alerts is 5153.\n",
      "optimized cutoff is 0.01874607801437378\n",
      "The F1 score for this choice is 0.046296296296296294.\n",
      "Number of alerts is 5344.\n",
      "optimized cutoff is 0.01745002157986164\n",
      "The F1 score for this choice is 0.024489795918367346.\n",
      "Number of alerts is 5358.\n",
      "optimized cutoff is 0.018104229122400284\n",
      "The F1 score for this choice is 0.03571428571428572.\n",
      "Number of alerts is 6172.\n",
      "optimized cutoff is 0.015243567526340485\n",
      "The F1 score for this choice is 0.05555555555555555.\n",
      "Number of alerts is 5650.\n",
      "optimized cutoff is 0.25644463300704956\n",
      "The F1 score for this choice is 0.04444444444444444.\n",
      "Number of alerts is 2905.\n",
      "optimized cutoff is 0.015578659251332283\n",
      "The F1 score for this choice is 0.03333333333333333.\n",
      "Number of alerts is 5482.\n",
      "optimized cutoff is 0.00772301247343421\n",
      "The F1 score for this choice is 0.030612244897959186.\n",
      "Number of alerts is 7294.\n",
      "optimized cutoff is 0.011245950125157833\n",
      "The F1 score for this choice is 0.03309692671394799.\n",
      "Number of alerts is 7032.\n",
      "optimized cutoff is 0.005416755564510822\n",
      "The F1 score for this choice is 0.034334763948497854.\n",
      "Number of alerts is 7404.\n",
      "optimized cutoff is 0.3184521496295929\n",
      "The F1 score for this choice is 0.23529411764705885.\n",
      "Number of alerts is 1313.\n",
      "optimized cutoff is 0.5047919154167175\n",
      "The F1 score for this choice is 0.22222222222222218.\n",
      "Number of alerts is 845.\n",
      "optimized cutoff is 0.39727428555488586\n",
      "The F1 score for this choice is 0.23529411764705885.\n",
      "Number of alerts is 944.\n",
      "optimized cutoff is 0.9800862073898315\n",
      "The F1 score for this choice is 0.12987012987012989.\n",
      "Number of alerts is 411.\n",
      "optimized cutoff is 0.3360012173652649\n",
      "The F1 score for this choice is 0.19999999999999998.\n",
      "Number of alerts is 1895.\n",
      "optimized cutoff is 0.32532134652137756\n",
      "The F1 score for this choice is 0.2285714285714286.\n",
      "Number of alerts is 1004.\n",
      "optimized cutoff is 0.9999642372131348\n",
      "The F1 score for this choice is 0.14634146341463414.\n",
      "Number of alerts is 49.\n",
      "optimized cutoff is 0.9121118783950806\n",
      "The F1 score for this choice is 0.22222222222222218.\n",
      "Number of alerts is 603.\n",
      "optimized cutoff is 0.8439007997512817\n",
      "The F1 score for this choice is 0.21428571428571427.\n",
      "Number of alerts is 1006.\n",
      "optimized cutoff is 0.7022560834884644\n",
      "The F1 score for this choice is 0.16.\n",
      "Number of alerts is 1044.\n",
      "optimized cutoff is 0.09410934150218964\n",
      "The F1 score for this choice is 0.1568627450980392.\n",
      "Number of alerts is 2315.\n",
      "optimized cutoff is 0.2887754440307617\n",
      "The F1 score for this choice is 0.09302325581395349.\n",
      "Number of alerts is 1983.\n",
      "optimized cutoff is 0.2795393466949463\n",
      "The F1 score for this choice is 0.06896551724137931.\n",
      "Number of alerts is 1251.\n",
      "optimized cutoff is 0.9750773310661316\n",
      "The F1 score for this choice is 0.026143790849673203.\n",
      "Number of alerts is 2150.\n",
      "optimized cutoff is 0.9999873638153076\n",
      "The F1 score for this choice is 0.1408450704225352.\n",
      "Number of alerts is 222.\n",
      "optimized cutoff is 0.12903328239917755\n",
      "The F1 score for this choice is 0.04166666666666667.\n",
      "Number of alerts is 2688.\n",
      "optimized cutoff is 0.027746494859457016\n",
      "The F1 score for this choice is 0.058823529411764705.\n",
      "Number of alerts is 3744.\n",
      "optimized cutoff is 0.050281789153814316\n",
      "The F1 score for this choice is 0.0449438202247191.\n",
      "Number of alerts is 3408.\n",
      "optimized cutoff is 0.7800175547599792\n",
      "The F1 score for this choice is 0.014760147601476013.\n",
      "Number of alerts is 2496.\n",
      "optimized cutoff is 0.15575852990150452\n",
      "The F1 score for this choice is 0.09090909090909091.\n",
      "Number of alerts is 1850.\n",
      "optimized cutoff is 0.18461400270462036\n",
      "The F1 score for this choice is 0.1333333333333333.\n",
      "Number of alerts is 2299.\n",
      "optimized cutoff is 0.27439209818840027\n",
      "The F1 score for this choice is 0.1904761904761905.\n",
      "Number of alerts is 1699.\n",
      "optimized cutoff is 0.16676676273345947\n",
      "The F1 score for this choice is 0.09302325581395349.\n",
      "Number of alerts is 1723.\n",
      "optimized cutoff is 0.07839242368936539\n",
      "The F1 score for this choice is 0.08695652173913043.\n",
      "Number of alerts is 2626.\n",
      "optimized cutoff is 0.0886714830994606\n",
      "The F1 score for this choice is 0.08695652173913043.\n",
      "Number of alerts is 2395.\n",
      "optimized cutoff is 0.022291308268904686\n",
      "The F1 score for this choice is 0.0625.\n",
      "Number of alerts is 4296.\n",
      "optimized cutoff is 0.17109531164169312\n",
      "The F1 score for this choice is 0.10810810810810811.\n",
      "Number of alerts is 1816.\n",
      "optimized cutoff is 0.1056254580616951\n",
      "The F1 score for this choice is 0.11320754716981131.\n",
      "Number of alerts is 2341.\n",
      "optimized cutoff is 0.432837575674057\n",
      "The F1 score for this choice is 0.13333333333333333.\n",
      "Number of alerts is 1042.\n",
      "optimized cutoff is 0.6603419780731201\n",
      "The F1 score for this choice is 0.08333333333333334.\n",
      "Number of alerts is 738.\n",
      "optimized cutoff is 0.22046968340873718\n",
      "The F1 score for this choice is 0.10810810810810811.\n",
      "Number of alerts is 1796.\n",
      "optimized cutoff is 0.11401430517435074\n",
      "The F1 score for this choice is 0.09523809523809522.\n",
      "Number of alerts is 2396.\n",
      "optimized cutoff is 0.9893859624862671\n",
      "The F1 score for this choice is 0.15384615384615383.\n",
      "Number of alerts is 421.\n",
      "optimized cutoff is 0.1032455787062645\n",
      "The F1 score for this choice is 0.07142857142857142.\n",
      "Number of alerts is 2633.\n",
      "optimized cutoff is 0.33890488743782043\n",
      "The F1 score for this choice is 0.07142857142857144.\n",
      "Number of alerts is 1328.\n",
      "optimized cutoff is 0.9796465039253235\n",
      "The F1 score for this choice is 0.09302325581395349.\n",
      "Number of alerts is 864.\n",
      "optimized cutoff is 0.2581721246242523\n",
      "The F1 score for this choice is 0.10256410256410256.\n",
      "Number of alerts is 1830.\n",
      "optimized cutoff is 0.17734724283218384\n",
      "The F1 score for this choice is 0.07407407407407407.\n",
      "Number of alerts is 2227.\n",
      "optimized cutoff is 0.07580063492059708\n",
      "The F1 score for this choice is 0.1176470588235294.\n",
      "Number of alerts is 2874.\n",
      "optimized cutoff is 0.05085785686969757\n",
      "The F1 score for this choice is 0.06956521739130433.\n",
      "Number of alerts is 3651.\n",
      "optimized cutoff is 0.18956147134304047\n",
      "The F1 score for this choice is 0.08888888888888888.\n",
      "Number of alerts is 1308.\n",
      "optimized cutoff is 0.22973474860191345\n",
      "The F1 score for this choice is 0.21621621621621623.\n",
      "Number of alerts is 1227.\n",
      "optimized cutoff is 0.4302818179130554\n",
      "The F1 score for this choice is 0.20689655172413793.\n",
      "Number of alerts is 971.\n",
      "optimized cutoff is 0.5934954881668091\n",
      "The F1 score for this choice is 0.08695652173913045.\n",
      "Number of alerts is 649.\n",
      "optimized cutoff is 0.1606997400522232\n",
      "The F1 score for this choice is 0.09836065573770493.\n",
      "Number of alerts is 1974.\n",
      "optimized cutoff is 0.45969080924987793\n",
      "The F1 score for this choice is 0.10526315789473685.\n",
      "Number of alerts is 1239.\n",
      "optimized cutoff is 0.11488839238882065\n",
      "The F1 score for this choice is 0.15151515151515152.\n",
      "Number of alerts is 2058.\n",
      "optimized cutoff is 0.32870376110076904\n",
      "The F1 score for this choice is 0.11764705882352942.\n",
      "Number of alerts is 1446.\n",
      "optimized cutoff is 0.07670558243989944\n",
      "The F1 score for this choice is 0.14492753623188404.\n",
      "Number of alerts is 2295.\n",
      "optimized cutoff is 0.2568380832672119\n",
      "The F1 score for this choice is 0.18181818181818182.\n",
      "Number of alerts is 1565.\n",
      "optimized cutoff is 0.9849475026130676\n",
      "The F1 score for this choice is 0.08.\n",
      "Number of alerts is 285.\n",
      "optimized cutoff is 0.6187793612480164\n",
      "The F1 score for this choice is 0.1290322580645161.\n",
      "Number of alerts is 532.\n",
      "optimized cutoff is 0.9211817383766174\n",
      "The F1 score for this choice is 0.16.\n",
      "Number of alerts is 464.\n",
      "optimized cutoff is 0.6117420196533203\n",
      "The F1 score for this choice is 0.12121212121212123.\n",
      "Number of alerts is 810.\n",
      "optimized cutoff is 0.8979555368423462\n",
      "The F1 score for this choice is 0.0975609756097561.\n",
      "Number of alerts is 198.\n",
      "optimized cutoff is 0.8408496379852295\n",
      "The F1 score for this choice is 0.15384615384615385.\n",
      "Number of alerts is 477.\n",
      "optimized cutoff is 0.7671438455581665\n",
      "The F1 score for this choice is 0.14285714285714288.\n",
      "Number of alerts is 436.\n",
      "optimized cutoff is 0.2892002463340759\n",
      "The F1 score for this choice is 0.12121212121212123.\n",
      "Number of alerts is 991.\n",
      "optimized cutoff is 0.8226810693740845\n",
      "The F1 score for this choice is 0.08695652173913045.\n",
      "Number of alerts is 262.\n",
      "optimized cutoff is 0.6360616087913513\n",
      "The F1 score for this choice is 0.14285714285714288.\n",
      "Number of alerts is 621.\n",
      "optimized cutoff is 0.2779749631881714\n",
      "The F1 score for this choice is 0.15384615384615383.\n",
      "Number of alerts is 1151.\n",
      "optimized cutoff is 0.10532546788454056\n",
      "The F1 score for this choice is 0.14814814814814814.\n",
      "Number of alerts is 2636.\n",
      "optimized cutoff is 0.0934099555015564\n",
      "The F1 score for this choice is 0.16949152542372883.\n",
      "Number of alerts is 2354.\n",
      "optimized cutoff is 0.3217516541481018\n",
      "The F1 score for this choice is 0.14634146341463414.\n",
      "Number of alerts is 1701.\n",
      "optimized cutoff is 0.2110108882188797\n",
      "The F1 score for this choice is 0.14634146341463414.\n",
      "Number of alerts is 1917.\n",
      "optimized cutoff is 0.08887047320604324\n",
      "The F1 score for this choice is 0.17857142857142858.\n",
      "Number of alerts is 2281.\n",
      "optimized cutoff is 0.2886843979358673\n",
      "The F1 score for this choice is 0.27586206896551724.\n",
      "Number of alerts is 1142.\n",
      "optimized cutoff is 0.9921608567237854\n",
      "The F1 score for this choice is 0.10256410256410256.\n",
      "Number of alerts is 427.\n",
      "optimized cutoff is 0.42545950412750244\n",
      "The F1 score for this choice is 0.1290322580645161.\n",
      "Number of alerts is 1230.\n",
      "optimized cutoff is 0.14220775663852692\n",
      "The F1 score for this choice is 0.1702127659574468.\n",
      "Number of alerts is 1900.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-7755fb865c95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mAnswers_for_each_org_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0morg_num\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mAnswers_for_each_org_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0morg_num\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_all_answers_for_org\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morg_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manswer_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAnswers_for_each_org_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0morg_num\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/Mimran/Google Drive/GMU SCITE/RCPs Fifth Quarter/RCP14/RCP14_algo2_Jul24_DVersion_Case3_4weights_answers/algo2_July24_10iter_answers_for_org_{}.csv\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morg_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-db79ee318611>\u001b[0m in \u001b[0;36mget_all_answers_for_org\u001b[0;34m(simulated_org_number, iter_per_weight, answer_iterations)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mT_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_generated_attributes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_test_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaler_from_training_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     return RCP14_Algorithm_2(S1_x, S2_x, S3_x, S1_y, S2_y, S3_y, T_x, T_y, \n\u001b[0;32m---> 13\u001b[0;31m                       T_generated_attributes, best_weights = [170, 180, 30, 40], num_iterations = answer_iterations, r_p_vector=[1, 1, 1, 1])\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-000499d2f9d7>\u001b[0m in \u001b[0;36mRCP14_Algorithm_2\u001b[0;34m(S1_x, S2_x, S3_x, S1_y, S2_y, S3_y, T_x, T_y, T_generated_attributes, best_weights, num_iterations, r_p_vector)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[1;31m#print(\"Model prediction output looks like\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[1;31m#print(model_prediction_output[0:10])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mchosen_tau\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_threshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue_y_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mS3_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_results\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmodel_prediction_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"optimized cutoff is {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchosen_tau\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The F1 score for this choice is {}.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-edab62f468ad>\u001b[0m in \u001b[0;36mfind_threshold\u001b[0;34m(true_y_values, pred_results)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[1;31m# Score Thresholds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue_y_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpred_results\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mthres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'int'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mthres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Answers_for_each_org_dict = {}\n",
    "for org_num in range(20):\n",
    "    Answers_for_each_org_dict[org_num] = get_all_answers_for_org(org_num, answer_iterations=10)\n",
    "    pd.DataFrame(Answers_for_each_org_dict[org_num]).to_csv(\"C:/Users/Mimran/Google Drive/GMU SCITE/RCPs Fifth Quarter/RCP14/RCP14_algo2_Jul24_DVersion_Case3_4weights_answers/algo2_July24_10iter_answers_for_org_{}.csv\".format(org_num), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Answers_for_each_org_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.concat(pd.DataFrame(Answers_for_each_org_dict[org_num]) for org_num in range(10)).to_csv(\"/Users/mimran/Google Drive/GMU SCITE/RCPs Fifth Quarter/RCP14/RCP14_algo2_Jul24_DVersion_Case3_4weights_answers/MultiIter10Answers_for_orgs0thru9.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(Answers_for_each_org_dict).T.astype(float).to_csv(\"Mohanad_prelim_answers.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_answers_for_org_alt_Mohanad(simulated_org_number, iter_per_weight = 25, answer_iterations = 1):\n",
    "    training_data = read_org_data(simulated_org_number, 7, 33)\n",
    "    training_data.replace([-np.inf,np.inf], np.nan, inplace=True)\n",
    "    training_data.dropna(inplace=True)\n",
    "    S1_x, S2_x, S3_x, S1_y, S2_y, S3_y, scaler_from_training_data = split_training_data(training_data)\n",
    "    #weight_to_use = RCP14_Algorithm_1(S1_x, S2_x, S3_x, S1_y, S2_y, S3_y, \n",
    "    #                                  iter_per_weight = iter_per_weight)\n",
    "    weight_to_use = 180\n",
    "    test_data = read_org_test_data(simulated_org_number, 36, 49)\n",
    "    test_data.replace([-np.inf,np.inf], np.nan, inplace=True)\n",
    "    test_data.dropna(inplace=True)\n",
    "    T_x, T_y, T_generated_attributes = split_test_data(test_data, scaler_from_training_data)\n",
    "    return RCP14_Algorithm_2(S1_x, S2_x, S3_x, S1_y, S2_y, S3_y, T_x, T_y, \n",
    "                      T_generated_attributes, weight_to_use, num_iterations = answer_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized cutoff is 0.8155121803283691\n",
      "The F1 score for this choice is 0.08695652173913045.\n",
      "Number of alerts is 665.\n",
      "optimized cutoff is 0.9124085903167725\n",
      "The F1 score for this choice is 0.07692307692307693.\n",
      "Number of alerts is 625.\n",
      "optimized cutoff is 0.9997060894966125\n",
      "The F1 score for this choice is 0.07692307692307693.\n",
      "Number of alerts is 62.\n",
      "optimized cutoff is 0.9179666638374329\n",
      "The F1 score for this choice is 0.08695652173913045.\n",
      "Number of alerts is 607.\n",
      "optimized cutoff is 0.9999988079071045\n",
      "The F1 score for this choice is 0.04878048780487805.\n",
      "Number of alerts is 147.\n",
      "optimized cutoff is 0.8741991519927979\n",
      "The F1 score for this choice is 0.08333333333333334.\n",
      "Number of alerts is 676.\n",
      "optimized cutoff is 0.9641646146774292\n",
      "The F1 score for this choice is 0.08695652173913045.\n",
      "Number of alerts is 386.\n",
      "optimized cutoff is 0.8126572370529175\n",
      "The F1 score for this choice is 0.08333333333333334.\n",
      "Number of alerts is 668.\n",
      "optimized cutoff is 0.9999444484710693\n",
      "The F1 score for this choice is 0.00625.\n",
      "Number of alerts is 1857.\n",
      "optimized cutoff is 0.9180194139480591\n",
      "The F1 score for this choice is 0.08695652173913045.\n",
      "Number of alerts is 607.\n",
      "optimized cutoff is 0.014615917578339577\n",
      "The F1 score for this choice is 0.015444015444015444.\n",
      "Number of alerts is 4070.\n",
      "optimized cutoff is 0.9976632595062256\n",
      "The F1 score for this choice is 0.036585365853658534.\n",
      "Number of alerts is 462.\n",
      "optimized cutoff is 0.08336986601352692\n",
      "The F1 score for this choice is 0.008750607681088964.\n",
      "Number of alerts is 6557.\n",
      "optimized cutoff is 0.0006275217165239155\n",
      "The F1 score for this choice is 0.008915304606240713.\n",
      "Number of alerts is 7946.\n",
      "optimized cutoff is 0.37988176941871643\n",
      "The F1 score for this choice is 0.05128205128205128.\n",
      "Number of alerts is 1539.\n",
      "optimized cutoff is 0.11825218796730042\n",
      "The F1 score for this choice is 0.03636363636363636.\n",
      "Number of alerts is 2179.\n",
      "optimized cutoff is 0.001500993617810309\n",
      "The F1 score for this choice is 0.015909090909090907.\n",
      "Number of alerts is 6318.\n",
      "optimized cutoff is 0.004202194046229124\n",
      "The F1 score for this choice is 0.01609657947686117.\n",
      "Number of alerts is 4982.\n",
      "optimized cutoff is 0.03894324600696564\n",
      "The F1 score for this choice is 0.01652892561983471.\n",
      "Number of alerts is 2980.\n",
      "optimized cutoff is 0.026573341339826584\n",
      "The F1 score for this choice is 0.014388489208633096.\n",
      "Number of alerts is 3228.\n",
      "optimized cutoff is 0.42393916845321655\n",
      "The F1 score for this choice is 0.14814814814814814.\n",
      "Number of alerts is 1285.\n",
      "optimized cutoff is 0.7831906080245972\n",
      "The F1 score for this choice is 0.056603773584905655.\n",
      "Number of alerts is 1200.\n",
      "optimized cutoff is 0.8371103405952454\n",
      "The F1 score for this choice is 0.16.\n",
      "Number of alerts is 451.\n",
      "optimized cutoff is 0.2175491452217102\n",
      "The F1 score for this choice is 0.1111111111111111.\n",
      "Number of alerts is 1591.\n",
      "optimized cutoff is 0.5192146301269531\n",
      "The F1 score for this choice is 0.07142857142857144.\n",
      "Number of alerts is 874.\n",
      "optimized cutoff is 0.6619579792022705\n",
      "The F1 score for this choice is 0.07692307692307693.\n",
      "Number of alerts is 972.\n",
      "optimized cutoff is 0.3477037847042084\n",
      "The F1 score for this choice is 0.1142857142857143.\n",
      "Number of alerts is 1252.\n",
      "optimized cutoff is 0.045409802347421646\n",
      "The F1 score for this choice is 0.05405405405405405.\n",
      "Number of alerts is 2625.\n",
      "optimized cutoff is 0.4954127371311188\n",
      "The F1 score for this choice is 0.13333333333333333.\n",
      "Number of alerts is 1011.\n",
      "optimized cutoff is 0.4250834882259369\n",
      "The F1 score for this choice is 0.12500000000000003.\n",
      "Number of alerts is 1200.\n",
      "optimized cutoff is 0.9998385906219482\n",
      "The F1 score for this choice is 0.11320754716981131.\n",
      "Number of alerts is 273.\n",
      "optimized cutoff is 0.15563060343265533\n",
      "The F1 score for this choice is 0.19607843137254902.\n",
      "Number of alerts is 2343.\n",
      "optimized cutoff is 0.32509681582450867\n",
      "The F1 score for this choice is 0.37499999999999994.\n",
      "Number of alerts is 1910.\n",
      "optimized cutoff is 0.9998103976249695\n",
      "The F1 score for this choice is 0.03821656050955414.\n",
      "Number of alerts is 614.\n",
      "optimized cutoff is 0.998823344707489\n",
      "The F1 score for this choice is 0.11764705882352942.\n",
      "Number of alerts is 521.\n",
      "optimized cutoff is 0.2665103077888489\n",
      "The F1 score for this choice is 0.21621621621621623.\n",
      "Number of alerts is 1928.\n",
      "optimized cutoff is 0.9997628331184387\n",
      "The F1 score for this choice is 0.0392156862745098.\n",
      "Number of alerts is 147.\n",
      "optimized cutoff is 0.2486853301525116\n",
      "The F1 score for this choice is 0.21276595744680854.\n",
      "Number of alerts is 1984.\n",
      "optimized cutoff is 0.3663250505924225\n",
      "The F1 score for this choice is 0.3428571428571428.\n",
      "Number of alerts is 1860.\n",
      "optimized cutoff is 0.23893582820892334\n",
      "The F1 score for this choice is 0.19999999999999998.\n",
      "Number of alerts is 2265.\n",
      "optimized cutoff is 0.16269129514694214\n",
      "The F1 score for this choice is 0.1395348837209302.\n",
      "Number of alerts is 2057.\n",
      "optimized cutoff is 0.2697516977787018\n",
      "The F1 score for this choice is 0.09999999999999999.\n",
      "Number of alerts is 1546.\n",
      "optimized cutoff is 0.1857898086309433\n",
      "The F1 score for this choice is 0.13636363636363635.\n",
      "Number of alerts is 1681.\n",
      "optimized cutoff is 0.20169015228748322\n",
      "The F1 score for this choice is 0.11764705882352941.\n",
      "Number of alerts is 1677.\n",
      "optimized cutoff is 0.1243545264005661\n",
      "The F1 score for this choice is 0.08823529411764705.\n",
      "Number of alerts is 2487.\n",
      "optimized cutoff is 0.9999052286148071\n",
      "The F1 score for this choice is 0.1764705882352941.\n",
      "Number of alerts is 112.\n",
      "optimized cutoff is 0.21347518265247345\n",
      "The F1 score for this choice is 0.0975609756097561.\n",
      "Number of alerts is 1918.\n",
      "optimized cutoff is 0.10859356820583344\n",
      "The F1 score for this choice is 0.07142857142857142.\n",
      "Number of alerts is 2104.\n",
      "optimized cutoff is 0.3177100718021393\n",
      "The F1 score for this choice is 0.15384615384615383.\n",
      "Number of alerts is 1633.\n",
      "optimized cutoff is 0.5032689571380615\n",
      "The F1 score for this choice is 0.13333333333333333.\n",
      "Number of alerts is 1099.\n",
      "optimized cutoff is 0.9995619654655457\n",
      "The F1 score for this choice is 0.07407407407407407.\n",
      "Number of alerts is 50.\n",
      "optimized cutoff is 0.9999212026596069\n",
      "The F1 score for this choice is 0.052631578947368425.\n",
      "Number of alerts is 225.\n",
      "optimized cutoff is 0.07139676064252853\n",
      "The F1 score for this choice is 0.02702702702702703.\n",
      "Number of alerts is 2005.\n",
      "optimized cutoff is 0.9992859959602356\n",
      "The F1 score for this choice is 0.0975609756097561.\n",
      "Number of alerts is 340.\n",
      "optimized cutoff is 0.005958836991339922\n",
      "The F1 score for this choice is 0.02380952380952381.\n",
      "Number of alerts is 3626.\n",
      "optimized cutoff is 0.9999024868011475\n",
      "The F1 score for this choice is 0.07407407407407407.\n",
      "Number of alerts is 46.\n",
      "optimized cutoff is 0.981023907661438\n",
      "The F1 score for this choice is 0.009900990099009901.\n",
      "Number of alerts is 980.\n",
      "optimized cutoff is 0.9987888932228088\n",
      "The F1 score for this choice is 0.04.\n",
      "Number of alerts is 99.\n",
      "optimized cutoff is 0.2564903497695923\n",
      "The F1 score for this choice is 0.05128205128205128.\n",
      "Number of alerts is 1231.\n",
      "optimized cutoff is 0.12165634334087372\n",
      "The F1 score for this choice is 0.06349206349206349.\n",
      "Number of alerts is 1661.\n",
      "optimized cutoff is 0.05812959745526314\n",
      "The F1 score for this choice is 0.05825242718446602.\n",
      "Number of alerts is 1961.\n",
      "optimized cutoff is 0.28538191318511963\n",
      "The F1 score for this choice is 0.07692307692307691.\n",
      "Number of alerts is 1186.\n",
      "optimized cutoff is 0.2758660912513733\n",
      "The F1 score for this choice is 0.17777777777777776.\n",
      "Number of alerts is 1090.\n",
      "optimized cutoff is 0.13345491886138916\n",
      "The F1 score for this choice is 0.0909090909090909.\n",
      "Number of alerts is 1581.\n",
      "optimized cutoff is 0.2466128170490265\n",
      "The F1 score for this choice is 0.08333333333333334.\n",
      "Number of alerts is 1204.\n",
      "optimized cutoff is 0.10744040459394455\n",
      "The F1 score for this choice is 0.10810810810810811.\n",
      "Number of alerts is 1755.\n",
      "optimized cutoff is 0.9995429515838623\n",
      "The F1 score for this choice is 0.1702127659574468.\n",
      "Number of alerts is 300.\n",
      "optimized cutoff is 0.3707692623138428\n",
      "The F1 score for this choice is 0.14285714285714282.\n",
      "Number of alerts is 1051.\n",
      "optimized cutoff is 0.13361909985542297\n",
      "The F1 score for this choice is 0.19354838709677416.\n",
      "Number of alerts is 1425.\n",
      "optimized cutoff is 0.22092749178409576\n",
      "The F1 score for this choice is 0.11999999999999998.\n",
      "Number of alerts is 1031.\n",
      "optimized cutoff is 0.5598065853118896\n",
      "The F1 score for this choice is 0.14285714285714288.\n",
      "Number of alerts is 1400.\n",
      "optimized cutoff is 0.10687768459320068\n",
      "The F1 score for this choice is 0.13157894736842105.\n",
      "Number of alerts is 2792.\n",
      "optimized cutoff is 0.058567147701978683\n",
      "The F1 score for this choice is 0.15555555555555556.\n",
      "Number of alerts is 3321.\n",
      "optimized cutoff is 0.17455849051475525\n",
      "The F1 score for this choice is 0.11538461538461538.\n",
      "Number of alerts is 2267.\n",
      "optimized cutoff is 0.3398488759994507\n",
      "The F1 score for this choice is 0.08888888888888888.\n",
      "Number of alerts is 1742.\n",
      "optimized cutoff is 0.7202052474021912\n",
      "The F1 score for this choice is 0.15384615384615385.\n",
      "Number of alerts is 1219.\n",
      "optimized cutoff is 0.24317015707492828\n",
      "The F1 score for this choice is 0.22222222222222224.\n",
      "Number of alerts is 1944.\n",
      "optimized cutoff is 0.2407759726047516\n",
      "The F1 score for this choice is 0.12244897959183673.\n",
      "Number of alerts is 2291.\n",
      "optimized cutoff is 0.9996084570884705\n",
      "The F1 score for this choice is 0.08163265306122448.\n",
      "Number of alerts is 666.\n",
      "optimized cutoff is 0.9956432580947876\n",
      "The F1 score for this choice is 0.04477611940298507.\n",
      "Number of alerts is 479.\n",
      "optimized cutoff is 0.006711726076900959\n",
      "The F1 score for this choice is 0.026041666666666668.\n",
      "Number of alerts is 4557.\n",
      "optimized cutoff is 0.9999161958694458\n",
      "The F1 score for this choice is 0.043478260869565216.\n",
      "Number of alerts is 145.\n",
      "optimized cutoff is 0.04368095099925995\n",
      "The F1 score for this choice is 0.03125.\n",
      "Number of alerts is 2725.\n",
      "optimized cutoff is 0.00047324583283625543\n",
      "The F1 score for this choice is 0.01819560272934041.\n",
      "Number of alerts is 7131.\n",
      "optimized cutoff is 0.999554455280304\n",
      "The F1 score for this choice is 0.04.\n",
      "Number of alerts is 272.\n",
      "optimized cutoff is 0.24435071647167206\n",
      "The F1 score for this choice is 0.045454545454545456.\n",
      "Number of alerts is 1655.\n",
      "optimized cutoff is 0.022740773856639862\n",
      "The F1 score for this choice is 0.03278688524590164.\n",
      "Number of alerts is 3137.\n",
      "optimized cutoff is 0.007387962657958269\n",
      "The F1 score for this choice is 0.02237136465324385.\n",
      "Number of alerts is 4775.\n",
      "optimized cutoff is 0.020951898768544197\n",
      "The F1 score for this choice is 0.03846153846153846.\n",
      "Number of alerts is 3209.\n",
      "optimized cutoff is 0.9987311959266663\n",
      "The F1 score for this choice is 0.01675977653631285.\n",
      "Number of alerts is 738.\n",
      "optimized cutoff is 0.04656072333455086\n",
      "The F1 score for this choice is 0.05309734513274336.\n",
      "Number of alerts is 3460.\n",
      "optimized cutoff is 0.046576399356126785\n",
      "The F1 score for this choice is 0.05084745762711864.\n",
      "Number of alerts is 3475.\n",
      "optimized cutoff is 0.9999628067016602\n",
      "The F1 score for this choice is 0.060606060606060615.\n",
      "Number of alerts is 94.\n",
      "optimized cutoff is 0.015168941579759121\n",
      "The F1 score for this choice is 0.027522935779816515.\n",
      "Number of alerts is 4185.\n",
      "optimized cutoff is 0.0373787097632885\n",
      "The F1 score for this choice is 0.04195804195804196.\n",
      "Number of alerts is 3774.\n",
      "optimized cutoff is 0.04153977707028389\n",
      "The F1 score for this choice is 0.04724409448818897.\n",
      "Number of alerts is 3252.\n",
      "optimized cutoff is 0.4378534257411957\n",
      "The F1 score for this choice is 0.06451612903225805.\n",
      "Number of alerts is 1354.\n",
      "optimized cutoff is 0.1198296993970871\n",
      "The F1 score for this choice is 0.03571428571428571.\n",
      "Number of alerts is 2547.\n"
     ]
    }
   ],
   "source": [
    "Answers_for_each_org_dict = {}\n",
    "for org_num in range(20):\n",
    "    Answers_for_each_org_dict[org_num] = get_all_answers_for_org_alt_Mohanad(org_num, answer_iterations=10)\n",
    "    pd.DataFrame(Answers_for_each_org_dict[org_num]).to_csv(\"C:/Users/Mimran/OneDrive - George Mason University/C4I PC Backup/SCITE/RCPs Fifth Quarter/RCP14/Dev/20Org10Iter_Answers_Mohanad_{}.csv\".format(org_num), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.concat(pd.DataFrame(Answers_for_each_org_dict[org_num]) for org_num in range(20)).to_csv(\"MultiIter10Answers_for_first_20_orgs.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(Answers_for_each_org_dict).T.astype(float).to_csv(\"Mohanad_alt__prelim_answers.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized cutoff is 0.8509586453437805\n",
      "The F1 score for this choice is 0.24000000000000002.\n",
      "Number of alerts is 939.\n",
      "optimized cutoff is 0.20511609315872192\n",
      "The F1 score for this choice is 0.09302325581395349.\n",
      "Number of alerts is 2077.\n",
      "optimized cutoff is 0.14999797940254211\n",
      "The F1 score for this choice is 0.14035087719298245.\n",
      "Number of alerts is 3227.\n",
      "optimized cutoff is 0.10508623719215393\n",
      "The F1 score for this choice is 0.1038961038961039.\n",
      "Number of alerts is 4539.\n",
      "optimized cutoff is 0.06337732821702957\n",
      "The F1 score for this choice is 0.0449438202247191.\n",
      "Number of alerts is 3427.\n"
     ]
    }
   ],
   "source": [
    "Answers_for_each_org_dict = {}\n",
    "for org_num in range(5):\n",
    "    Answers_for_each_org_dict[org_num] = get_all_answers_for_org(org_num)\n",
    "    #pd.DataFrame(Answers_for_each_org_dict[org_num]).to_csv(\"C:/Users/Mimran/OneDrive - George Mason University/C4I PC Backup/SCITE/RCPs Fifth Quarter/RCP14/Dev/Answers_Mohanad_{}.csv\".format(org_num), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(Answers_for_each_org_dict).T.astype(float).to_csv(\"Mohanad_more_prelim_answers.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "some_arr = np.array([[2.62939807e-07],\n",
    " [  9.73050701e-05],\n",
    " [  3.06557581e-07],\n",
    " [  2.80383620e-06],\n",
    " [  2.15645372e-07],\n",
    " [  4.46738454e-07],\n",
    " [  5.90954005e-06],\n",
    " [  9.25215500e-06],\n",
    " [  3.62556960e-07],\n",
    " [  1.25229633e-06]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.62939807e-07,   9.73050701e-05,   3.06557581e-07,\n",
       "         2.80383620e-06,   2.15645372e-07,   4.46738454e-07,\n",
       "         5.90954005e-06,   9.25215500e-06,   3.62556960e-07,\n",
       "         1.25229633e-06])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_arr[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized cutoff is 0.7352255582809448\n",
      "The F1 score for this choice is 0.20689655172413793.\n",
      "Number of alerts is 1511.\n",
      "optimized cutoff is 0.08171864598989487\n",
      "The F1 score for this choice is 0.02702702702702703.\n",
      "Number of alerts is 3580.\n",
      "optimized cutoff is 0.5472603440284729\n",
      "The F1 score for this choice is 0.13793103448275862.\n",
      "Number of alerts is 1624.\n",
      "optimized cutoff is 0.9986234903335571\n",
      "The F1 score for this choice is 0.06896551724137931.\n",
      "Number of alerts is 14.\n",
      "optimized cutoff is 0.7334756851196289\n",
      "The F1 score for this choice is 0.08333333333333334.\n",
      "Number of alerts is 1346.\n",
      "optimized cutoff is 0.20547465980052948\n",
      "The F1 score for this choice is 0.14285714285714282.\n",
      "Number of alerts is 1907.\n",
      "optimized cutoff is 0.9969298243522644\n",
      "The F1 score for this choice is 0.11940298507462685.\n",
      "Number of alerts is 399.\n",
      "optimized cutoff is 0.9954180717468262\n",
      "The F1 score for this choice is 0.02898550724637681.\n",
      "Number of alerts is 322.\n",
      "optimized cutoff is 0.04548053443431854\n",
      "The F1 score for this choice is 0.049586776859504134.\n",
      "Number of alerts is 3542.\n",
      "optimized cutoff is 0.05174603313207626\n",
      "The F1 score for this choice is 0.08602150537634409.\n",
      "Number of alerts is 4256.\n",
      "optimized cutoff is 0.03952081874012947\n",
      "The F1 score for this choice is 0.12962962962962965.\n",
      "Number of alerts is 4199.\n",
      "optimized cutoff is 0.08714776486158371\n",
      "The F1 score for this choice is 0.05797101449275362.\n",
      "Number of alerts is 4241.\n",
      "optimized cutoff is 0.3591715097427368\n",
      "The F1 score for this choice is 0.05714285714285715.\n",
      "Number of alerts is 2167.\n",
      "optimized cutoff is 0.24868173897266388\n",
      "The F1 score for this choice is 0.052631578947368425.\n",
      "Number of alerts is 2240.\n",
      "optimized cutoff is 0.26274216175079346\n",
      "The F1 score for this choice is 0.13043478260869565.\n",
      "Number of alerts is 1210.\n",
      "optimized cutoff is 0.053920067846775055\n",
      "The F1 score for this choice is 0.0625.\n",
      "Number of alerts is 2813.\n",
      "optimized cutoff is 0.5038806796073914\n",
      "The F1 score for this choice is 0.1142857142857143.\n",
      "Number of alerts is 1139.\n",
      "optimized cutoff is 0.017124086618423462\n",
      "The F1 score for this choice is 0.029268292682926834.\n",
      "Number of alerts is 5339.\n",
      "optimized cutoff is 0.18979662656784058\n",
      "The F1 score for this choice is 0.12698412698412698.\n",
      "Number of alerts is 3012.\n",
      "optimized cutoff is 0.5269011855125427\n",
      "The F1 score for this choice is 0.13793103448275862.\n",
      "Number of alerts is 1927.\n",
      "optimized cutoff is 0.192698672413826\n",
      "The F1 score for this choice is 0.16949152542372883.\n",
      "Number of alerts is 2642.\n",
      "optimized cutoff is 0.9985697269439697\n",
      "The F1 score for this choice is 0.09302325581395349.\n",
      "Number of alerts is 560.\n",
      "optimized cutoff is 0.9995785355567932\n",
      "The F1 score for this choice is 0.03225806451612903.\n",
      "Number of alerts is 195.\n",
      "optimized cutoff is 0.9984379410743713\n",
      "The F1 score for this choice is 0.08.\n",
      "Number of alerts is 84.\n",
      "optimized cutoff is 0.12752866744995117\n",
      "The F1 score for this choice is 0.04.\n",
      "Number of alerts is 3249.\n",
      "optimized cutoff is 0.999018669128418\n",
      "The F1 score for this choice is 0.12500000000000003.\n",
      "Number of alerts is 56.\n",
      "optimized cutoff is 0.18836043775081635\n",
      "The F1 score for this choice is 0.09523809523809525.\n",
      "Number of alerts is 1892.\n",
      "optimized cutoff is 0.047061849385499954\n",
      "The F1 score for this choice is 0.09433962264150943.\n",
      "Number of alerts is 4442.\n",
      "optimized cutoff is 0.055175021290779114\n",
      "The F1 score for this choice is 0.0606060606060606.\n",
      "Number of alerts is 3509.\n",
      "optimized cutoff is 0.6575167775154114\n",
      "The F1 score for this choice is 0.14285714285714288.\n",
      "Number of alerts is 691.\n",
      "optimized cutoff is 0.949266791343689\n",
      "The F1 score for this choice is 0.08.\n",
      "Number of alerts is 201.\n",
      "optimized cutoff is 0.31768202781677246\n",
      "The F1 score for this choice is 0.11764705882352942.\n",
      "Number of alerts is 1179.\n",
      "optimized cutoff is 0.43747109174728394\n",
      "The F1 score for this choice is 0.14814814814814814.\n",
      "Number of alerts is 1545.\n",
      "optimized cutoff is 0.032318226993083954\n",
      "The F1 score for this choice is 0.059880239520958084.\n",
      "Number of alerts is 3153.\n",
      "optimized cutoff is 0.9998433589935303\n",
      "The F1 score for this choice is 0.07407407407407407.\n",
      "Number of alerts is 79.\n",
      "optimized cutoff is 0.9963209629058838\n",
      "The F1 score for this choice is 0.01818181818181818.\n",
      "Number of alerts is 1184.\n",
      "optimized cutoff is 0.714061439037323\n",
      "The F1 score for this choice is 0.08888888888888888.\n",
      "Number of alerts is 660.\n",
      "optimized cutoff is 0.041683997958898544\n",
      "The F1 score for this choice is 0.06666666666666667.\n",
      "Number of alerts is 5209.\n",
      "optimized cutoff is 0.2888932526111603\n",
      "The F1 score for this choice is 0.10810810810810811.\n",
      "Number of alerts is 2218.\n",
      "optimized cutoff is 0.0019619239028543234\n",
      "The F1 score for this choice is 0.013717421124828532.\n",
      "Number of alerts is 8631.\n",
      "optimized cutoff is 0.3619981110095978\n",
      "The F1 score for this choice is 0.10256410256410256.\n",
      "Number of alerts is 1561.\n",
      "optimized cutoff is 0.3891967535018921\n",
      "The F1 score for this choice is 0.07142857142857144.\n",
      "Number of alerts is 1037.\n",
      "optimized cutoff is 0.09689554572105408\n",
      "The F1 score for this choice is 0.0963855421686747.\n",
      "Number of alerts is 1862.\n",
      "optimized cutoff is 0.38656026124954224\n",
      "The F1 score for this choice is 0.15384615384615383.\n",
      "Number of alerts is 1688.\n",
      "optimized cutoff is 0.9998108744621277\n",
      "The F1 score for this choice is 0.08333333333333334.\n",
      "Number of alerts is 137.\n",
      "optimized cutoff is 0.9998268485069275\n",
      "The F1 score for this choice is 0.27586206896551724.\n",
      "Number of alerts is 137.\n",
      "optimized cutoff is 0.05327776446938515\n",
      "The F1 score for this choice is 0.05769230769230768.\n",
      "Number of alerts is 3026.\n",
      "optimized cutoff is 0.5406126379966736\n",
      "The F1 score for this choice is 0.06666666666666667.\n",
      "Number of alerts is 888.\n",
      "optimized cutoff is 0.7843238711357117\n",
      "The F1 score for this choice is 0.08333333333333334.\n",
      "Number of alerts is 939.\n",
      "optimized cutoff is 0.9047666192054749\n",
      "The F1 score for this choice is 0.08333333333333334.\n",
      "Number of alerts is 370.\n"
     ]
    }
   ],
   "source": [
    "Answers_for_each_org_dict = {}\n",
    "for org_num in range(50):\n",
    "    Answers_for_each_org_dict[org_num] = get_all_answers_for_org(org_num)\n",
    "    #pd.DataFrame(Answers_for_each_org_dict[org_num]).to_csv(\"C:/Users/Mimran/OneDrive - George Mason University/C4I PC Backup/SCITE/RCPs Fifth Quarter/RCP14/Dev/Answers_Mohanad_{}.csv\".format(org_num), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(Answers_for_each_org_dict).T.astype(float).to_csv(\"Mohanad_even_more_prelim_answers.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Answer_20': array([ 1.]), 'Answer_13': array([ 0.00214158]), 'Answer_11': array([ 0.00178117]), 'Answer_1': array([ 0.85]), 'Answer_10': array([ 0.2734375]), 'Answer_16': array([ 0.21875]), 'Answer_21': array([ 0.10847458]), 'Answer_7': array([ 0.00219421]), 'Answer_6': array([ 0.3359375]), 'Answer_18': array([ 0.3125]), 'Answer_5': array([ 0.00263484]), 'Answer_2': array([ 0.265625]), 'Answer_9': array([ 0.00201384]), 'Answer_3': array([ 0.0006867]), 'Answer_19': array([ 0.00210393]), 'Answer_4': array([ 0.3984375]), 'Answer_8': array([ 0.3046875]), 'Answer_12': array([ 0.2890625]), 'Answer_15': array([ 0.00225722]), 'Answer_14': array([ 0.3359375]), 'Answer_17': array([ 0.00166468])}\n"
     ]
    }
   ],
   "source": [
    "print(Answers_for_each_org_dict[org_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 52.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Answers_for_each_org_dict[org_num][\"Answer_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = read_org_data(3, 7, 33)\n",
    "S1_x, S2_x, S3_x, S1_y, S2_y, S3_y, scaler_from_training_data = split_training_data(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S1_y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S2_y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S3_y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "76+13+23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['Target_t'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7946428571428571"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "89/112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12359550561797752"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11/89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20535714285714285"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "23/112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = read_org_test_data(4, 34, 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1192"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['trait_20'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['X058a_t'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
