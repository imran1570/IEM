{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import average_precision_score, f1_score\n",
    "\n",
    "from scipy.optimize import minimize_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_training_week_data_filename_QD(week_number, simulated_org_number):\n",
    "    head_folder_name = \"/Users/me/Work/FifthQuarterQD/Zhengyang_July4\"\n",
    "    full_filename = \"{}/{}/org ({}).csv\".format(head_folder_name, \n",
    "                                                     week_number, \n",
    "                                                     simulated_org_number)\n",
    "    return full_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can find week 5:True\n",
      "can find week 6:True\n",
      "can find week 7:True\n",
      "can find week 8:True\n",
      "can find week 9:True\n",
      "can find week 10:True\n",
      "can find week 11:True\n",
      "can find week 12:True\n",
      "can find week 13:True\n",
      "can find week 14:True\n",
      "can find week 15:True\n",
      "can find week 16:True\n",
      "can find week 17:True\n",
      "can find week 18:True\n",
      "can find week 19:True\n",
      "can find week 20:True\n",
      "can find week 21:True\n",
      "can find week 22:True\n",
      "can find week 23:True\n",
      "can find week 24:True\n",
      "can find week 25:True\n",
      "can find week 26:True\n",
      "can find week 27:True\n",
      "can find week 28:True\n",
      "can find week 29:True\n",
      "can find week 30:True\n",
      "can find week 31:True\n",
      "can find week 32:True\n",
      "can find week 33:True\n",
      "can find week 34:True\n",
      "can find week 35:True\n",
      "can find week 36:True\n",
      "can find week 37:True\n",
      "can find week 38:True\n",
      "can find week 39:True\n",
      "can find week 40:True\n",
      "can find week 41:True\n",
      "can find week 42:True\n",
      "can find week 43:True\n",
      "can find week 44:True\n",
      "can find week 45:True\n",
      "can find week 46:True\n",
      "can find week 47:True\n",
      "can find week 48:True\n",
      "can find week 49:True\n",
      "can find week 50:True\n",
      "can find week 51:True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "for week_number in range(5, 52):\n",
    "    print(\"can find week {}:{}\".format(week_number, \n",
    "                                       Path(get_training_week_data_filename_QD(week_number, 9)).exists()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_training_week_data_QD(week_number, simulated_org_number):\n",
    "    full_filename = get_training_week_data_filename_QD(week_number, simulated_org_number)\n",
    "    week_df = pd.read_csv(full_filename, index_col = 0) # Note this assumes similar order of users everywhere\n",
    "    week_df.replace([-np.inf,np.inf], np.nan, inplace=True) #(no matching of users is necessary *under this assumption*)\n",
    "    return week_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "detector_names = read_training_week_data_QD(20, 5).columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Target',\n",
       " 'X001a',\n",
       " 'X001b',\n",
       " 'X001c',\n",
       " 'X014a',\n",
       " 'X015a',\n",
       " 'X021a',\n",
       " 'X021d',\n",
       " 'X021e',\n",
       " 'X021f',\n",
       " 'X021g',\n",
       " 'X021h',\n",
       " 'X021i',\n",
       " 'X021j',\n",
       " 'X022a',\n",
       " 'X022d',\n",
       " 'X022e',\n",
       " 'X022f',\n",
       " 'X022g',\n",
       " 'X022h',\n",
       " 'X022i',\n",
       " 'X022j',\n",
       " 'X027a',\n",
       " 'X027d',\n",
       " 'X027e',\n",
       " 'X027f',\n",
       " 'X027g',\n",
       " 'X027h',\n",
       " 'X027i',\n",
       " 'X027j',\n",
       " 'X028a',\n",
       " 'X028d',\n",
       " 'X028e',\n",
       " 'X028f',\n",
       " 'X028g',\n",
       " 'X028h',\n",
       " 'X028i',\n",
       " 'X028j',\n",
       " 'X029a',\n",
       " 'X030a',\n",
       " 'X031a',\n",
       " 'X032a',\n",
       " 'X033a',\n",
       " 'X034a',\n",
       " 'X035a',\n",
       " 'X036a',\n",
       " 'X037a',\n",
       " 'X038a',\n",
       " 'X039a',\n",
       " 'X040a',\n",
       " 'X041a',\n",
       " 'X042a',\n",
       " 'X043a',\n",
       " 'X044a',\n",
       " 'X045a',\n",
       " 'X046a',\n",
       " 'X047a',\n",
       " 'X048a',\n",
       " 'X049a',\n",
       " 'X050a',\n",
       " 'X051a',\n",
       " 'X052a',\n",
       " 'X053a',\n",
       " 'X058a',\n",
       " 'X059a',\n",
       " 'X060a']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(662017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_training_org_data(simulated_org_number, first_full_week, last_full_week):\n",
    "    all_full_week_dfs = []\n",
    "    list_of_dfs_for_feature_vectors = [read_training_week_data_QD(week_number, simulated_org_number) for week_number in range(first_full_week - 2, first_full_week + 2)]\n",
    "    for current_week in range(first_full_week, last_full_week + 1):\n",
    "        list_of_dfs_for_feature_vectors.append(read_week_data_QD(current_week + 2, simulated_org_number))\n",
    "        current_week_df = pd.concat([list_of_dfs_for_feature_vectors[0].rename(columns = lambda some_str: some_str + \"_t-2\"), \n",
    "                                     list_of_dfs_for_feature_vectors[1].rename(columns = lambda some_str: some_str + \"_t-1\"), \n",
    "                                     list_of_dfs_for_feature_vectors[2].rename(columns = lambda some_str: some_str + \"_t\"), \n",
    "                                     list_of_dfs_for_feature_vectors[3].rename(columns = lambda some_str: some_str + \"_t+1\"), \n",
    "                                     list_of_dfs_for_feature_vectors[4].rename(columns = lambda some_str: some_str + \"_t+2\")], \n",
    "                                    axis = 1)\n",
    "        current_week_df.dropna(inplace=True)\n",
    "        all_full_week_dfs.append(current_week_df)\n",
    "        del list_of_dfs_for_feature_vectors[0]\n",
    "    return pd.concat(all_full_week_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the RCP description:\n",
    "\n",
    "We first separate the data from the training period (Weeks 7—33) from the data from the testing period (Weeks 34—49). Then, we separate the data from the training period into three distinct groups. The first, $S_1$, is used for training the model and contains 70% of the data. The second, $S_2$, is reserved to use for potential early stopping and contains 10% of the data. The third, $S_3$, is used for validation and contains 20% of the data. Note that we divide the data in a stratified manner so that each group contains both zeros and ones. To be clear, $S_1$ contains 70% of the zeros from the training period and 70% of the ones from the training period. We computed the means and standard deviations of all features over $S_1 \\cup S_2$ and used these to standardize all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_training_data(sample_training_data_df):\n",
    "    S12_x, S3_x, S12_y, S3_y = train_test_split(sample_training_data_df.drop(['Target_t-2',\n",
    "                                                                              'Target_t-1',\n",
    "                                                                              'Target_t',\n",
    "                                                                              'Target_t+1',\n",
    "                                                                              'Target_t+2'], 1),\n",
    "                                                sample_training_data_df['Target_t'],\n",
    "                                                test_size = 0.2)\n",
    "    scaler = StandardScaler().fit(S12_x)\n",
    "    S12_x = scaler.transform(S12_x)\n",
    "    S3_x = scaler.transform(S3_x)\n",
    "    S1_x, S2_x, S1_y, S2_y = train_test_split(S12_x, S12_y, test_size = 0.125)\n",
    "    return S1_x, S2_x, S3_x, S1_y.values, S2_y.values, S3_y.values, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RCP14_Algorithm_1(S1_x, S2_x, S3_x, S1_y, S2_y, S3_y, iter_per_weight = 25):\n",
    "    weight_average_areas = {}\n",
    "    for weight_to_test in range(10, 210, 10):\n",
    "        weight_areas = np.zeros(iter_per_weight)\n",
    "        for iteration_num in range(iter_per_weight):\n",
    "            n_net = MLPRegressor(hidden_layer_sizes=(325,325), \n",
    "                                  activation = 'tanh', \n",
    "                                  solver = 'sgd', max_iter = 100, early_stopping = True)\n",
    "            model_prediction_output = get_model_predictions(S1_x, S1_y, S2_x, S2_y, weight_to_test, S3_x)\n",
    "            weight_areas[iteration_num] = average_precision_score(S3_y, model_prediction_output)\n",
    "            print(\"current area is {}\".format(weight_areas[iteration_num]))\n",
    "        weight_average_areas[weight_to_test] = weight_areas.mean()\n",
    "        print(\"weight {} had an average area of {}\".format(weight_to_test, weight_average_areas[weight_to_test]))\n",
    "    return max(weight_average_areas.keys(), key=(lambda key: weight_average_areas[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_org_test_data(simulated_org_number, first_full_week, last_full_week):\n",
    "    all_full_week_dfs = []\n",
    "    list_of_dfs_for_feature_vectors = [read_week_data_QD(week_number, simulated_org_number) for week_number in range(first_full_week - 2, first_full_week + 2)]\n",
    "    for current_week in range(first_full_week, last_full_week + 1):\n",
    "        list_of_dfs_for_feature_vectors.append(read_week_data_QD(current_week + 2, simulated_org_number))\n",
    "        current_week_df = pd.concat([list_of_dfs_for_feature_vectors[0].rename(columns = lambda some_str: some_str + \"_t-2\"), \n",
    "                                     list_of_dfs_for_feature_vectors[1].rename(columns = lambda some_str: some_str + \"_t-1\"), \n",
    "                                     list_of_dfs_for_feature_vectors[2].rename(columns = lambda some_str: some_str + \"_t\"), \n",
    "                                     list_of_dfs_for_feature_vectors[3].rename(columns = lambda some_str: some_str + \"_t+1\"), \n",
    "                                     list_of_dfs_for_feature_vectors[4].rename(columns = lambda some_str: some_str + \"_t+2\")], \n",
    "                                    axis = 1)\n",
    "        current_week_df.dropna(inplace=True)\n",
    "        detector_string = 'X021f'\n",
    "        current_week_df['trait_4'] = ((current_week_df['{}_t-2'.format(detector_string)] > np.percentile(current_week_df['{}_t-2'.format(detector_string)], 90)) | \n",
    "                                      (current_week_df['{}_t-1'.format(detector_string)] > np.percentile(current_week_df['{}_t-1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t'.format(detector_string)] > np.percentile(current_week_df['{}_t'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+1'.format(detector_string)] > np.percentile(current_week_df['{}_t+1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+2'.format(detector_string)] > np.percentile(current_week_df['{}_t+2'.format(detector_string)], 90)))\n",
    "        detector_string = 'X021h'\n",
    "        current_week_df['trait_6'] = ((current_week_df['{}_t-2'.format(detector_string)] > np.percentile(current_week_df['{}_t-2'.format(detector_string)], 90)) | \n",
    "                                      (current_week_df['{}_t-1'.format(detector_string)] > np.percentile(current_week_df['{}_t-1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t'.format(detector_string)] > np.percentile(current_week_df['{}_t'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+1'.format(detector_string)] > np.percentile(current_week_df['{}_t+1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+2'.format(detector_string)] > np.percentile(current_week_df['{}_t+2'.format(detector_string)], 90)))\n",
    "        detector_string = 'X022f'\n",
    "        current_week_df['trait_8'] = ((current_week_df['{}_t-2'.format(detector_string)] > np.percentile(current_week_df['{}_t-2'.format(detector_string)], 90)) | \n",
    "                                      (current_week_df['{}_t-1'.format(detector_string)] > np.percentile(current_week_df['{}_t-1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t'.format(detector_string)] > np.percentile(current_week_df['{}_t'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+1'.format(detector_string)] > np.percentile(current_week_df['{}_t+1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+2'.format(detector_string)] > np.percentile(current_week_df['{}_t+2'.format(detector_string)], 90)))\n",
    "        detector_string = 'X022h'\n",
    "        current_week_df['trait_10'] = ((current_week_df['{}_t-2'.format(detector_string)] > np.percentile(current_week_df['{}_t-2'.format(detector_string)], 90)) | \n",
    "                                      (current_week_df['{}_t-1'.format(detector_string)] > np.percentile(current_week_df['{}_t-1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t'.format(detector_string)] > np.percentile(current_week_df['{}_t'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+1'.format(detector_string)] > np.percentile(current_week_df['{}_t+1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+2'.format(detector_string)] > np.percentile(current_week_df['{}_t+2'.format(detector_string)], 90)))\n",
    "        detector_string = 'X027f'\n",
    "        current_week_df['trait_12'] = ((current_week_df['{}_t-2'.format(detector_string)] > np.percentile(current_week_df['{}_t-2'.format(detector_string)], 90)) | \n",
    "                                      (current_week_df['{}_t-1'.format(detector_string)] > np.percentile(current_week_df['{}_t-1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t'.format(detector_string)] > np.percentile(current_week_df['{}_t'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+1'.format(detector_string)] > np.percentile(current_week_df['{}_t+1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+2'.format(detector_string)] > np.percentile(current_week_df['{}_t+2'.format(detector_string)], 90)))\n",
    "        detector_string = 'X027h'\n",
    "        current_week_df['trait_14'] = ((current_week_df['{}_t-2'.format(detector_string)] > np.percentile(current_week_df['{}_t-2'.format(detector_string)], 90)) | \n",
    "                                      (current_week_df['{}_t-1'.format(detector_string)] > np.percentile(current_week_df['{}_t-1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t'.format(detector_string)] > np.percentile(current_week_df['{}_t'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+1'.format(detector_string)] > np.percentile(current_week_df['{}_t+1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+2'.format(detector_string)] > np.percentile(current_week_df['{}_t+2'.format(detector_string)], 90)))\n",
    "        detector_string = 'X028f'\n",
    "        current_week_df['trait_16'] = ((current_week_df['{}_t-2'.format(detector_string)] > np.percentile(current_week_df['{}_t-2'.format(detector_string)], 90)) | \n",
    "                                      (current_week_df['{}_t-1'.format(detector_string)] > np.percentile(current_week_df['{}_t-1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t'.format(detector_string)] > np.percentile(current_week_df['{}_t'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+1'.format(detector_string)] > np.percentile(current_week_df['{}_t+1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+2'.format(detector_string)] > np.percentile(current_week_df['{}_t+2'.format(detector_string)], 90)))\n",
    "        detector_string = 'X028h'\n",
    "        current_week_df['trait_18'] = ((current_week_df['{}_t-2'.format(detector_string)] > np.percentile(current_week_df['{}_t-2'.format(detector_string)], 90)) | \n",
    "                                      (current_week_df['{}_t-1'.format(detector_string)] > np.percentile(current_week_df['{}_t-1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t'.format(detector_string)] > np.percentile(current_week_df['{}_t'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+1'.format(detector_string)] > np.percentile(current_week_df['{}_t+1'.format(detector_string)], 90)) |\n",
    "                                      (current_week_df['{}_t+2'.format(detector_string)] > np.percentile(current_week_df['{}_t+2'.format(detector_string)], 90)))\n",
    "        detector_string = 'X058a'\n",
    "        current_week_df['trait_20'] = ((current_week_df['{}_t-2'.format(detector_string)]).astype(int) | \n",
    "                                      (current_week_df['{}_t-1'.format(detector_string)]).astype(int) |\n",
    "                                      (current_week_df['{}_t'.format(detector_string)]).astype(int) |\n",
    "                                      (current_week_df['{}_t+1'.format(detector_string)]).astype(int) |\n",
    "                                      (current_week_df['{}_t+2'.format(detector_string)]).astype(int))\n",
    "        all_full_week_dfs.append(current_week_df)\n",
    "        del list_of_dfs_for_feature_vectors[0]\n",
    "    return pd.concat(all_full_week_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_detector_names = [detector + relative_time for detector in detector_names[1:] for relative_time in ['_t-2', \n",
    "                                                                                                        '_t-1', \n",
    "                                                                                                        '_t', \n",
    "                                                                                                        '_t+1', \n",
    "                                                                                                        '_t+2']]\n",
    "all_trait_names = [\"trait_\" + str(trait_num) for trait_num in range(4, 21, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_test_data(sample_test_data_df, scaler_from_training_data):\n",
    "    T_x, T_y, T_generated_attributes = (sample_test_data_df[all_detector_names], \n",
    "                                        sample_test_data_df['Target_t'], \n",
    "                                        sample_test_data_df[all_trait_names])\n",
    "    T_x = scaler_from_training_data.transform(T_x)\n",
    "    return T_x, T_y.values, T_generated_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_f1_score_using_threshold(test_threshold, actual_y, output_for_y):\n",
    "    return f1_score(actual_y, (output_for_y > test_threshold).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RCP14_Algorithm_2(S1_x, S2_x, S3_x, S1_y, S2_y, S3_y, T_x, T_y, \n",
    "                      T_generated_attributes, chosen_weight, num_iterations = 100):\n",
    "    answer_dict = {\"Answer_\" + str(answer_num) : np.zeros(num_iterations) for answer_num in range(1, 22)}\n",
    "    for iteration_num in range(num_iterations):\n",
    "        current_model = get_model(S1_x, S1_y, S2_x, S2_y, chosen_weight)\n",
    "        model_prediction_output = current_model.predict(S3_x)\n",
    "        def func_to_min(x):\n",
    "            return -calc_f1_score_using_threshold(x, S3_y, model_prediction_output)\n",
    "        chosen_tau = minimize_scalar(func_to_min, bounds = (0, 1), method = 'bounded').x\n",
    "        print(\"optimized cutoff is {}\".format(chosen_tau))\n",
    "        prediction_output_for_test_data = current_model.predict(T_x)\n",
    "        T_labels = (prediction_output_for_test_data > chosen_tau).astype(int)\n",
    "        answer_dict[\"Answer_1\"][iteration_num] = (T_y & T_labels).sum() / T_y.sum()\n",
    "        answer_dict[\"Answer_2\"][iteration_num] = (T_y & T_labels).sum() / T_labels.sum()\n",
    "        answer_dict[\"Answer_3\"][iteration_num] = (T_y & T_labels).sum() / (T_y ^ 1).sum()\n",
    "        answer_dict[\"Answer_4\"][iteration_num] = (T_generated_attributes['trait_4'].values & T_labels).mean() / (T_labels).mean()\n",
    "        answer_dict[\"Answer_5\"][iteration_num] = (T_generated_attributes['trait_4'].values & T_labels).mean() / np.mean(T_generated_attributes['trait_4'])\n",
    "        answer_dict[\"Answer_6\"][iteration_num] = (T_generated_attributes['trait_6'].values & T_labels).mean() / (T_labels).mean()\n",
    "        answer_dict[\"Answer_7\"][iteration_num] = (T_generated_attributes['trait_6'].values & T_labels).mean() / np.mean(T_generated_attributes['trait_6'])\n",
    "        answer_dict[\"Answer_8\"][iteration_num] = (T_generated_attributes['trait_8'].values & T_labels).mean() / (T_labels).mean()\n",
    "        answer_dict[\"Answer_9\"][iteration_num] = (T_generated_attributes['trait_8'].values & T_labels).mean() / np.mean(T_generated_attributes['trait_8'])\n",
    "        answer_dict[\"Answer_10\"][iteration_num] = (T_generated_attributes['trait_10'].values & T_labels).mean() / (T_labels).mean()\n",
    "        answer_dict[\"Answer_11\"][iteration_num] = (T_generated_attributes['trait_10'].values & T_labels).mean() / np.mean(T_generated_attributes['trait_10'])\n",
    "        answer_dict[\"Answer_12\"][iteration_num] = (T_generated_attributes['trait_12'].values & T_labels).mean() / (T_labels).mean()\n",
    "        answer_dict[\"Answer_13\"][iteration_num] = (T_generated_attributes['trait_12'].values & T_labels).mean() / np.mean(T_generated_attributes['trait_12'])\n",
    "        answer_dict[\"Answer_14\"][iteration_num] = (T_generated_attributes['trait_14'].values & T_labels).mean() / (T_labels).mean()\n",
    "        answer_dict[\"Answer_15\"][iteration_num] = (T_generated_attributes['trait_14'].values & T_labels).mean() / np.mean(T_generated_attributes['trait_14'])\n",
    "        answer_dict[\"Answer_16\"][iteration_num] = (T_generated_attributes['trait_16'].values & T_labels).mean() / (T_labels).mean()\n",
    "        answer_dict[\"Answer_17\"][iteration_num] = (T_generated_attributes['trait_16'].values & T_labels).mean() / np.mean(T_generated_attributes['trait_16'])\n",
    "        answer_dict[\"Answer_18\"][iteration_num] = (T_generated_attributes['trait_18'].values & T_labels).mean() / (T_labels).mean()\n",
    "        answer_dict[\"Answer_19\"][iteration_num] = (T_generated_attributes['trait_18'].values & T_labels).mean() / np.mean(T_generated_attributes['trait_18'])\n",
    "        answer_dict[\"Answer_20\"][iteration_num] = (T_generated_attributes['trait_20'].values & T_labels).mean() / (T_labels).mean()\n",
    "        answer_dict[\"Answer_21\"][iteration_num] = (T_generated_attributes['trait_20'].values & T_labels).mean() / np.mean(T_generated_attributes['trait_20'])\n",
    "    return answer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_answers_for_org(simulated_org_number, iter_per_weight = 25, answer_iterations = 100):\n",
    "    training_data = read_org_data(simulated_org_number, 7, 33)\n",
    "    training_data.replace([-np.inf,np.inf], np.nan, inplace=True)\n",
    "    training_data.dropna(inplace=True)\n",
    "    S1_x, S2_x, S3_x, S1_y, S2_y, S3_y, scaler_from_training_data = split_training_data(training_data)\n",
    "    weight_to_use = RCP14_Algorithm_1(S1_x, S2_x, S3_x, S1_y, S2_y, S3_y, \n",
    "                                      iter_per_weight = iter_per_weight)\n",
    "    test_data = read_org_test_data(simulated_org_number, 34, 49)\n",
    "    test_data.replace([-np.inf,np.inf], np.nan, inplace=True)\n",
    "    test_data.dropna(inplace=True)\n",
    "    T_x, T_y, T_generated_attributes = split_test_data(test_data, scaler_from_training_data)\n",
    "    return RCP14_Algorithm_2(S1_x, S2_x, S3_x, S1_y, S2_y, S3_y, T_x, T_y, \n",
    "                      T_generated_attributes, weight_to_use, num_iterations = answer_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# some_df = read_org_data(10, 7, 33)\n",
    "\n",
    "# some_df.head()\n",
    "\n",
    "# some_df.shape\n",
    "\n",
    "# some_df['X028f_t'][~np.isfinite(some_df['X028f_t'])]\n",
    "\n",
    "# some_test_df = read_org_test_data(10, 34, 49)\n",
    "\n",
    "# some_test_df['X058a_t'][~np.isfinite(some_test_df['X058a_t+1'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Answers_for_each_org_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1_x has shape (58250, 325)\n",
      "S2_x has shape (8322, 325)\n",
      "S3_x has shape (16643, 325)\n",
      "S1_y has shape (58250,)\n",
      "S2_y has shape (8322,)\n",
      "S3_y has shape (16643,)\n",
      "Train on 58250 samples, validate on 8322 samples\n",
      "Epoch 1/100\n",
      "58250/58250 [==============================] - 4s - loss: 0.0725 - val_loss: 0.0087\n",
      "Epoch 2/100\n",
      "58250/58250 [==============================] - 4s - loss: 0.0070 - val_loss: 0.0036\n",
      "Epoch 3/100\n",
      "58250/58250 [==============================] - 4s - loss: 0.0036 - val_loss: 0.0023\n",
      "Epoch 4/100\n",
      "58250/58250 [==============================] - 4s - loss: 0.0023 - val_loss: 0.0016\n",
      "Epoch 5/100\n",
      "58250/58250 [==============================] - 4s - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 6/100\n",
      "58250/58250 [==============================] - 4s - loss: 0.0011 - val_loss: 9.3303e-04\n",
      "Epoch 7/100\n",
      "58250/58250 [==============================] - 4s - loss: 8.8765e-04 - val_loss: 7.8763e-04\n",
      "Epoch 8/100\n",
      "58250/58250 [==============================] - 4s - loss: 7.2499e-04 - val_loss: 6.4660e-04\n",
      "Epoch 9/100\n",
      "58250/58250 [==============================] - 4s - loss: 6.1071e-04 - val_loss: 5.5007e-04\n",
      "Epoch 10/100\n",
      "58250/58250 [==============================] - 4s - loss: 5.2677e-04 - val_loss: 4.7744e-04\n",
      "Epoch 11/100\n",
      "58250/58250 [==============================] - 4s - loss: 4.6178e-04 - val_loss: 4.2387e-04\n",
      "Epoch 12/100\n",
      "58250/58250 [==============================] - 4s - loss: 4.1044e-04 - val_loss: 3.8335e-04\n",
      "Epoch 13/100\n",
      "58250/58250 [==============================] - 4s - loss: 3.6727e-04 - val_loss: 3.4355e-04\n",
      "Epoch 14/100\n",
      "58250/58250 [==============================] - 4s - loss: 3.3360e-04 - val_loss: 3.1486e-04\n",
      "Epoch 15/100\n",
      "58250/58250 [==============================] - 4s - loss: 3.0281e-04 - val_loss: 2.8660e-04\n",
      "Epoch 16/100\n",
      "58250/58250 [==============================] - 4s - loss: 2.7965e-04 - val_loss: 2.6532e-04\n",
      "Epoch 17/100\n",
      "58250/58250 [==============================] - 4s - loss: 2.5778e-04 - val_loss: 2.4571e-04\n",
      "Epoch 18/100\n",
      "58250/58250 [==============================] - 4s - loss: 2.3997e-04 - val_loss: 2.3054e-04\n",
      "Epoch 19/100\n",
      "58250/58250 [==============================] - 4s - loss: 2.2308e-04 - val_loss: 2.1534e-04\n",
      "Epoch 20/100\n",
      "58250/58250 [==============================] - 4s - loss: 2.0927e-04 - val_loss: 2.0302e-04\n",
      "Epoch 21/100\n",
      "58250/58250 [==============================] - 4s - loss: 1.9620e-04 - val_loss: 1.9132e-04\n",
      "Epoch 22/100\n",
      "58250/58250 [==============================] - 4s - loss: 1.8511e-04 - val_loss: 1.8112e-04\n",
      "Epoch 23/100\n",
      "58250/58250 [==============================] - 4s - loss: 1.7473e-04 - val_loss: 1.7133e-04\n",
      "Epoch 24/100\n",
      "58250/58250 [==============================] - 4s - loss: 1.6573e-04 - val_loss: 1.6295e-04\n",
      "Epoch 25/100\n",
      "58250/58250 [==============================] - 4s - loss: 1.5746e-04 - val_loss: 1.5570e-04\n",
      "Epoch 26/100\n",
      "58250/58250 [==============================] - 4s - loss: 1.4957e-04 - val_loss: 1.4827e-04\n",
      "Epoch 27/100\n",
      "58250/58250 [==============================] - 4s - loss: 1.4288e-04 - val_loss: 1.4205e-04\n",
      "Epoch 28/100\n",
      "58250/58250 [==============================] - 4s - loss: 1.3646e-04 - val_loss: 1.3625e-04\n",
      "Epoch 29/100\n",
      "58250/58250 [==============================] - 4s - loss: 1.3045e-04 - val_loss: 1.3045e-04\n",
      "Epoch 30/100\n",
      "35232/58250 [=================>............] - ETA: 1s - loss: 1.2858e-04"
     ]
    }
   ],
   "source": [
    "for org_num in range(1, 13):\n",
    "    Answers_for_each_org_dict[org_num] = get_all_answers_for_org(org_num)\n",
    "    pd.DataFrame(Answers_for_each_org_dict[org_num]).to_csv(\"/home/ec2-user/generated_answers/Answers_Zhengyang_{}.csv\".format(org_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
